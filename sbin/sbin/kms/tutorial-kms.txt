# Demo for KMS
# Login into admin01 as sysadmin

# VARIABLES
# Set this variable
export PASSWORD=BadPass%1

# SETUP USERS 
# Set Kerberos for users
kinit
## enter BadPass%1
sudo -u horton kinit
## enter BadPass%1
# kinit as hdfs using the headless keytab and the principal name
sudo -u hdfs kinit -kt /etc/security/keytabs/hdfs.headless.keytab "hdfs-cloudair"

# SETUP DIRECTORIES AND KEYS
# List the keys and metadata
hadoop key list -metadata

# Create directories for EZs
hdfs dfs -mkdir -p /encrypt/zone1
hdfs dfs -mkdir -p /encrypt/zone2

# As hdfs create 2 EZs using the 2 keys
sudo -u hdfs hdfs crypto -createZone -keyName ridekey1 -path /encrypt/zone1
sudo -u hdfs hdfs crypto -createZone -keyName ridekey2 -path /encrypt/zone2
# if you get 'RemoteException' error it means you have not given namenode user permissions on ridekey1 by creating a policy for KMS in Ranger

# TEST DIRECTORIES AND KEYS
# Check EZs got created
sudo -u hdfs hdfs crypto -listZones  

# Create test files
echo "My test file1 for zone1" > /tmp/test1.log
echo "My test file2 for zone2" > /tmp/test2.log

# Copy files to EZs
hdfs dfs -copyFromLocal /tmp/test1.log /encrypt/zone1
hdfs dfs -copyFromLocal /tmp/test2.log /encrypt/zone1
hdfs dfs -copyFromLocal /tmp/test2.log /encrypt/zone2

# Test that sysadmin is allowed to decrypt EEK but not horton user 
# (since there is no Ranger policy allowing horton permission)
hdfs dfs -cat /encrypt/zone1/test1.log
hdfs dfs -cat /encrypt/zone2/test2.log
sudo -u horton  hdfs dfs -cat /encrypt/zone1/test1.log
	cat: User:horton not allowed to do 'DECRYPT_EEK' on 'ridekey1'

# Test to remove file from EZ using usual -rm command 
hdfs dfs -rm /encrypt/zone1/test2.log
# This works because as of HDP2.4.3 -skipTrash option no longer needs to be specified

# Confirm that test2.log was deleted and that zone_encr only contains test1.log
hdfs dfs -ls  /encrypt/zone1/
 
# Copy a file between EZs using distcp with -skipcrccheck option
hadoop distcp -skipcrccheck -update /encrypt/zone2/test2.log /encrypt/zone1

# SECURING ACCESS TO RAW FILES
# View contents of raw file in encrypted zone as hdfs super user. This should show some encrypted characters
sudo -u hdfs hdfs dfs -cat /.reserved/raw/encrypt/zone1/test1.log

# Prevent user hdfs from reading the file by setting security.hdfs.unreadable.by.superuser attribute. 
# Note that this attribute can only be set on files and can never be removed.
sudo -u hdfs hdfs dfs -setfattr -n security.hdfs.unreadable.by.superuser  /.reserved/raw/encrypt/zone1/test1.log

# Now as hdfs super user, try to read the files or the contents of the raw file
sudo -u hdfs hdfs dfs -cat /.reserved/raw/encrypt/zone1/test1.log
	cat: Access is denied for hdfs since the superuser is not allowed to perform this operation.

# ENCRYPTING HIVE WAREHOUSE
# This is setup for HDP2.6.5, the file system is different for HDP3.1
# List current Hive warehouse tables
hdfs dfs -ls /apps/hive/warehouse/

# Test if there are external tables
hdfs dfs -ls /warehouse/tablespace/external/hive
Found 2 items
drwxrwxrwx+  - hive hadoop          0 2018-10-26 18:31 /warehouse/tablespace/external/hive/information_schema.db
drwxr-xr-t+  - hive hadoop          0 2018-10-26 18:32 /warehouse/tablespace/external/hive/sys.db

# Create Secure Directory for Hive
hdfs dfs -mv /apps/hive/warehouse /warehouse-old
hdfs dfs -mkdir /apps/hive/warehouse
sudo -u hdfs hdfs crypto -createZone -keyName testkey -path /apps/hive/warehouse
hadoop distcp -skipcrccheck -update /warehouse-old /apps/hive/warehouse

# Configure the Hive scrath directory inside of the encryption zone
hdfs dfs -mkdir /apps/hive/tmp
sudo -u hdfs hdfs crypto -createZone -keyName testkey -path /apps/hive/tmp
sudo -u hdfs hdfs dfs -chmod -R 1777 /apps/hive/tmp

In Ambari > Hive > Configs > Advanced, change below to newly created dir
hive.exec.scratchdir = /apps/hive/tmp
Restart Hive and any other components that need it

# Validate
sudo -u horton hdfs dfs -ls /apps/hive/tmp
## this should provide listing

