{"paragraphs":[{"text":"%md\n# About\n**Lab:** Working with Apache Kafka Streaming Messages\n**Objective:** Read streaming data from a Kafka topic and send data to a Kafka topic.\n**File locations:**\n    Exercise directory: /home/training/training_materials/devsh/exercises/streaming-kafka\n    Test data (local): /home/training/training_materials/devsh/data/activations_stream\n    Test script: /home/training/training_materials/devsh/scripts/streamtest-kafka.sh\n    Solution files: /home/training/training_materials/devsh/scripts/streaming-kafka-source.pyspark\n                    /home/training/training_materials/devsh/scripts/streaming-kafka-sink.pyspark\n**Successful outcome:** \n**Before you begin:** \n**Related lessons:** \n\n---","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>About</h1>\n<p><strong>Lab:</strong> Working with Apache Kafka Streaming Messages\n<br  /><strong>Objective:</strong> Read streaming data from a Kafka topic and send data to a Kafka topic.\n<br  /><strong>File locations:</strong></p>\n<pre><code>Exercise directory: /home/training/training_materials/devsh/exercises/streaming-kafka\nTest data (local): /home/training/training_materials/devsh/data/activations_stream\nTest script: /home/training/training_materials/devsh/scripts/streamtest-kafka.sh\nSolution files: /home/training/training_materials/devsh/scripts/streaming-kafka-source.pyspark\n                /home/training/training_materials/devsh/scripts/streaming-kafka-sink.pyspark\n</code></pre>\n<p><strong>Successful outcome:</strong>\n<br  /><strong>Before you begin:</strong>\n<br  /><strong>Related lessons:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1617309750334_-959156810","id":"20181126-092644_1457476546","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:216574"},{"text":"%md\n# Setup","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Setup</h1>\n"}]},"apps":[],"jobName":"paragraph_1617309750336_-876888195","id":"20181201-044336_178705192","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216575"},{"title":"Open two terminals","text":"%md\nTerminal 1 for Kafka\nTerminal 2 for Spark","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Terminal 1 for Kafka\n<br  />Terminal 2 for Spark</p>\n"}]},"apps":[],"jobName":"paragraph_1617309750337_415454200","id":"20210124-165745_382368452","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216576"},{"text":"%md\n# Lab\n","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Lab</h1>\n"}]},"apps":[],"jobName":"paragraph_1617309750337_1475957320","id":"20181126-093358_358613711","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216577"},{"text":"%md\n### Read Messages from a Kafka Source\n\nIn this section, you will create a streaming Kafka DataFrame based on device activation Kafka messages in the `activations` topic. The message content is in \nJSON format. You will extract the required data and calculate the number of activations by model name.\n\nThe Kafka messages will be generated by a test script using the files in `/home/training/training_materials/devsh/data/activations_stream/`. \nYou may wish to review the data before proceeding.","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Read Messages from a Kafka Source</h3>\n<p>In this section, you will create a streaming Kafka DataFrame based on device activation Kafka messages in the <code>activations</code> topic. The message content is in\n<br  />JSON format. You will extract the required data and calculate the number of activations by model name.</p>\n<p>The Kafka messages will be generated by a test script using the files in <code>/home/training/training_materials/devsh/data/activations_stream/</code>.\n<br  />You may wish to review the data before proceeding.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309750338_999588333","id":"20200426-203807_1093644001","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216578"},{"title":"1 - Terminate any Spark shells","text":"%md\nIf you currently have a Spark shell running in a terminal session, exit it.","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>If you currently have a Spark shell running in a terminal session, exit it.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309750339_-1249717336","id":"20200426-203923_163702109","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216579"},{"title":"2 - In terminal 1 create a Kafka topic","text":"%md\nUse Kafka to create the `activations` topic for the streaming messages.\n\n```xml\nkafka-topics --create --bootstrap-server localhost:9092 --partitions 2 --replication-factor 1 --topic activations\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Use Kafka to create the <code>activations</code> topic for the streaming messages.</p>\n<pre><code class=\"xml\">kafka-topics --create --bootstrap-server localhost:9092 --partitions 2 --replication-factor 1 --topic activations\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750339_1117011939","id":"20200426-203953_665503242","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216580"},{"title":"3 - In terminal 2 start a new local Spark shell","text":"%md\n```\npyspark --master local[2]\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<pre><code>pyspark --master local[2]\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750339_1949335740","id":"20200426-203952_996510190","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216581"},{"title":"4 - Create a streaming DataFrame","text":"%md\nCreate a streaming DataFrame called `kafkaDF` using the following settings:\n\n- Format: `kafka`\n- `kafka.bootstrap.servers` option: `localhost:9092`\n- `subscribe` option: `activations` (topic name)\n\n```\nkafkaDF = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", \"activations\").load()\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a streaming DataFrame called <code>kafkaDF</code> using the following settings:</p>\n<ul>\n<li>Format: <code>kafka</code></li>\n<li><code>kafka.bootstrap.servers</code> option: <code>localhost:9092</code></li>\n<li><code>subscribe</code> option: <code>activations</code> (topic name)</li>\n</ul>\n<pre><code>kafkaDF = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", \"activations\").load()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750340_-2101225602","id":"20200426-203952_1826451491","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216582"},{"title":"5 - Display the schema of the DataFrame","text":"%md\nNote that the `value` column contains binary values.\n```\nkafkaDF.printSchema()\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Note that the <code>value</code> column contains binary values.</p>\n<pre><code>kafkaDF.printSchema()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750340_-1531004076","id":"20200426-203950_166689905","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216583"},{"title":"6 - Create a new DataFrame","text":"%md\nCreate a new DataFrame called `stringValueDF` containing only the value column from `kafkaDF`. The new `value` should be string type.\n\nHint: use the `cast(\"string\")` function on the value column reference.\n\n```\nstringValueDF = kafkaDF.select(kafkaDF.value.cast(\"string\"))\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame called <code>stringValueDF</code> containing only the value column from <code>kafkaDF</code>. The new <code>value</code> should be string type.</p>\n<p>Hint: use the <code>cast(\"string\")</code> function on the value column reference.</p>\n<pre><code>stringValueDF = kafkaDF.select(kafkaDF.value.cast(\"string\"))\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750341_-1939595008","id":"20200426-203948_477490279","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216584"},{"title":"7 - Confirm the column type was changed","text":"%md\nDisplay the schema to confirm that the column type was changed correctly.\n\n```\nstringValueDF.printSchema()\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Display the schema to confirm that the column type was changed correctly.</p>\n<pre><code>stringValueDF.printSchema()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750341_1620978479","id":"20200426-203947_1010888942","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216585"},{"title":"8 - Create a schema of column objects","text":"%md\nCreate a new DataFrame called `activationsDF` with a single column called `activation`. The column should contain sub-columns for the four values in the \nactivation JSON records: `acct_num`, `dev_id`, `phone`, and `model`.\n\nCreate a schema to map the JSON values to columns.","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame called <code>activationsDF</code> with a single column called <code>activation</code>. The column should contain sub-columns for the four values in the\n<br  />activation JSON records: <code>acct_num</code>, <code>dev_id</code>, <code>phone</code>, and <code>model</code>.</p>\n<p>Create a schema to map the JSON values to columns.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309750342_2104689771","id":"20200426-203946_1670977750","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216586"},{"text":"%pyspark\n\nfrom pyspark.sql.types import *\n\nactivationsSchema = StructType([\n    StructField(\"acct_num\", IntegerType()),\n    StructField(\"dev_id\", StringType()),\n    StructField(\"phone\", StringType()),\n    StructField(\"model\", StringType())])","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617309750342_-566591296","id":"20200426-203945_2047118956","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216587"},{"title":"8 - Create a new DataFrame from the new schema","text":"%md\nUse the `from_json` function to parse the values in the `value` column to the schema above.","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Use the <code>from_json</code> function to parse the values in the <code>value</code> column to the schema above.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309750343_1496338818","id":"20200426-203944_240993201","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216588"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import *\n\nactivationsDF = stringValueDF. \\\nselect(from_json(stringValueDF.value,\nactivationsSchema).\nalias(\"activation\"))","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617309750343_615996620","id":"20200426-203944_1147612167","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216589"},{"title":"9 - Confirm the new DataFrame's schema is correct","text":"%md\nView the new DataFrames's schema to confirm that it is correct.","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>View the new DataFrames's schema to confirm that it is correct.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309750344_-1167008141","id":"20200426-203943_532597384","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216590"},{"title":"10 - Create and start a streaming query","text":"%md\nCreate and start a streaming query based on `activationsDF`. Display the output to the console using output mode `append`. The elements in the DataFrame are \nlonger than the default output, so set the `truncate` option to `false`.","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create and start a streaming query based on <code>activationsDF</code>. Display the output to the console using output mode <code>append</code>. The elements in the DataFrame are\n<br  />longer than the default output, so set the <code>truncate</code> option to <code>false</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309750344_1910916467","id":"20200426-203928_558847417","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216591"},{"title":"11 - In terminal 1 test the query output","text":"%md\nTest the query output using the provided test script. Run the script in a separate terminal window (not running the Spark shell).\n\n```\n    /home/training/training_materials/devsh/scripts/streamtest-kafka.sh activations \\\n    localhost:9092 10 /home/training/training_materials/devsh/data/activations_stream\n```\n\nThis command produces Kafka messages based on the JSON lines in `/user/zeppelin/activations_stream/`. It sends the messages to the Kafka broker running \non `localhost` on the `activations` topic, 10 messages per second.\n\nWhen prompted, confirm that the script settings are correct.","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Test the query output using the provided test script. Run the script in a separate terminal window (not running the Spark shell).</p>\n<pre><code>    /home/training/training_materials/devsh/scripts/streamtest-kafka.sh activations \\\n    localhost:9092 10 /home/training/training_materials/devsh/data/activations_stream\n</code></pre>\n<p>This command produces Kafka messages based on the JSON lines in <code>/user/zeppelin/activations_stream/</code>. It sends the messages to the Kafka broker running\n<br  />on <code>localhost</code> on the <code>activations</code> topic, 10 messages per second.</p>\n<p>When prompted, confirm that the script settings are correct.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309750344_-1290331775","id":"20200426-203927_353325578","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216592"},{"title":"12 - Run the test script for several seconds","text":"%md\nThe script will display each line from the file as it sends the corresponding Kafka message. Let the script run for several seconds, then stop it using Ctrl+C.","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The script will display each line from the file as it sends the corresponding Kafka message. Let the script run for several seconds, then stop it using Ctrl+C.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309750345_471767697","id":"20200426-210541_2098834615","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216593"},{"title":"13 - Review the output displayed by the streaming query","text":"%md\nReturn to the Spark shell and review the output displayed by the streaming query. Note that each element in the result DataFrame is an array of strings -- \nthat is, the value of the `activation` column.","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Return to the Spark shell and review the output displayed by the streaming query. Note that each element in the result DataFrame is an array of strings &ndash;\n<br  />that is, the value of the <code>activation</code> column.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309750345_-217212318","id":"20200426-210541_1596990","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216594"},{"title":"14 - Stopping the query","text":"%md\nWhen you are done testing, stop the query by calling the `stop` function on the `StreamingQuery` object.","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>When you are done testing, stop the query by calling the <code>stop</code> function on the <code>StreamingQuery</code> object.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309750346_670825274","id":"20200426-210541_1127240569","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216595"},{"title":"15 - Optional: Perform additional transformations","text":"%md\n*Optional:* Try performing additional transformations on the device activation data in the Kafka message stream. For instance, try counting activations by model.","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><em>Optional:</em> Try performing additional transformations on the device activation data in the Kafka message stream. For instance, try counting activations by model.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309750346_2124066935","id":"20200426-210541_1555786742","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216596"},{"text":"%md\n### Send a Stream of Messages to a Kafka Sink\n\nIn this section, you will send Kafka messages containing information about device activations to the `activations-out` topic.","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Send a Stream of Messages to a Kafka Sink</h3>\n<p>In this section, you will send Kafka messages containing information about device activations to the <code>activations-out</code> topic.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309750347_-1002443529","id":"20200426-210541_1938942316","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216597"},{"title":"16 - Exit the Spark shell you started in the section above","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617309750347_-739571712","id":"20200426-210541_34201750","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216598"},{"title":"17 - In terminal 1 create the topic for the output messages","text":"%md\n```xml\nkafka-topics --create --bootstrap-server localhost:9092 \\\n--partitions 2 --replication-factor 1 \\\n--topic activations-out\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<pre><code class=\"xml\">kafka-topics --create --bootstrap-server localhost:9092 \\\n--partitions 2 --replication-factor 1 \\\n--topic activations-out\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750347_-1617719360","id":"20200426-210540_728715783","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216599"},{"title":"18 - Start a Kafka consumer","text":"%md\nStart a Kafka consumer running on the command line to test the output you will produce below. Leave the consumer application running. You will return later \nto confirm your Kafka message output.\n\n```xml\nkafka-console-consumer \\\n    --bootstrap-server localhost:9092 \\\n    --topic activations-out \n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Start a Kafka consumer running on the command line to test the output you will produce below. Leave the consumer application running. You will return later\n<br  />to confirm your Kafka message output.</p>\n<pre><code class=\"xml\">kafka-console-consumer \\\n    --bootstrap-server localhost:9092 \\\n    --topic activations-out \n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750348_-719668364","id":"20200426-210540_732200048","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216600"},{"title":"19 - In terminal 1 restart the shell","text":"%md\n```\n$ pyspark --master local[2]\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<pre><code>$ pyspark --master local[2]\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750348_-516492977","id":"20200426-210540_1848021454","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216601"},{"title":"20 - Create the schema for the streaming DataFrame","text":"%md\nThe test data files for this section are the same ones you use in the previous section, containing JSON records with device activation data. You will simulate \nincoming streaming data by reading the existing JSON files in the data directory, one file per micro-batch trigger.\n\nCreate a schema to map the JSON values to columns.\n\n```pyspark\nfrom pyspark.sql.types import *\n\nactivationsSchema = StructType([\n    StructField(\"acct_num\", IntegerType()),\n    StructField(\"dev_id\", StringType()),\n    StructField(\"phone\", StringType()),\n    StructField(\"model\", StringType())])\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The test data files for this section are the same ones you use in the previous section, containing JSON records with device activation data. You will simulate\n<br  />incoming streaming data by reading the existing JSON files in the data directory, one file per micro-batch trigger.</p>\n<p>Create a schema to map the JSON values to columns.</p>\n<pre><code class=\"pyspark\">from pyspark.sql.types import *\n\nactivationsSchema = StructType([\n    StructField(\"acct_num\", IntegerType()),\n    StructField(\"dev_id\", StringType()),\n    StructField(\"phone\", StringType()),\n    StructField(\"model\", StringType())])\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750349_-1214306120","id":"20200426-210540_841145454","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216602"},{"title":"21 - Create a simulated streaming DataFrame","text":"%md\n\nCreate a simulated streaming DataFrame based on the JSON activation data, using the schema you created above.\n\n```pyspark\nactivationsDF = spark.readStream. \\\nschema(activationsSchema). \\\noption(\"maxFilesPerTrigger\",1). \\\njson(\"file:///home/devuser/data/activations_stream/\")\n```\n","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a simulated streaming DataFrame based on the JSON activation data, using the schema you created above.</p>\n<pre><code class=\"pyspark\">activationsDF = spark.readStream. \\\nschema(activationsSchema). \\\noption(\"maxFilesPerTrigger\",1). \\\njson(\"file:///home/devuser/data/activations_stream/\")\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750350_1313852405","id":"20200426-210539_1086808636","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216603"},{"title":"22 - Transform the input streaming DataFrame","text":"%md\nTransform the input streaming DataFrame into a new DataFrame with the correct schema and content:\n\n- A `key` column for which all rows contain an empty string (\"\")\n    - Hint: use the Spark SQL `lit(`*literal*`)` function to specify a column with a literal value\n- A `value` string column containing the account number and device ID separated by a comma.\n    - Hint: use the Spark SQL `concat_ws(`*separator*`,` *columns...*`)` function create the comma-delimited string","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Transform the input streaming DataFrame into a new DataFrame with the correct schema and content:</p>\n<ul>\n<li>A <code>key</code> column for which all rows contain an empty string (&ldquo;&ldquo;)<ul>\n<li>Hint: use the Spark SQL <code>lit(</code><em>literal</em><code>)</code> function to specify a column with a literal value</li>\n</ul>\n</li>\n<li>A <code>value</code> string column containing the account number and device ID separated by a comma.<ul>\n<li>Hint: use the Spark SQL <code>concat_ws(</code><em>separator</em><code>,</code> <em>columns&hellip;</em><code>)</code> function create the comma-delimited string</li>\n</ul>\n</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1617309750350_-433916877","id":"20200426-210538_261794404","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216604"},{"title":"23 - Create and start a query to produces Kafka messages","text":"%md\nThe query should\n- Use format `kafka`\n- Set the `checkpointLocation` option to `/tmp/kafka-checkpoint`\n- Set the `kafka.bootstrap.servers` to `localhost:9092`\n- Set the `topic` option to `activations-out`\n\n**Note:** If you need to run the query above multiple times, be sure to remove thecheckpoint directory above before rerunning it.\n```\n    $ rm -rf /tmp/kafka-checkpoint\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The query should</p>\n<ul>\n<li>Use format <code>kafka</code></li>\n<li>Set the <code>checkpointLocation</code> option to <code>/tmp/kafka-checkpoint</code></li>\n<li>Set the <code>kafka.bootstrap.servers</code> to <code>localhost:9092</code></li>\n<li>Set the <code>topic</code> option to <code>activations-out</code></li>\n</ul>\n<p><strong>Note:</strong> If you need to run the query above multiple times, be sure to remove thecheckpoint directory above before rerunning it.</p>\n<pre><code>    $ rm -rf /tmp/kafka-checkpoint\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750351_767743837","id":"20200426-210537_37861484","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216605"},{"title":"24 - Confirm that the messages are being received correctly","text":"%md\nThe query should start generating Kafka messages immediately. Return to the window where you started the console consumer to confirm that the messages are \nbeing received and have the correct format.","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The query should start generating Kafka messages immediately. Return to the window where you started the console consumer to confirm that the messages are\n<br  />being received and have the correct format.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309750351_1526272885","id":"20200426-210535_991493700","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216606"},{"title":"25 - Stopping the query","text":"%md\nWhen you are done testing, exit the consumer application using `Ctrl+C`. Then stop the query using the `stop` function.","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617309750352_-1273603614","id":"20200426-203926_711291629","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216607"},{"text":"%md\n# Result\n**You have now:** \n\n---","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Result</h1>\n<p><strong>You have now:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1617309750352_-1174999491","id":"20181126-133507_1472573213","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216608"},{"text":"%md\n# Solution\n---","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Solution</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1617309750352_-167397890","id":"20181018-125200_1133281582","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216609"},{"text":"%md\n### Read Messages from a Kafka Source\n\nIn this section, you will create a streaming Kafka DataFrame based on device activation Kafka messages in the `activations` topic. The message content is \nin JSON format. You will extract the required data and calculate the number of activations by model name.\n\nThe Kafka messages will be generated by a test script using the files in `/home/training/training_materials/devsh/data/activations_stream/`. \nYou may wish to review the data before proceeding.","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Read Messages from a Kafka Source</h3>\n<p>In this section, you will create a streaming Kafka DataFrame based on device activation Kafka messages in the <code>activations</code> topic. The message content is\n<br  />in JSON format. You will extract the required data and calculate the number of activations by model name.</p>\n<p>The Kafka messages will be generated by a test script using the files in <code>/home/training/training_materials/devsh/data/activations_stream/</code>.\n<br  />You may wish to review the data before proceeding.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309750353_-166575478","id":"20200429-224937_1880357458","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216610"},{"title":"1 - In terminal 1 create a topic","text":"%md\n```sh\nkafka-topics --list --bootstrap-server localhost:9092\nkafka-topics --create --bootstrap-server localhost:9092 --partitions 2 --replication-factor 1 --topic activations\n```\n","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<pre><code class=\"sh\">kafka-topics --list --bootstrap-server localhost:9092\nkafka-topics --create --bootstrap-server localhost:9092 --partitions 2 --replication-factor 1 --topic activations\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750353_-1049919144","id":"20200429-225016_359372993","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216611"},{"title":"3 - In terminal 2 start a new local Spark shell","text":"%md\n```pyspark\npyspark --master local[2]\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<pre><code class=\"pyspark\">pyspark --master local[2]\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750354_-1218925317","id":"20200429-225015_615463066","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216612"},{"title":"4 - Create a streaming DataFrame","text":"%md\nCreate a streaming DataFrame called `kafkaDF` using the following settings:\n\n- Format: `kafka`\n- `kafka.bootstrap.servers` option: `localhost:9092`\n- `subscribe` option: `activations` (topic name)\n\n```pyspark\nkafkaDF = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", \"activations\").load()\n```","user":"anonymous","dateUpdated":"2021-04-01T15:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a streaming DataFrame called <code>kafkaDF</code> using the following settings:</p>\n<ul>\n<li>Format: <code>kafka</code></li>\n<li><code>kafka.bootstrap.servers</code> option: <code>localhost:9092</code></li>\n<li><code>subscribe</code> option: <code>activations</code> (topic name)</li>\n</ul>\n<pre><code class=\"pyspark\">kafkaDF = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", \"activations\").load()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750354_-270524416","id":"20200429-225013_613497445","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216613"},{"title":"5 - Display the schema of the DataFrame","text":"%md\nNote that the `value` column contains binary values.\n\n```pyspark\nkafkaDF.printSchema()\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Note that the <code>value</code> column contains binary values.</p>\n<pre><code class=\"pyspark\">kafkaDF.printSchema()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750355_150443207","id":"20200429-225010_1354703023","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216614"},{"title":"6 - Create a new DataFrame","text":"%md\nCreate a new DataFrame called `stringValueDF` containing only the value column from `kafkaDF`. The new `value` should be string type.\n\nHint: use the `cast(\"string\")` function on the value column reference.\n\n```pyspark\nstringValueDF = kafkaDF.select(kafkaDF.value.cast(\"string\"))\n```","user":"anonymous","dateUpdated":"2021-04-01T15:45:03-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame called <code>stringValueDF</code> containing only the value column from <code>kafkaDF</code>. The new <code>value</code> should be string type.</p>\n<p>Hint: use the <code>cast(\"string\")</code> function on the value column reference.</p>\n<pre><code class=\"pyspark\">stringValueDF = kafkaDF.select(kafkaDF.value.cast(\"string\"))\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750355_1335978280","id":"20200429-225009_10700973","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216615"},{"title":"7 - Confirm the column type was changed","text":"%md\nPrint out the schema\n\n```pyspark\nstringValueDF.printSchema()\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Print out the schema</p>\n<pre><code class=\"pyspark\">stringValueDF.printSchema()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750356_-120592339","id":"20200429-225008_660411338","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216616"},{"title":"8 - Create a schema of column objects","text":"%md\nCreate a new DataFrame called `activationsDF` with a single column called `activation`. The column should contain sub-columns for the four values in the \nactivation JSON records: `acct_num`, `dev_id`, `phone`, and `model`.\n\nCreate a schema of column objects to map the JSON values to columns.\n\n```pyspark\nfrom pyspark.sql.types import *\n\nactivationsSchema = StructType([\n  StructField(\"acct_num\", IntegerType()),\n  StructField(\"dev_id\", StringType()),\n  StructField(\"phone\", StringType()),\n  StructField(\"model\", StringType())])\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame called <code>activationsDF</code> with a single column called <code>activation</code>. The column should contain sub-columns for the four values in the\n<br  />activation JSON records: <code>acct_num</code>, <code>dev_id</code>, <code>phone</code>, and <code>model</code>.</p>\n<p>Create a schema of column objects to map the JSON values to columns.</p>\n<pre><code class=\"pyspark\">from pyspark.sql.types import *\n\nactivationsSchema = StructType([\n  StructField(\"acct_num\", IntegerType()),\n  StructField(\"dev_id\", StringType()),\n  StructField(\"phone\", StringType()),\n  StructField(\"model\", StringType())])\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750356_-1788554990","id":"20200429-225007_367971138","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216617"},{"title":"8 - Create a new DataFrame with the new schema","text":"%md\n```pyspark\nfrom pyspark.sql.functions import *\n\nactivationsDF = stringValueDF.select(from_json(stringValueDF.value, activationsSchema).alias(\"activation\"))\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<pre><code class=\"pyspark\">from pyspark.sql.functions import *\n\nactivationsDF = stringValueDF.select(from_json(stringValueDF.value, activationsSchema).alias(\"activation\"))\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750357_102702262","id":"20200429-225006_1594579468","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216618"},{"title":"9 - Confirm the new DataFrame's schema is correct","text":"%md\nView the new DataFrames's schema to confirm that it is correct.\n\n```pyspark\nactivationsDF.printSchema()\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>View the new DataFrames's schema to confirm that it is correct.</p>\n<pre><code class=\"pyspark\">activationsDF.printSchema()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750357_-4585340","id":"20200429-225005_1171313932","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216619"},{"title":"10 - Create and start a streaming query","text":"%md\nCreate and start a streaming query based on `activationsDF`. Display the output to the console using output mode `append`. The elements in the DataFrame are \nlonger than the default output, so set the `truncate` option to `false`.\n\n```pyspark\nactivationsQuery = activationsDF.writeStream.outputMode(\"append\").option(\"truncate\",\"false\").format(\"console\").start()\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create and start a streaming query based on <code>activationsDF</code>. Display the output to the console using output mode <code>append</code>. The elements in the DataFrame are\n<br  />longer than the default output, so set the <code>truncate</code> option to <code>false</code>.</p>\n<pre><code class=\"pyspark\">activationsQuery = activationsDF.writeStream.outputMode(\"append\").option(\"truncate\",\"false\").format(\"console\").start()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750358_1004118490","id":"20200429-225003_190569295","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216620"},{"title":"11 - In terminal 1 test the query output","text":"%md\nTest the query output using the provided test script. Run the script in a separate terminal window (not running the Spark shell).\n\n```\n    /home/training/training_materials/devsh/scripts/streamtest-kafka.sh activations \\\n    localhost:9092 10 /home/training/training_materials/devsh/data/activations_stream\n```\n\nThis command produces Kafka messages based on the JSON lines in \n`/home/training/training_materials/devsh/data/activations_stream/`. It sends the messages to the Kafka broker running on `localhost` on the `activations` \ntopic, 10 messages per second.\n\nWhen prompted, confirm that the script settings are correct.\n","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Test the query output using the provided test script. Run the script in a separate terminal window (not running the Spark shell).</p>\n<pre><code>    /home/training/training_materials/devsh/scripts/streamtest-kafka.sh activations \\\n    localhost:9092 10 /home/training/training_materials/devsh/data/activations_stream\n</code></pre>\n<p>This command produces Kafka messages based on the JSON lines in\n<br  /><code>/home/training/training_materials/devsh/data/activations_stream/</code>. It sends the messages to the Kafka broker running on <code>localhost</code> on the <code>activations</code>\n<br  />topic, 10 messages per second.</p>\n<p>When prompted, confirm that the script settings are correct.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309750358_-1277343965","id":"20210122-151638_630102604","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216621"},{"title":"12 - Run the test script for several seconds","text":"%md\nThe script will display each line from the file as it sends the corresponding Kafka message. Let the script run for several seconds, then stop it using Ctrl+C.","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The script will display each line from the file as it sends the corresponding Kafka message. Let the script run for several seconds, then stop it using Ctrl+C.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309750359_-62104536","id":"20200429-225002_1467321867","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216622"},{"title":"13 - Review the output displayed by the streaming query","text":"%md\nReturn to the Spark shell and review the output displayed by the streaming query. Note that each element in the result DataFrame is an array of strings -- \nthat is, the value of the `activation` column.","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Return to the Spark shell and review the output displayed by the streaming query. Note that each element in the result DataFrame is an array of strings &ndash;\n<br  />that is, the value of the <code>activation</code> column.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309750359_-2050168842","id":"20200429-225001_1997974606","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216623"},{"title":"14 - Stopping the query","text":"%md\nWhen you are done testing, stop the query by calling the `stop` function on the `StreamingQuery` object.\n\n```pyspark\nactivationsQuery.stop()\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>When you are done testing, stop the query by calling the <code>stop</code> function on the <code>StreamingQuery</code> object.</p>\n<pre><code class=\"pyspark\">activationsQuery.stop()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750360_2031346840","id":"20200429-225001_1085492957","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216624"},{"title":"15 - Optional: Perform additional transformations","text":"%md\nTry performing additional transformations on the device activation data in the Kafka message stream. For instance, try counting activations by model.\n\n```pyspark\n# Count activations by model\ncountByModelDF = activationsDF.groupBy(\"activation.model\").count()\n# count aggregation only supported with complete output mode\ncountByModelQuery = countByModelDF.writeStream.outputMode(\"complete\").format(\"console\").start()\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Try performing additional transformations on the device activation data in the Kafka message stream. For instance, try counting activations by model.</p>\n<pre><code class=\"pyspark\"># Count activations by model\ncountByModelDF = activationsDF.groupBy(\"activation.model\").count()\n# count aggregation only supported with complete output mode\ncountByModelQuery = countByModelDF.writeStream.outputMode(\"complete\").format(\"console\").start()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750360_1809535879","id":"20200429-225000_1716593565","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216625"},{"text":"%md\n","user":"anonymous","dateUpdated":"2021-04-01T15:57:00-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617317820815_-1475312754","id":"20210401-155700_1867837373","dateCreated":"2021-04-01T15:57:00-0700","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:216626"},{"title":"16 - Stop the query and quit the Spark shell","text":"%md\n\n```pyspark\ncountByModelQuery.stop()\nquit()\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<pre><code class=\"pyspark\">countByModelQuery.stop()\nquit()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750361_-1407770231","id":"20200429-224958_1282131172","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216627"},{"text":"%md\n### Send a Stream of Messages to a Kafka Sink\n\nIn this section, you will send Kafka messages containing information about device activations to the `activations-out` topic.\n","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Send a Stream of Messages to a Kafka Sink</h3>\n<p>In this section, you will send Kafka messages containing information about device activations to the <code>activations-out</code> topic.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309750361_279427934","id":"20210122-152019_1335014867","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216628"},{"title":"17 - In terminal 1 create the topic for the output messages","text":"%md\nCreate a topic activations-out\n\n~~~xml\nkafka-topics --create --bootstrap-server localhost:9092 \\\n--partitions 2 --replication-factor 1 \\\n--topic activations-out\n\nkafka-topics --list --bootstrap-server localhost:9092\n~~~","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a topic activations-out</p>\n<pre><code class=\"xml\">kafka-topics --create --bootstrap-server localhost:9092 \\\n--partitions 2 --replication-factor 1 \\\n--topic activations-out\n\nkafka-topics --list --bootstrap-server localhost:9092\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750362_1892939160","id":"20200429-224956_1476701879","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216629"},{"title":"18 - Start a Kafka consumer","text":"%md\nStart a Kafka consumer running on the command line to test the output you will produce below. Leave the consumer application running. You will return later \nto confirm your Kafka message output.\n\n```xml\nkafka-console-consumer --topic activations-out \\\n--bootstrap-server localhost:9092\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Start a Kafka consumer running on the command line to test the output you will produce below. Leave the consumer application running. You will return later\n<br  />to confirm your Kafka message output.</p>\n<pre><code class=\"xml\">kafka-console-consumer --topic activations-out \\\n--bootstrap-server localhost:9092\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750362_-922210169","id":"20200429-224954_879965496","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216630"},{"title":"19 - In terminal 2 restart the Pyspark shell","text":"%md\n```\npyspark --master local[2]\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<pre><code>pyspark --master local[2]\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750363_1951836380","id":"20210122-152251_1237107482","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216631"},{"title":"20 - Create the schema for the streaming DataFrame","text":"%md\nThe test data files for this section are the same ones you use in the previous section, containing JSON records with device activation data. You will simulate \nincoming streaming data by reading the existing JSON files in the data directory, one file per micro-batch trigger.\n\nCreate a schema to map the JSON values to columns.\n\n```pyspark\nfrom pyspark.sql.types import *\n\nactivationsSchema = StructType([\n  StructField(\"acct_num\", IntegerType()),\n  StructField(\"dev_id\", StringType()),\n  StructField(\"phone\", StringType()),\n  StructField(\"model\", StringType())])\n  ```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The test data files for this section are the same ones you use in the previous section, containing JSON records with device activation data. You will simulate\n<br  />incoming streaming data by reading the existing JSON files in the data directory, one file per micro-batch trigger.</p>\n<p>Create a schema to map the JSON values to columns.</p>\n<pre><code class=\"pyspark\">from pyspark.sql.types import *\n\nactivationsSchema = StructType([\n  StructField(\"acct_num\", IntegerType()),\n  StructField(\"dev_id\", StringType()),\n  StructField(\"phone\", StringType()),\n  StructField(\"model\", StringType())])\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750363_-1991943036","id":"20200429-231545_221624480","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216632"},{"title":"21 - Create a simulated streaming DataFrame","text":"%md\nCreate a simulated streaming DataFrame based on the JSON activation data, using the schema you created above.\n\n```pyspark\nactivationsDF = spark.readStream.schema(activationsSchema).option(\"maxFilesPerTrigger\",1).json(\"file:///home/training/training_materials/devsh/data/activations_stream/\")\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a simulated streaming DataFrame based on the JSON activation data, using the schema you created above.</p>\n<pre><code class=\"pyspark\">activationsDF = spark.readStream.schema(activationsSchema).option(\"maxFilesPerTrigger\",1).json(\"file:///home/training/training_materials/devsh/data/activations_stream/\")\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750364_-1064454648","id":"20200429-231543_1403219086","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216633"},{"title":"22 - Transform the input streaming DataFrame","text":"%md\nTransform the input streaming DataFrame into a new DataFrame with the correct schema and content:\n\n- A `key` column for which all rows contain an empty string (\"\")\n    - Hint: use the Spark SQL `lit(`*literal*`)` function to specify a column with a literal value\n- A `value` string column containing the account number and device ID separated by a comma.\n\nHint: use the Spark SQL `concat_ws(`*separator*`,` *columns...*`)` function create the comma-delimited string\n\n```pyspark\nfrom pyspark.sql.functions import *\n\nactivationsValueDF = activationsDF.select(lit(\"\").alias(\"key\"),concat_ws(\",\",activationsDF.acct_num,activationsDF.dev_id).alias(\"value\"))\n\nactivationsValueDF.printSchema()\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Transform the input streaming DataFrame into a new DataFrame with the correct schema and content:</p>\n<ul>\n<li>A <code>key</code> column for which all rows contain an empty string (&ldquo;&ldquo;)<ul>\n<li>Hint: use the Spark SQL <code>lit(</code><em>literal</em><code>)</code> function to specify a column with a literal value</li>\n</ul>\n</li>\n<li>A <code>value</code> string column containing the account number and device ID separated by a comma.</li>\n</ul>\n<p>Hint: use the Spark SQL <code>concat_ws(</code><em>separator</em><code>,</code> <em>columns&hellip;</em><code>)</code> function create the comma-delimited string</p>\n<pre><code class=\"pyspark\">from pyspark.sql.functions import *\n\nactivationsValueDF = activationsDF.select(lit(\"\").alias(\"key\"),concat_ws(\",\",activationsDF.acct_num,activationsDF.dev_id).alias(\"value\"))\n\nactivationsValueDF.printSchema()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750365_1115489739","id":"20200429-231541_612551067","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216634"},{"title":"23 - Create and start a query that produces Kafka messages","text":"%md\nThe query should\n- Use format `kafka`\n- Set the `checkpointLocation` option to `/tmp/kafka-checkpoint`\n- Set the `kafka.bootstrap.servers` to `localhost:9092`\n- Set the `topic` option to `activations-out`\n\n**Note:** If you need to run the query above multiple times, be sure to remove the checkpoint directory above before re-running it.\n\n```xml\nrm -rf /tmp/kafka-checkpoint\n```\n**Note:** if you rerun query be sure to clear out checkpoint location directory\n```pyspark\nactivationsKafkaQuery = activationsValueDF.writeStream.format(\"kafka\").option(\"checkpointLocation\",\"/tmp/kafka-checkpoint\").option(\"topic\",\"activations-out\").option(\"kafka.bootstrap.servers\",\"localhost:9092\").start()\n```\n","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The query should</p>\n<ul>\n<li>Use format <code>kafka</code></li>\n<li>Set the <code>checkpointLocation</code> option to <code>/tmp/kafka-checkpoint</code></li>\n<li>Set the <code>kafka.bootstrap.servers</code> to <code>localhost:9092</code></li>\n<li>Set the <code>topic</code> option to <code>activations-out</code></li>\n</ul>\n<p><strong>Note:</strong> If you need to run the query above multiple times, be sure to remove the checkpoint directory above before re-running it.</p>\n<pre><code class=\"xml\">rm -rf /tmp/kafka-checkpoint\n</code></pre>\n<p><strong>Note:</strong> if you rerun query be sure to clear out checkpoint location directory</p>\n<pre><code class=\"pyspark\">activationsKafkaQuery = activationsValueDF.writeStream.format(\"kafka\").option(\"checkpointLocation\",\"/tmp/kafka-checkpoint\").option(\"topic\",\"activations-out\").option(\"kafka.bootstrap.servers\",\"localhost:9092\").start()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750365_565285602","id":"20200429-231945_1868607638","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216635"},{"title":"24 - Confirm that the messages are being received correctly","text":"%md\nThe query should start generating Kafka messages immediately. Return to the window where you started the console consumer to confirm that the messages are \nbeing received and have the correct format.","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The query should start generating Kafka messages immediately. Return to the window where you started the console consumer to confirm that the messages are\n<br  />being received and have the correct format.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309750366_438278669","id":"20200429-232115_809190101","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216636"},{"title":"25 - Stopping the query","text":"%md\nWhen you are done testing, exit the consumer application using `Ctrl+C`. Then stop the query using the `stop` function.\n\n```pyspark\nactivationsKafkaQuery.stop()\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>When you are done testing, exit the consumer application using <code>Ctrl+C</code>. Then stop the query using the <code>stop</code> function.</p>\n<pre><code class=\"pyspark\">activationsKafkaQuery.stop()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750366_2067520316","id":"20200429-232112_1285256751","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216637"},{"text":"%md\n# Tear Down\n---\n","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Tear Down</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1617309750367_-654270733","id":"20210122-005733_1474143321","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216638"},{"title":"Quit the Spark shell","text":"%md\nExit the Spark shell.\n\n```pyspark\nquit()\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Exit the Spark shell.</p>\n<pre><code class=\"pyspark\">quit()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750367_2039946143","id":"20210207-103921_1615063006","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216639"},{"title":"Exit terminals","text":"%md\n```xml\nexit\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<pre><code class=\"xml\">exit\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309750367_409521460","id":"20210124-175135_366489100","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216640"},{"title":"Additional resources","text":"%md\nWe hope you've enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Cloudera Tutorials](http://cloudera.com/tutorials.html) are your natural next step where you can explore Spark in more depth.\n2. [Cloudera Community](https://community.cloudera.com) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Apache Spark Documentation](https://spark.apache.org/documentation.html) - official Spark documentation.\n4. [Apache Zeppelin Project Home Page](https://zeppelin.apache.org) - official Zeppelin web site.\n","user":"anonymous","dateUpdated":"2021-04-01T13:42:30-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We hope you've enjoyed this lab. Below are additional resources that you should find useful:</p>\n<ol>\n<li><a href=\"http://cloudera.com/tutorials.html\">Cloudera Tutorials</a> are your natural next step where you can explore Spark in more depth.</li>\n<li><a href=\"https://community.cloudera.com\">Cloudera Community</a> is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.</li>\n<li><a href=\"https://spark.apache.org/documentation.html\">Apache Spark Documentation</a> - official Spark documentation.</li>\n<li><a href=\"https://zeppelin.apache.org\">Apache Zeppelin Project Home Page</a> - official Zeppelin web site.</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1617309750368_-1568338487","id":"20181126-133017_244739700","dateCreated":"2021-04-01T13:42:30-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216641"}],"name":"/DevSH/Pyspark/StreamWithKafka","id":"2G1NRTE8Q","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"angular:shared_process":[],"sh:shared_process":[],"livy:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}