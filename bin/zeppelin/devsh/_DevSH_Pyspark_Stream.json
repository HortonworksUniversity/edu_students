{"paragraphs":[{"text":"%md\n# About\n**Lab:** Processing Streaming Data\n**Objective:** Read and process streaming data from a set of files.\n**File locations:**\n    Exercise directory: /home/training/training_materials/devsh/exercises/streaming\n    Test data (local): /home/training/training_materials/devsh/data/activations_stream/\n    Test script: /home/training/training_materialls/devsh/scripts/streamtest-file.sh\n**Successful outcome:** \n**Before you begin:** \n**Related lessons:** \n\n---","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>About</h1>\n<p><strong>Lab:</strong> Processing Streaming Data\n<br  /><strong>Objective:</strong> Read and process streaming data from a set of files.\n<br  /><strong>File locations:</strong></p>\n<pre><code>Exercise directory: /home/training/training_materials/devsh/exercises/streaming\nTest data (local): /home/training/training_materials/devsh/data/activations_stream/\nTest script: /home/training/training_materialls/devsh/scripts/streamtest-file.sh\n</code></pre>\n<p><strong>Successful outcome:</strong>\n<br  /><strong>Before you begin:</strong>\n<br  /><strong>Related lessons:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1617309765328_1630542199","id":"20181126-092644_1457476546","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:259780"},{"text":"%md\n# Setup","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Setup</h1>\n"}]},"apps":[],"jobName":"paragraph_1617309765329_-1781148642","id":"20181201-044336_178705192","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259781"},{"title":"Open Two Terminals","text":"%md\nTerminal 1 for file commands\nTerminal 2 for Spark","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Terminal 1 for file commands\n<br  />Terminal 2 for Spark</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765330_1439853478","id":"20210124-141349_1595199340","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259782"},{"text":"%md\n# Lab\n","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Lab</h1>\n"}]},"apps":[],"jobName":"paragraph_1617309765330_1193289220","id":"20181126-093358_358613711","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259783"},{"text":"%md\n### Display Streaming Data to the Console\n\nIn this section, you will read data from a file-based stream and display the results to the console. The query in this section is very simple -- it does not \ntransform the data, and simply outputs the data it receives \"as-is.\"\"","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Display Streaming Data to the Console</h3>\n<p>In this section, you will read data from a file-based stream and display the results to the console. The query in this section is very simple &ndash; it does not\n<br  />transform the data, and simply outputs the data it receives &ldquo;as-is.&ldquo;&rdquo;</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765331_1551245884","id":"20200426-194011_597040821","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259784"},{"title":"1 - Review the test data you will be using in this exercise","text":"%md\nIt contains information about device activations on Loudacre's cellular network in JSON format.\nThe data is in `/home/training/training_materials/devsh/data/activations_stream/`.\nYou will use these files later to simulate a stream of JSON data by running a script that copies the files in one at a time.","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>It contains information about device activations on Loudacre's cellular network in JSON format.\n<br  />The data is in <code>/home/training/training_materials/devsh/data/activations_stream/</code>.\n<br  />You will use these files later to simulate a stream of JSON data by running a script that copies the files in one at a time.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765331_1116373712","id":"20200426-194056_111072930","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259785"},{"title":"2 - In terminal 1 set up a local directory to contain the data files that Spark will read","text":"%md\nCreate a directory `/tmp/devsh-streaming` to load the streaming data. Set the file permissions to allow your application to access the files. Do not copy any \ndata into the directory yet.\n\n```\nmkdir -p /tmp/devsh-streaming\nchmod go+wr /tmp/devsh-streaming\n```\n\n**Note:** The directory from which Spark will load data must exist before the you create the DataFrame based on the data.","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a directory <code>/tmp/devsh-streaming</code> to load the streaming data. Set the file permissions to allow your application to access the files. Do not copy any\n<br  />data into the directory yet.</p>\n<pre><code>mkdir -p /tmp/devsh-streaming\nchmod go+wr /tmp/devsh-streaming\n</code></pre>\n<p><strong>Note:</strong> The directory from which Spark will load data must exist before the you create the DataFrame based on the data.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765332_317740385","id":"20200426-194111_1243893916","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259786"},{"title":"3 - Prepare a shell to run a streaming application","text":"%md\nIf you currenty have a Spark shell running in a terminal session, exit it.\n\nStart a new Python or Scala Spark shell running on the cluster.\n\n```\npyspark --master local[2]\n```\n\nEnter the code in the following paragraphs into your Spark shell.","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>If you currenty have a Spark shell running in a terminal session, exit it.</p>\n<p>Start a new Python or Scala Spark shell running on the cluster.</p>\n<pre><code>pyspark --master local[2]\n</code></pre>\n<p>Enter the code in the following paragraphs into your Spark shell.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765332_-966366028","id":"20200426-194110_1702903882","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259787"},{"title":"4 - Define a schema for the structure of the input data","text":"%md\n```\ninputDir = \"file:/tmp/devsh-streaming/\"\n\nfrom pyspark.sql.types import *\n\nactivationsSchema = StructType([\nStructField(\"acct_num\", IntegerType()),\nStructField(\"dev_id\", StringType()),\nStructField(\"phone\", StringType()),\nStructField(\"model\", StringType())])\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<pre><code>inputDir = \"file:/tmp/devsh-streaming/\"\n\nfrom pyspark.sql.types import *\n\nactivationsSchema = StructType([\nStructField(\"acct_num\", IntegerType()),\nStructField(\"dev_id\", StringType()),\nStructField(\"phone\", StringType()),\nStructField(\"model\", StringType())])\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765333_-307121981","id":"20200426-194109_271714022","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259788"},{"text":"%md\n","user":"anonymous","dateUpdated":"2021-04-01T14:02:38-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617310958276_-424672494","id":"20210401-140238_1052940661","dateCreated":"2021-04-01T14:02:38-0700","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:259789"},{"title":"5 - Create a streaming DataFrame by reading the data you reviewed above","text":"%md\n```\nactivationsDF = spark.readStream. \\\nschema(activationsSchema). \\\njson(\"file:/tmp/devsh-streaming/\")\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<pre><code>activationsDF = spark.readStream. \\\nschema(activationsSchema). \\\njson(\"file:/tmp/devsh-streaming/\")\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765333_1141699252","id":"20200426-194108_1321483966","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259790"},{"title":"6 - Display the streaming DataFrame's schema to confirm that it is set up correctly","text":"","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617309765334_152389504","id":"20200426-194106_1144996716","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259791"},{"title":"7 - Confirm that the DataFrame's isStreaming property is set","text":"","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617309765334_-120462760","id":"20200426-194105_131508129","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259792"},{"title":"8 - Start a streaming query","text":"%md\nStart a streaming query that displays results to the console. Use append mode to display the first several records in each new input stream micro-batch.\n\nSet the `truncate` option so that you will be able to see all the data in each record.\n\n```\nactivationsQuery = activationsDF.writeStream. \\\noutputMode(\"append\"). \\\nformat(\"console\").option(\"truncate\",\"false\"). \\\nstart()\n```\n\nThe query will not display any output yet, because no files are available to read yet.","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Start a streaming query that displays results to the console. Use append mode to display the first several records in each new input stream micro-batch.</p>\n<p>Set the <code>truncate</code> option so that you will be able to see all the data in each record.</p>\n<pre><code>activationsQuery = activationsDF.writeStream. \\\noutputMode(\"append\"). \\\nformat(\"console\").option(\"truncate\",\"false\"). \\\nstart()\n</code></pre>\n<p>The query will not display any output yet, because no files are available to read yet.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765335_-1905386448","id":"20200426-194103_454623960","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259793"},{"title":"9 - In terminal 1 begin streaming data","text":"%md\nOpen a new terminal window. Run the test script to copy the test data files into the streaming directory at a rate of one per second.\n\n```\n/home/training/training_materials/devsh/scripts/streamtest-file.sh /home/training/training_materials/devsh/data/activations_stream /tmp/devsh-streaming\n```\n\nThe script will display the names of the files as it copies them.\n\n**Note:** Spark keeps track of files that have been previously read by each query. If you need to re-run the script later to test the same query, Spark will \nignore any files that were previously copied.","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Open a new terminal window. Run the test script to copy the test data files into the streaming directory at a rate of one per second.</p>\n<pre><code>/home/training/training_materials/devsh/scripts/streamtest-file.sh /home/training/training_materials/devsh/data/activations_stream /tmp/devsh-streaming\n</code></pre>\n<p>The script will display the names of the files as it copies them.</p>\n<p><strong>Note:</strong> Spark keeps track of files that have been previously read by each query. If you need to re-run the script later to test the same query, Spark will\n<br  />ignore any files that were previously copied.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765335_1818576516","id":"20200426-194101_1798043560","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259794"},{"title":"10 - Confirm that the query is displaying data","text":"%md\nReturn to your Spark shell and confirm that the query is displaying data from each batch. Note that the first batch processed and displayed will always be empty.","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Return to your Spark shell and confirm that the query is displaying data from each batch. Note that the first batch processed and displayed will always be empty.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765336_593560254","id":"20200426-195419_757368827","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259795"},{"title":"11 - Stopping the stream","text":"%md\nWhen you are done, stop the stream by entering \n\n```\nactivationsQuery.stop()\n```\n\n**Note:** You will not see a prompt while the shell is displaying output, but you can enter commands in the shell anyway.","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>When you are done, stop the stream by entering</p>\n<pre><code>activationsQuery.stop()\n</code></pre>\n<p><strong>Note:</strong> You will not see a prompt while the shell is displaying output, but you can enter commands in the shell anyway.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765336_1804699145","id":"20200426-195419_1156531890","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259796"},{"title":"12 - Stopping the test script","text":"%md\nTerminate the test script in the second terminal window using `Ctrl+C`.","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Terminate the test script in the second terminal window using <code>Ctrl+C</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765337_1339507574","id":"20200426-195418_1358698244","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259797"},{"text":"%md\n### Display Aggregated Streaming Data to the Console\n\nIn this section, you will perform a simple aggregation -- counting devices by model -- and display the results to the console.","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Display Aggregated Streaming Data to the Console</h3>\n<p>In this section, you will perform a simple aggregation &ndash; counting devices by model &ndash; and display the results to the console.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765337_1105205897","id":"20200426-195418_1158851408","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259798"},{"title":"13 - In terminal 1 clean the streaming directory","text":"%md\nIn your test script terminal window (not the Spark shell), remove the data that was copied into the streaming directory in the last section.\n\n```\nrm -rf /tmp/devsh-streaming/*\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>In your test script terminal window (not the Spark shell), remove the data that was copied into the streaming directory in the last section.</p>\n<pre><code>rm -rf /tmp/devsh-streaming/*\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765338_278466250","id":"20200426-195417_1588990866","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259799"},{"title":"14 - Create a DataFrame to count each device model","text":"%md\nGo to your Spark shell. Starting with the `activationsDF` DataFrame you created above, create a second DataFrame containing the count of each device model.\n\n```\nactivationCountDF = activationsDF. \\\ngroupBy(\"model\").count()\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Go to your Spark shell. Starting with the <code>activationsDF</code> DataFrame you created above, create a second DataFrame containing the count of each device model.</p>\n<pre><code>activationCountDF = activationsDF. \\\ngroupBy(\"model\").count()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765340_972801382","id":"20200426-195417_926762270","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259800"},{"title":"15 - Start a streaming query that aggregates the data","text":"%md\nStart a streaming query that displays the model name and count for activated devices. Use `complete` mode so that the data will be aggregated across all the \ndata in all the batches received so far for each interval, rather than just each individual batch.\n\n```\nactivationCountQuery = activationCountDF. \\\nwriteStream.outputMode(\"complete\"). \\\nformat(\"console\").start()\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Start a streaming query that displays the model name and count for activated devices. Use <code>complete</code> mode so that the data will be aggregated across all the\n<br  />data in all the batches received so far for each interval, rather than just each individual batch.</p>\n<pre><code>activationCountQuery = activationCountDF. \\\nwriteStream.outputMode(\"complete\"). \\\nformat(\"console\").start()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765340_48873068","id":"20200426-195415_1492878269","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259801"},{"title":"16 - Run the streamtest-file.sh script again","text":"%md\nReturn to your second terminal window and re-run the `streamtest-file.sh` script you ran in the previous section.","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Return to your second terminal window and re-run the <code>streamtest-file.sh</code> script you ran in the previous section.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765341_-2047281235","id":"20200426-195414_562992384","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259802"},{"title":"17 - Confirm that complete output mode is activated","text":"%md\nIn your Spark shell, make sure that the models and their counts are being displayed. Confirm that the counts are going up in each batch because you used `complete`\noutput mode.","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>In your Spark shell, make sure that the models and their counts are being displayed. Confirm that the counts are going up in each batch because you used <code>complete</code>\n<br  />output mode.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765341_-1183484043","id":"20200426-195413_572889012","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259803"},{"title":"18 - Stop the query and terminate the test script","text":"%md\nWhen you are done, stop the `activationCountQuery` query and terminate the test script.","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>When you are done, stop the <code>activationCountQuery</code> query and terminate the test script.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765342_-1345400173","id":"20200426-195412_2056098062","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259804"},{"text":"%md\n### Output Data to Files\n\nIn this section, you will run a query that outputs to a set of files.","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Output Data to Files</h3>\n<p>In this section, you will run a query that outputs to a set of files.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765342_736006692","id":"20200426-194100_1694382703","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259805"},{"title":"19 - Clean the streaming directory","text":"%md\nIn your test script terminal window (not the Spark shell), remove the data copied into the streaming directory in the last section.\n\n```\nrm -rf /tmp/devsh-streaming/*\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>In your test script terminal window (not the Spark shell), remove the data copied into the streaming directory in the last section.</p>\n<pre><code>rm -rf /tmp/devsh-streaming/*\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765342_-739624186","id":"20200426-201404_1653661450","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259806"},{"title":"20 - Create a new streaming DataFrame","text":"%md\nGo to your Spark shell. Starting with the `activationsDF` DataFrame you created previously, create a new streaming DataFrame with just the rows for the \n\"Titanic 1000\" devices and the `dev_id` and `acct_num` columns.\n\n```\ntitanic1000DF = activationsDF. \\\nwhere(\"model = 'Titanic 1000'\"). \\\nselect(\"dev_id\",\"acct_num\")\n\ntitanic1000DF.printSchema()\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Go to your Spark shell. Starting with the <code>activationsDF</code> DataFrame you created previously, create a new streaming DataFrame with just the rows for the\n<br  />&ldquo;Titanic 1000&rdquo; devices and the <code>dev_id</code> and <code>acct_num</code> columns.</p>\n<pre><code>titanic1000DF = activationsDF. \\\nwhere(\"model = 'Titanic 1000'\"). \\\nselect(\"dev_id\",\"acct_num\")\n\ntitanic1000DF.printSchema()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765343_-1441020670","id":"20200426-201404_1292067866","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259807"},{"title":"21 - Start a query that saves to HDFS","text":"%md\nStart a query that saves the data in the streaming DataFrame you just created to the `/devsh_loudacre/titanic1000/` directory in HDFS. Set the query to trigger \nevery three seconds.\n\n**a.** Checkpointing must be enabled to ensure fault tolerance when saving files. Set the checkpoint HDFS directory for the Spark session.\n\n```\nspark.conf.set(\n\"spark.sql.streaming.checkpointLocation\",\n\"/tmp/streaming-checkpoint\")\n```\n\n**b.** Start the query\n\n```\ntitanic1000Query = titanic1000DF. \\\nwriteStream.trigger(processingTime=\"3 seconds\"). \\\noutputMode(\"append\").format(\"csv\"). \\\noption(\"path\",\"/devsh_loudacre/titanic1000/\"). \\\nstart()\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Start a query that saves the data in the streaming DataFrame you just created to the <code>/devsh_loudacre/titanic1000/</code> directory in HDFS. Set the query to trigger\n<br  />every three seconds.</p>\n<p><strong>a.</strong> Checkpointing must be enabled to ensure fault tolerance when saving files. Set the checkpoint HDFS directory for the Spark session.</p>\n<pre><code>spark.conf.set(\n\"spark.sql.streaming.checkpointLocation\",\n\"/tmp/streaming-checkpoint\")\n</code></pre>\n<p><strong>b.</strong> Start the query</p>\n<pre><code>titanic1000Query = titanic1000DF. \\\nwriteStream.trigger(processingTime=\"3 seconds\"). \\\noutputMode(\"append\").format(\"csv\"). \\\noption(\"path\",\"/devsh_loudacre/titanic1000/\"). \\\nstart()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765343_-251395445","id":"20200426-201402_528992491","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259808"},{"title":"22 - Re-run the stream test script","text":"%md\nReturn to your second terminal window and re-run the `streamtest-file.sh` test script.","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Return to your second terminal window and re-run the <code>streamtest-file.sh</code> test script.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765344_1222746406","id":"20200426-202548_611263496","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259809"},{"title":"23 - Confirm the target directory is receiving the files","text":"%md\nOpen a new (third) terminal window and list the contents of the HDFS target directory: `/devsh_loudacre/titanic1000`. Take note of the number of files.\n\nList the contents again after a few seconds and notice that the number of files is growing as Spark adds new data files to the directory for each incoming microbatch received.","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Open a new (third) terminal window and list the contents of the HDFS target directory: <code>/devsh_loudacre/titanic1000</code>. Take note of the number of files.</p>\n<p>List the contents again after a few seconds and notice that the number of files is growing as Spark adds new data files to the directory for each incoming microbatch received.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765344_-81381221","id":"20200426-202547_2080286423","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259810"},{"title":"24 - Stopping the test","text":"%md\nWhen you are done, stop the query, terminate the test script, and exit your Spark shell.","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>When you are done, stop the query, terminate the test script, and exit your Spark shell.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765345_-17629618","id":"20200426-202547_1010479452","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259811"},{"text":"%md\n# Result\n**You have now:** \n\n---","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Result</h1>\n<p><strong>You have now:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1617309765345_-86274466","id":"20181126-133507_1472573213","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259812"},{"text":"%md\n# Solution\n---","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Solution</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1617309765345_1535133014","id":"20181018-125200_1133281582","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259813"},{"text":"%md\n### Display Streaming Data to the Console\n\nIn this section, you will read data from a file-based stream and display the results to the console. The query in this section is very simple -- it does not \ntransform the data, and simply outputs the data it receives \"as-is.\"\"","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Display Streaming Data to the Console</h3>\n<p>In this section, you will read data from a file-based stream and display the results to the console. The query in this section is very simple &ndash; it does not\n<br  />transform the data, and simply outputs the data it receives &ldquo;as-is.&ldquo;&rdquo;</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765346_2143383148","id":"20200429-213425_223954687","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259814"},{"title":"1   - Review the test data you will be using in this exercise","text":"%sh\nhead /home/training/training_materials/devsh/data/activations_stream/part-00000-ad954932-b2d7-426d-a7c1-429cfc1ebb33-c000.json","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"{\"acct_num\":128931,\"dev_id\":\"247abaa7-6c85-4f2c-a96f-c11450250506\",\"phone\":\"6190399439\",\"model\":\"Sorrento F41L\"}\n{\"acct_num\":97620,\"dev_id\":\"16512c4a-b3d3-4f92-97b7-80f9df6a487e\",\"phone\":\"7072821950\",\"model\":\"Sorrento F24L\"}\n{\"acct_num\":74940,\"dev_id\":\"361cbf31-cc6c-4233-ad4e-28d10ab09568\",\"phone\":\"5623783063\",\"model\":\"MeeToo 3.0\"}\n{\"acct_num\":125327,\"dev_id\":\"6db1bbf9-3903-47f8-b150-388dcfd8d0d3\",\"phone\":\"9168710424\",\"model\":\"Titanic 2000\"}\n{\"acct_num\":32782,\"dev_id\":\"e97f4769-13f7-4a28-9843-827f49e767dd\",\"phone\":\"8056881980\",\"model\":\"Sorrento F41L\"}\n{\"acct_num\":94440,\"dev_id\":\"1512a38e-721b-4712-afd4-79fd94ec2b56\",\"phone\":\"9513506516\",\"model\":\"MeeToo 2.0\"}\n{\"acct_num\":4337,\"dev_id\":\"6f495bc4-d6a9-43bb-90fc-650cbd6da073\",\"phone\":\"8056442972\",\"model\":\"Titanic 2200\"}\n{\"acct_num\":28646,\"dev_id\":\"eee8add4-1740-4a39-9f23-c9ee8b4e04ac\",\"phone\":\"5039316268\",\"model\":\"Titanic 2000\"}\n{\"acct_num\":44835,\"dev_id\":\"aed40d19-d5fc-4e11-a04a-f68094b38d73\",\"phone\":\"9289444545\",\"model\":\"Sorrento F41L\"}\n{\"acct_num\":121026,\"dev_id\":\"e67cd2c4-7358-4796-81ea-bc8a7bf91212\",\"phone\":\"5419395477\",\"model\":\"Sorrento F32L\"}\n"}]},"apps":[],"jobName":"paragraph_1617309765346_-10514852","id":"20200429-213439_92529216","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259815"},{"text":"%md\nIt contains information about device activations on Loudacre's cellular network in JSON format. The data is in `/home/training/training_materials/devsh/data/activations_stream/`. You will use these files later to simulate a stream of JSON data by running a script that copies the files in one at a time.\n","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>It contains information about device activations on Loudacre's cellular network in JSON format. The data is in <code>/home/training/training_materials/devsh/data/activations_stream/</code>. You will use these files later to simulate a stream of JSON data by running a script that copies the files in one at a time.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765347_1483536257","id":"20210121-234935_1737210278","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259816"},{"title":"2 - In terminal 1 set up a local directory to contain the data files that Spark will read","text":"%md\nCreate a directory `/tmp/devsh-streaming` to load the streaming data. Set the file permissions to allow your application to access the files. Do not copy any \ndata into the directory yet. This is a workaround to simulate streaming data. We will use a shell script to read out the data from the local file system.\n\n```\nmkdir -p /tmp/devsh-streaming\nchmod go+wr /tmp/devsh-streaming\n```\n\n**Note:** The directory from which Spark will load data must exist before the you create the DataFrame based on the data.\n","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a directory <code>/tmp/devsh-streaming</code> to load the streaming data. Set the file permissions to allow your application to access the files. Do not copy any\n<br  />data into the directory yet. This is a workaround to simulate streaming data. We will use a shell script to read out the data from the local file system.</p>\n<pre><code>mkdir -p /tmp/devsh-streaming\nchmod go+wr /tmp/devsh-streaming\n</code></pre>\n<p><strong>Note:</strong> The directory from which Spark will load data must exist before the you create the DataFrame based on the data.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765348_-429672106","id":"20210121-235347_1533472052","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259817"},{"title":"3 - In terminal 2 open a pyspark shell to run a streaming application","text":"%md\nIf you currenty have a Spark shell running in a terminal session, exit it.\n\nStart a new Python or Scala Spark shell running on the cluster.\n\n```\npyspark --master local[2]\n```\n\nEnter the code in the following paragraphs into your Spark shell.\n","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>If you currenty have a Spark shell running in a terminal session, exit it.</p>\n<p>Start a new Python or Scala Spark shell running on the cluster.</p>\n<pre><code>pyspark --master local[2]\n</code></pre>\n<p>Enter the code in the following paragraphs into your Spark shell.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765350_123896854","id":"20210121-235533_769764513","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259818"},{"title":"4 - Define a schema for the structure of the input data","text":"%md\nDefine column objects to create a schema. This will be applied to the inbound data.\n```\ninputDir = \"file:/tmp/devsh-streaming/\"\n\nfrom pyspark.sql.types import *\n\nactivationsSchema = StructType([ \n  StructField(\"acct_num\", IntegerType()),\n  StructField(\"dev_id\", StringType()),\n  StructField(\"phone\", StringType()),\n  StructField(\"model\", StringType())])\n```\n\nBe sure to hit return twice to get back to the pyspark prompt","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Define column objects to create a schema. This will be applied to the inbound data.</p>\n<pre><code>inputDir = \"file:/tmp/devsh-streaming/\"\n\nfrom pyspark.sql.types import *\n\nactivationsSchema = StructType([ \n  StructField(\"acct_num\", IntegerType()),\n  StructField(\"dev_id\", StringType()),\n  StructField(\"phone\", StringType()),\n  StructField(\"model\", StringType())])\n</code></pre>\n<p>Be sure to hit return twice to get back to the pyspark prompt</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765350_1803780260","id":"20200601-192211_252144301","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259819"},{"title":"5 - Create a streaming DataFrame by reading the data you reviewed above","text":"%md\n```\nactivationsDF = spark.readStream.schema(activationsSchema).json(inputDir)\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<pre><code>activationsDF = spark.readStream.schema(activationsSchema).json(inputDir)\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765351_336713609","id":"20200429-213501_2000175821","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259820"},{"title":"6 - Display the streaming DataFrame's schema to confirm that it is set up correctly","text":"%md\n```\nactivationsDF.printSchema()\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<pre><code>activationsDF.printSchema()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765351_-231112422","id":"20200429-213459_1552306503","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259821"},{"title":"7 - Confirm that the DataFrame's isStreaming property is set","text":"%md\n```\nactivationsDF.isStreaming\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<pre><code>activationsDF.isStreaming\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765353_520444872","id":"20200429-213458_677198705","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259822"},{"title":"8 - Start a streaming query","text":"%md\nStart a streaming query that displays results to the console. Use append mode to display the first several records in each new input stream micro-batch.\n\nSet the `truncate` option so that you will be able to see all the data in each record.\n\n```\nactivationsQuery = activationsDF.writeStream.outputMode(\"append\").option(\"truncate\",\"false\").format(\"console\").start()\n```\n\nThe query will not display any output yet, because no files are available to read yet.\n","user":"anonymous","dateUpdated":"2021-04-01T14:10:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Start a streaming query that displays results to the console. Use append mode to display the first several records in each new input stream micro-batch.</p>\n<p>Set the <code>truncate</code> option so that you will be able to see all the data in each record.</p>\n<pre><code>activationsQuery = activationsDF.writeStream.outputMode(\"append\").option(\"truncate\",\"false\").format(\"console\").start()\n</code></pre>\n<p>The query will not display any output yet, because no files are available to read yet.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765353_2712849","id":"20210121-235936_106982697","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259823"},{"title":"9 - In terminal 1 begin streaming data","text":"%md\nOpen a new terminal window. Run the test script to copy the test data files into the streaming directory at a rate of one per second.\n\n```\n/home/training/training_materials/devsh/scripts/streamtest-file.sh /home/training/training_materials/devsh/data/activations_stream /tmp/devsh-streaming\n```\n\nThe script will display the names of the files as it copies them.\n\n**Note:** Spark keeps track of files that have been previously read by each query. If you need to re-run the script later to test the same query, Spark will \nignore any files that were previously copied.\n","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Open a new terminal window. Run the test script to copy the test data files into the streaming directory at a rate of one per second.</p>\n<pre><code>/home/training/training_materials/devsh/scripts/streamtest-file.sh /home/training/training_materials/devsh/data/activations_stream /tmp/devsh-streaming\n</code></pre>\n<p>The script will display the names of the files as it copies them.</p>\n<p><strong>Note:</strong> Spark keeps track of files that have been previously read by each query. If you need to re-run the script later to test the same query, Spark will\n<br  />ignore any files that were previously copied.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765353_389812860","id":"20210122-000032_74525893","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259824"},{"title":"10 - Confirm that the query is displaying data","text":"%md\nReturn to your Spark shell and confirm that the query is displaying data from each batch. Note that the first batch processed and displayed will always be empty.","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Return to your Spark shell and confirm that the query is displaying data from each batch. Note that the first batch processed and displayed will always be empty.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765354_566721406","id":"20200429-213456_813615217","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259825"},{"title":"11 - Stopping the stream","text":"%md\nWhen you are done, stop the stream by entering \n\n```\nactivationsQuery.stop()\n```\n\n**Note:** You will not see a prompt while the shell is displaying output, but you can enter commands in the shell anyway.\n","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>When you are done, stop the stream by entering</p>\n<pre><code>activationsQuery.stop()\n</code></pre>\n<p><strong>Note:</strong> You will not see a prompt while the shell is displaying output, but you can enter commands in the shell anyway.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765354_2088603210","id":"20210122-000500_1039479287","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259826"},{"title":"12 - Stopping the test script","text":"%md\nTerminate the test script in the second terminal window using \n~~~\nCtrl+C\n~~~","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Terminate the test script in the second terminal window using</p>\n<pre><code>Ctrl+C\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765355_553369867","id":"20200429-213454_100119688","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259827"},{"title":"13 - In terminal 1 clean the streaming directory","text":"%md\nIn your test script terminal window (not the Spark shell), remove the data that was copied into the streaming directory in the last section.\n\n```\nrm -rf /tmp/devsh-streaming/*\n```\n","user":"anonymous","dateUpdated":"2021-04-01T14:18:01-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>In your test script terminal window (not the Spark shell), remove the data that was copied into the streaming directory in the last section.</p>\n<pre><code>rm -rf /tmp/devsh-streaming/*\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765355_-1624004671","id":"20210122-004049_1539762062","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259828"},{"text":"%md\n### Display Aggregated Streaming Data to the Console","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Display Aggregated Streaming Data to the Console</h3>\n"}]},"apps":[],"jobName":"paragraph_1617309765356_-847923845","id":"20200429-213454_1477948613","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259829"},{"title":"14 - Create a DataFrame to count each device model","text":"%md\nGo to your Spark shell. Starting with the `activationsDF` DataFrame you created above, create a second DataFrame containing the count of each device model.\n\n```\nactivationCountDF = activationsDF.groupBy(\"model\").count()\n```\n","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Go to your Spark shell. Starting with the <code>activationsDF</code> DataFrame you created above, create a second DataFrame containing the count of each device model.</p>\n<pre><code>activationCountDF = activationsDF.groupBy(\"model\").count()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765356_1477454386","id":"20210122-004138_575592903","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259830"},{"title":"15 - Start a streaming query that aggregates the data","text":"%md\nStart a streaming query that displays the model name and count for activated devices. Use `complete` mode so that the data will be aggregated across all the \ndata in all the batches received so far for each interval, rather than just each individual batch.\n\n```\nactivationCountQuery = activationCountDF.writeStream.outputMode(\"complete\").format(\"console\").start()\n```\n","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Start a streaming query that displays the model name and count for activated devices. Use <code>complete</code> mode so that the data will be aggregated across all the\n<br  />data in all the batches received so far for each interval, rather than just each individual batch.</p>\n<pre><code>activationCountQuery = activationCountDF.writeStream.outputMode(\"complete\").format(\"console\").start()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765357_1241916492","id":"20210122-004216_30449731","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259831"},{"title":"16 - Run the streamtest-file.sh script again","text":"%md\nReturn to your second terminal window and re-run the `streamtest-file.sh` script you ran in the previous section.\n\n```\n/home/training/training_materials/devsh/scripts/streamtest-file.sh /home/training/training_materials/devsh/data/activations_stream /tmp/devsh-streaming\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Return to your second terminal window and re-run the <code>streamtest-file.sh</code> script you ran in the previous section.</p>\n<pre><code>/home/training/training_materials/devsh/scripts/streamtest-file.sh /home/training/training_materials/devsh/data/activations_stream /tmp/devsh-streaming\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765357_-228905113","id":"20200429-213450_1359119850","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259832"},{"title":"17 - Confirm that complete output mode is activated","text":"%md\nIn your Spark shell, make sure that the models and their counts are being displayed. Confirm that the counts are going up in each batch because you \nused `complete` output mode.","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>In your Spark shell, make sure that the models and their counts are being displayed. Confirm that the counts are going up in each batch because you\n<br  />used <code>complete</code> output mode.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765358_-106242561","id":"20200429-213450_472997628","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259833"},{"title":"18 - Stop the query and terminate the test script","text":"%md\nWhen you are done, stop the `activationCountQuery` query\n```\nactivationCountQuery.stop()\n```\nand terminate the test script\n~~~\nCtrl+C\n~~~","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>When you are done, stop the <code>activationCountQuery</code> query</p>\n<pre><code>activationCountQuery.stop()\n</code></pre>\n<p>and terminate the test script</p>\n<pre><code>Ctrl+C\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765358_931227155","id":"20200429-213450_463682321","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259834"},{"title":"19 - Clean the streaming directory","text":"%md\nIn your test script terminal window (not the Spark shell), remove the data copied into the streaming directory in the last section.\n```\nrm -rf /tmp/devsh-streaming/*\n````","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>In your test script terminal window (not the Spark shell), remove the data copied into the streaming directory in the last section.</p>\n<pre><code>rm -rf /tmp/devsh-streaming/*\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765358_1388280902","id":"20200429-213449_1526293893","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259835"},{"text":"%md\n### Output Data to Files","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Output Data to Files</h3>\n"}]},"apps":[],"jobName":"paragraph_1617309765359_-1118111148","id":"20200429-213449_842091323","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259836"},{"title":"20 - Create a new streaming DataFrame","text":"%md\nGo to your Spark shell. Starting with the `activationsDF` DataFrame you created previously, create a new streaming DataFrame with just the rows for the \n\"Titanic 1000\" devices and the `dev_id` and `acct_num` columns.\n\n```\ntitanic1000DF = activationsDF.where(\"model = 'Titanic 1000'\").select(\"dev_id\",\"acct_num\")\n\ntitanic1000DF.printSchema()\n```\n","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Go to your Spark shell. Starting with the <code>activationsDF</code> DataFrame you created previously, create a new streaming DataFrame with just the rows for the\n<br  />&ldquo;Titanic 1000&rdquo; devices and the <code>dev_id</code> and <code>acct_num</code> columns.</p>\n<pre><code>titanic1000DF = activationsDF.where(\"model = 'Titanic 1000'\").select(\"dev_id\",\"acct_num\")\n\ntitanic1000DF.printSchema()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765359_-1801679871","id":"20210122-004657_134734719","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259837"},{"title":"21 - Start a query that saves to HDFS","text":"%md\nStart a query that saves the data in the streaming DataFrame you just created to the `/devsh_loudacre/titanic1000/` directory in HDFS. Set the query to trigger \nevery three seconds.\n\n**a.** Checkpointing must be enabled to ensure fault tolerance when saving files. Set the checkpoint HDFS directory for the Spark session.\n\n```\nspark.conf.set(\"spark.sql.streaming.checkpointLocation\", \"/tmp/streaming-checkpoint\")\n```\n\n**b.** Start the query\n\n```\ntitanic1000Query = titanic1000DF.writeStream.trigger(processingTime=\"3 seconds\").outputMode(\"append\").format(\"csv\").option(\"path\",\"/devsh_loudacre/titanic1000/\").start()\n```\n","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Start a query that saves the data in the streaming DataFrame you just created to the <code>/devsh_loudacre/titanic1000/</code> directory in HDFS. Set the query to trigger\n<br  />every three seconds.</p>\n<p><strong>a.</strong> Checkpointing must be enabled to ensure fault tolerance when saving files. Set the checkpoint HDFS directory for the Spark session.</p>\n<pre><code>spark.conf.set(\"spark.sql.streaming.checkpointLocation\", \"/tmp/streaming-checkpoint\")\n</code></pre>\n<p><strong>b.</strong> Start the query</p>\n<pre><code>titanic1000Query = titanic1000DF.writeStream.trigger(processingTime=\"3 seconds\").outputMode(\"append\").format(\"csv\").option(\"path\",\"/devsh_loudacre/titanic1000/\").start()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765360_394609731","id":"20210122-004834_1516220422","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259838"},{"title":"22 - Re-run the stream test script","text":"%md\nReturn to your terminal window and re-run the `streamtest-file.sh` test script.\n```\n/home/training/training_materials/devsh/scripts/streamtest-file.sh /home/training/training_materials/devsh/data/activations_stream /tmp/devsh-streaming\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Return to your terminal window and re-run the <code>streamtest-file.sh</code> test script.</p>\n<pre><code>/home/training/training_materials/devsh/scripts/streamtest-file.sh /home/training/training_materials/devsh/data/activations_stream /tmp/devsh-streaming\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765360_1284880524","id":"20200429-213445_1660540485","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259839"},{"title":"23 - Confirm the target directory is receiving the files","text":"%md\nOpen a new (third) terminal window and list the contents of the HDFS target directory: `/devsh_loudacre/titanic1000`. Take note of the number of files.\n\nList the contents again after a few seconds and notice that the number of files is growing as Spark adds new data files to the directory for each incoming \nmicrobatch received.","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Open a new (third) terminal window and list the contents of the HDFS target directory: <code>/devsh_loudacre/titanic1000</code>. Take note of the number of files.</p>\n<p>List the contents again after a few seconds and notice that the number of files is growing as Spark adds new data files to the directory for each incoming\n<br  />microbatch received.</p>\n"}]},"apps":[],"jobName":"paragraph_1617309765361_-884869487","id":"20200429-213445_371430499","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259840"},{"title":"24 - Stopping the test","text":"%md\nWhen you are done, stop the query and terminate the test script.\n```\ntitanic1000Query.stop()\n```\nTerminate the test script.\n```\nCtrl+C\n```","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>When you are done, stop the query and terminate the test script.</p>\n<pre><code>titanic1000Query.stop()\n</code></pre>\n<p>Terminate the test script.</p>\n<pre><code>Ctrl+C\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765361_700222705","id":"20200429-213445_2052407867","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259841"},{"text":"%md\n# Tear Down\n---\n","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Tear Down</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1617309765362_-1175348695","id":"20210121-234732_1030984281","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259842"},{"title":"Delete the streaming directories","text":"%md\nIn your test script terminal window (not the Spark shell), remove the data copied into the streaming \ndirectory in the last section.\n```\nrm -rf /tmp/devsh-streaming\nrm -rm /tmp/streaming-checkpoint\n````","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>In your test script terminal window (not the Spark shell), remove the data copied into the streaming\n<br  />directory in the last section.</p>\n<pre><code>rm -rf /tmp/devsh-streaming\nrm -rm /tmp/streaming-checkpoint\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765362_-1583080766","id":"20210204-144546_467833226","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259843"},{"title":"Delete the HDFS directory","text":"%md \n\n~~~\nhdfs dfs -rm -r -skipTrash /devsh_loudacre/titanic1000\n~~~","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<pre><code>hdfs dfs -rm -r -skipTrash /devsh_loudacre/titanic1000\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765362_-1056820584","id":"20210204-144847_1990583619","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259844"},{"title":"Quit pyspark","text":"%md \n\n~~~\nquit()\n~~~","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<pre><code>quit()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765363_1890249687","id":"20210207-104339_1477881345","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259845"},{"title":"Close the terminals","text":"%md\n~~~xml\nexit\n~~~","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<pre><code class=\"xml\">exit\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617309765363_-1054160646","id":"20210124-175311_1350270009","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259846"},{"title":"Additional resources","text":"%md\nWe hope you've enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Cloudera Tutorials](http://cloudera.com/tutorials.html) are your natural next step where you can explore Spark in more depth.\n2. [Cloudera Community](https://community.cloudera.com) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Apache Spark Documentation](https://spark.apache.org/documentation.html) - official Spark documentation.\n4. [Apache Zeppelin Project Home Page](https://zeppelin.apache.org) - official Zeppelin web site.","user":"anonymous","dateUpdated":"2021-04-01T13:42:45-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We hope you've enjoyed this lab. Below are additional resources that you should find useful:</p>\n<ol>\n<li><a href=\"http://cloudera.com/tutorials.html\">Cloudera Tutorials</a> are your natural next step where you can explore Spark in more depth.</li>\n<li><a href=\"https://community.cloudera.com\">Cloudera Community</a> is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.</li>\n<li><a href=\"https://spark.apache.org/documentation.html\">Apache Spark Documentation</a> - official Spark documentation.</li>\n<li><a href=\"https://zeppelin.apache.org\">Apache Zeppelin Project Home Page</a> - official Zeppelin web site.</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1617309765364_-784189971","id":"20181126-133017_244739700","dateCreated":"2021-04-01T13:42:45-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259847"}],"name":"/DevSH/Pyspark/Stream","id":"2G1F6ZFCC","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"angular:shared_process":[],"sh:shared_process":[],"livy:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}