{"paragraphs":[{"text":"%md\n# About This Lab\n**Objective:** In this lab, you will use Zeppelin to work with DataFrames. Begin by learning about Apache Spark documentation and JavaDocs. Read and display \na JSON file into a DataFrame. Query the DataFrame.\n**File locations:** \n    Exercises directory: /home/training/training_materials/devsh/exercises/spark-shell\n    Data (local): /home/training/training_materials/devsh/data/devices.json\n**Successful outcome:**\n**Before you begin:**\n**Related lessons:** Apache Spark Basics\n\n---","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>About This Lab</h1>\n<p><strong>Objective:</strong> In this lab, you will use Zeppelin to work with DataFrames. Begin by learning about Apache Spark documentation and JavaDocs. Read and display\n<br  />a JSON file into a DataFrame. Query the DataFrame.\n<br  /><strong>File locations:</strong></p>\n<pre><code>Exercises directory: /home/training/training_materials/devsh/exercises/spark-shell\nData (local): /home/training/training_materials/devsh/data/devices.json\n</code></pre>\n<p><strong>Successful outcome:</strong>\n<br  /><strong>Before you begin:</strong>\n<br  /><strong>Related lessons:</strong> Apache Spark Basics</p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1617116407408_-1507001958","id":"20171105-200834_1116095891","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:471891"},{"text":"%md\n# Setup\n---","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Setup</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1617116407409_2131445756","id":"20181114-164229_902436001","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471892"},{"title":"Set an environment variable to manage thread count","text":"%sh\n\nPYSPARK_PIN_THREAD=true","user":"anonymous","dateUpdated":"2021-03-30T08:37:06-0700","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1617116407409_1901631601","id":"20200830-062347_1042316978","dateCreated":"2021-03-30T08:00:07-0700","dateStarted":"2021-03-30T08:37:06-0700","dateFinished":"2021-03-30T08:37:06-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:471893"},{"title":"Set the the Spark Context for Livy","text":"%pyspark\n\nsc = spark.sparkContext","user":"anonymous","dateUpdated":"2021-03-30T10:45:18-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<font color=\"red\">Previous livy session is expired, new livy session is created. Paragraphs that depend on this paragraph need to be re-executed!</font>"}]},"apps":[],"jobName":"paragraph_1617116407409_-2030944312","id":"20200830-062453_1327233055","dateCreated":"2021-03-30T08:00:07-0700","dateStarted":"2021-03-30T08:37:21-0700","dateFinished":"2021-03-30T08:38:07-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:471894"},{"text":"%md\nIn a Zeppelin notebook the Spark shell is started by the Livy interpreter. You will not start the Spark shell manually. \nFrom a command line you will have to start the Spark shell with either `spark-shell` or `pyspark`.\n","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>In a Zeppelin notebook the Spark shell is started by the Livy interpreter. You will not start the Spark shell manually.\n<br  />From a command line you will have to start the Spark shell with either <code>spark-shell</code> or <code>pyspark</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1617116407409_1245715444","id":"20210121-115307_591923780","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471895"},{"title":"Check the status of the Spark context","text":"%pyspark\nspark","user":"anonymous","dateUpdated":"2021-03-30T08:38:22-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"<pyspark.sql.session.SparkSession object at 0x7fef9bd4b050>"}]},"apps":[],"jobName":"paragraph_1617116407409_-758751941","id":"20210121-115217_395565636","dateCreated":"2021-03-30T08:00:07-0700","dateStarted":"2021-03-30T08:38:22-0700","dateFinished":"2021-03-30T08:38:22-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:471896"},{"text":"%md\nSpark creates a SparkSession object called spark. The command `spark` will display information about the spark object. This is displaying the memory address.\n","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Spark creates a SparkSession object called spark. The command <code>spark</code> will display information about the spark object. This is displaying the memory address.</p>\n"}]},"apps":[],"jobName":"paragraph_1617116407410_220336955","id":"20210129-181238_276537837","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471897"},{"text":"%md\n# Lab\n---","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Lab</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1617116407410_19939675","id":"20181114-164844_1661453681","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471898"},{"text":"%md\n### Overview the Apache Spark Documentation\n\n1. View the Spark documentation in your web browser by visiting [http://spark.apache.org/docs/2.4.0/](http://spark.apache.org/docs/2.4.0/).\n2. From the Programming Guides menu, select SQL, DataFrames, and Datasets. Briefly review the guide and bookmark the page for later reference.\n3. From the API Docs menu, select either Scala or Python, depending on your language preference. Bookmark the API page for use during class. Later \n   exercises will refer to this documentation.\n4. If you are viewing the Scala API, notice that the package names are displayed on the left. Use the search box or scroll down to find the \n   org.apache.spark.sql package. This package contains most of the classes and objects you will be working with in this course. In particular, \n   note the Dataset class. Although this exercise focuses on DataFrames, remember that DataFrames are simply an alias for Datasets of Row objects. \n   So all the DataFrame operations you will practice using in this exercise are documented on the Dataset class.\n5. If you are viewing the Python API, locate the pyspark.sql module. This module contains most of the classes you will be working with in this \n   course. At the top are some    of the key classes in the module. View the API for the DataFrame class; these are the operations you will practice \n   using in this exercise.","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Overview the Apache Spark Documentation</h3>\n<ol>\n<li>View the Spark documentation in your web browser by visiting <a href=\"http://spark.apache.org/docs/2.4.0/\">http://spark.apache.org/docs/2.4.0/</a>.</li>\n<li>From the Programming Guides menu, select SQL, DataFrames, and Datasets. Briefly review the guide and bookmark the page for later reference.</li>\n<li>From the API Docs menu, select either Scala or Python, depending on your language preference. Bookmark the API page for use during class. Later\n<br  />exercises will refer to this documentation.</li>\n<li>If you are viewing the Scala API, notice that the package names are displayed on the left. Use the search box or scroll down to find the\n<br  />org.apache.spark.sql package. This package contains most of the classes and objects you will be working with in this course. In particular,\n<br  />note the Dataset class. Although this exercise focuses on DataFrames, remember that DataFrames are simply an alias for Datasets of Row objects.\n<br  />So all the DataFrame operations you will practice using in this exercise are documented on the Dataset class.</li>\n<li>If you are viewing the Python API, locate the pyspark.sql module. This module contains most of the classes you will be working with in this\n<br  />course. At the top are some    of the key classes in the module. View the API for the DataFrame class; these are the operations you will practice\n<br  />using in this exercise.</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1617116407410_-2129944692","id":"20171105-200519_752831754","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471899"},{"text":"%md\n### Read and Display a JSON File","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Read and Display a JSON File</h3>\n"}]},"apps":[],"jobName":"paragraph_1617116407410_464782480","id":"20200111-122228_1360988975","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471900"},{"title":"1 - Review the devices.json file","text":"%sh\n","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617116407410_1090221615","id":"20171105-200623_656362182","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471901"},{"title":"2 - Upload the data file to the /devsh_loudacre directory in HDFS","text":"%sh","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617116407410_1223269600","id":"20171105-201709_849284875","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471902"},{"title":"3 - Create a new DataFrame based on the devices.json file in HDFS","text":"%pyspark","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617116407410_-415265387","id":"20171105-201449_1118165660","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471903"},{"text":"%md\nSpark has not yet read the data in the file, but it has scanned the file to infer the schema. View the schema, and note that the column names match the \nrecord field names in the JSON file.","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Spark has not yet read the data in the file, but it has scanned the file to infer the schema. View the schema, and note that the column names match the\n<br  />record field names in the JSON file.</p>\n"}]},"apps":[],"jobName":"paragraph_1617116407410_-1399187915","id":"20200111-135850_551827200","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471904"},{"title":"4 - Print the schema of the new dataframe","text":"%pyspark\n","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617116407410_-1387118116","id":"20200111-135914_1639147600","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471905"},{"text":"%md\nDisplay the data in the DataFrame using the show function. If you don’t pass an argument to show, Spark will display the first 20 rows in the DataFrame. For \nthis step, display the first five rows. Note that the data is displayed in tabular form, using the column names defined in the schema.","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Display the data in the DataFrame using the show function. If you don’t pass an argument to show, Spark will display the first 20 rows in the DataFrame. For\n<br  />this step, display the first five rows. Note that the data is displayed in tabular form, using the column names defined in the schema.</p>\n"}]},"apps":[],"jobName":"paragraph_1617116407411_-440199136","id":"20200111-140906_1219001174","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471906"},{"title":"5 - Display the data of the new dataframe","text":"%pyspark\n","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617116407411_-392612154","id":"20200111-140737_1335490660","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471907"},{"text":"%md\nThe show and printSchema operations are actions—that is, they return a value from the distributed DataFrame to the Spark driver. Both functions display \nthe data in a nicely formatted table. These functions are intended for interactive use in the shell, but do not allow you actually work with the data that \nis returned. Try using the take action instead, which returns an array (Scala) or list (Python) of Row objects. You can display the data by iterating through \nthe collection.","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The show and printSchema operations are actions—that is, they return a value from the distributed DataFrame to the Spark driver. Both functions display\n<br  />the data in a nicely formatted table. These functions are intended for interactive use in the shell, but do not allow you actually work with the data that\n<br  />is returned. Try using the take action instead, which returns an array (Scala) or list (Python) of Row objects. You can display the data by iterating through\n<br  />the collection.</p>\n"}]},"apps":[],"jobName":"paragraph_1617116407411_1041935901","id":"20200111-141212_598289647","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471908"},{"title":"6 - Display the data using an iteration","text":"%pyspark\n","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617116407411_1466344773","id":"20200111-141240_53229344","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471909"},{"text":"%md\n### Query a Dataframe","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Query a Dataframe</h3>\n"}]},"apps":[],"jobName":"paragraph_1617116407411_1563862974","id":"20200111-163610_1311973668","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471910"},{"title":"7 - Count the rows of the dataframe","text":"%pyspark\n","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617116407411_-1090521592","id":"20200111-163656_483648092","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471911"},{"text":"%md\nDataFrame transformations typically return another DataFrame. Try using a select transformation to return a DataFrame with only the make and model columns, \nthen display its schema. Note that only the selected columns are in the schema.","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>DataFrame transformations typically return another DataFrame. Try using a select transformation to return a DataFrame with only the make and model columns,\n<br  />then display its schema. Note that only the selected columns are in the schema.</p>\n"}]},"apps":[],"jobName":"paragraph_1617116407411_-31356834","id":"20200111-163939_417159798","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471912"},{"title":"8 - Create a dataframe with only the make and model columns","text":"%pyspark\n","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617116407411_841405250","id":"20200111-164018_1558528758","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471913"},{"text":"%md\nA query is a series of one or more transformations followed by an action. Spark does not execute the query until you call the action operation. Display the \nfirst rows of the new dataframe.","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>A query is a series of one or more transformations followed by an action. Spark does not execute the query until you call the action operation. Display the\n<br  />first rows of the new dataframe.</p>\n"}]},"apps":[],"jobName":"paragraph_1617116407411_-1860739118","id":"20200111-164534_621394130","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471914"},{"title":"9 - View the content of this new dataframe","text":"%pyspark\n","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617116407412_1734469134","id":"20200111-164725_1216595747","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471915"},{"text":"%md\nTransformations in a query can be chained together. Execute a single command to show the results of a query using select and where. The resulting DataFrame \nwill contain only the columns devnum, make, and model, and only the rows where the make is Ronin.","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Transformations in a query can be chained together. Execute a single command to show the results of a query using select and where. The resulting DataFrame\n<br  />will contain only the columns devnum, make, and model, and only the rows where the make is Ronin.</p>\n"}]},"apps":[],"jobName":"paragraph_1617116407412_-58184556","id":"20200111-165124_839388669","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471916"},{"title":"10 - Chain several transformations and an action","text":"%pyspark\n\n","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617116407412_-1283766288","id":"20200111-165149_141561983","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471917"},{"text":"%md\n# Result\n**You have now:** explored a single dataframe using simple transformations.\n\n---","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Result</h1>\n<p><strong>You have now:</strong> explored a single dataframe using simple transformations.</p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1617116407412_1273685233","id":"20181119-142716_792318228","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471918"},{"text":"%md\n# Solution\n---","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Solution</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1617116407412_-324634795","id":"20171113-155535_1769142099","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471919"},{"text":"%md\n### Overview the Apache Spark Documentation\n\n1. View the Spark documentation in your web browser by visiting [http://spark.apache.org/docs/2.4.0/](http://spark.apache.org/docs/2.4.0/).\n2. From the Programming Guides menu, select SQL, DataFrames, and Datasets. Briefly review the guide and bookmark the page for later reference.\n3. From the API Docs menu, select either Scala or Python, depending on your language preference. Bookmark the API page for use during class. Later \n   exercises will refer to this documentation.\n4. If you are viewing the Scala API, notice that the package names are displayed on the left. Use the search box or scroll down to find the \n   org.apache.spark.sql package. This package contains most of the classes and objects you will be working with in this course. In particular, note \n   the Dataset class. Although this exercise focuses on DataFrames, remember that DataFrames are simply an alias for Datasets of Row objects. So all \n   the DataFrame operations you will practice using in this exercise are documented on the Dataset class.\n5. If you are viewing the Python API, locate the pyspark.sql module. This module contains most of the classes you will be working with in this course. \n   At the top are some of the key classes in the module. View the API for the DataFrame class; these are the operations you will practice using in \n   this exercise.","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Overview the Apache Spark Documentation</h3>\n<ol>\n<li>View the Spark documentation in your web browser by visiting <a href=\"http://spark.apache.org/docs/2.4.0/\">http://spark.apache.org/docs/2.4.0/</a>.</li>\n<li>From the Programming Guides menu, select SQL, DataFrames, and Datasets. Briefly review the guide and bookmark the page for later reference.</li>\n<li>From the API Docs menu, select either Scala or Python, depending on your language preference. Bookmark the API page for use during class. Later\n<br  />exercises will refer to this documentation.</li>\n<li>If you are viewing the Scala API, notice that the package names are displayed on the left. Use the search box or scroll down to find the\n<br  />org.apache.spark.sql package. This package contains most of the classes and objects you will be working with in this course. In particular, note\n<br  />the Dataset class. Although this exercise focuses on DataFrames, remember that DataFrames are simply an alias for Datasets of Row objects. So all\n<br  />the DataFrame operations you will practice using in this exercise are documented on the Dataset class.</li>\n<li>If you are viewing the Python API, locate the pyspark.sql module. This module contains most of the classes you will be working with in this course.\n<br  />At the top are some of the key classes in the module. View the API for the DataFrame class; these are the operations you will practice using in\n<br  />this exercise.</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1617116407412_1495059159","id":"20210121-114139_413683593","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471920"},{"text":"%md\n### Read and Display a JSON File","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Read and Display a JSON File</h3>\n"}]},"apps":[],"jobName":"paragraph_1617116407413_1110539125","id":"20210122-185311_907628106","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471921"},{"title":"1 - Review the devices.json file","text":"%sh\n\nhead /home/training/training_materials/devsh/data/devices.json","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"{\"devnum\":1,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"Sorrento\",\"model\":\"F00L\",\"dev_type\":\"phone\"}\n{\"devnum\":2,\"release_dt\":\"2010-04-19T00:00:00.000-07:00\",\"make\":\"Titanic\",\"model\":\"2100\",\"dev_type\":\"phone\"}\n{\"devnum\":3,\"release_dt\":\"2011-02-18T00:00:00.000-08:00\",\"make\":\"MeeToo\",\"model\":\"3.0\",\"dev_type\":\"phone\"}\n{\"devnum\":4,\"release_dt\":\"2011-09-21T00:00:00.000-07:00\",\"make\":\"MeeToo\",\"model\":\"3.1\",\"dev_type\":\"phone\"}\n{\"devnum\":5,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"1\",\"dev_type\":\"phone\"}\n{\"devnum\":6,\"release_dt\":\"2011-11-02T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"3\",\"dev_type\":\"phone\"}\n{\"devnum\":7,\"release_dt\":\"2010-05-20T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"2\",\"dev_type\":\"phone\"}\n{\"devnum\":8,\"release_dt\":\"2013-07-02T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"5\",\"dev_type\":\"phone\"}\n{\"devnum\":9,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"Titanic\",\"model\":\"1000\",\"dev_type\":\"phone\"}\n{\"devnum\":10,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"MeeToo\",\"model\":\"1.0\",\"dev_type\":\"phone\"}\n"}]},"apps":[],"jobName":"paragraph_1617116407413_1429324711","id":"20200111-123005_1525915571","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471922"},{"title":"2 - Upload the data file to the /devsh_loudacre directory in HDFS","text":"%sh\n\nhdfs dfs -rm -r -skipTrash /devsh_loudacre/devices.json\nhdfs dfs -put /home/training/training_materials/devsh/data/devices.json /devsh_loudacre/\nhdfs dfs -ls /devsh_loudacre","user":"anonymous","dateUpdated":"2021-03-30T09:01:12-0700","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"rm: `/devsh_loudacre/devices.json': No such file or directory\nFound 1 items\n-rw-r--r--   3 zeppelin supergroup       5483 2021-03-30 09:01 /devsh_loudacre/devices.json\n"}]},"apps":[],"jobName":"paragraph_1617116407414_-1301439834","id":"20200111-123604_463190353","dateCreated":"2021-03-30T08:00:07-0700","dateStarted":"2021-03-30T09:01:13-0700","dateFinished":"2021-03-30T09:01:18-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:471923"},{"title":"3 - Create a new DataFrame based on the devices.json file in HDFS","text":"%pyspark\n\nsc.setJobGroup(\"Exploring DataFrames\",\"Read the devices.json file in a DataFrame\")\ndevDF = spark.read.json(\"/devsh_loudacre/devices.json\")","user":"anonymous","dateUpdated":"2021-03-30T09:09:45-0700","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1617116407414_904350737","id":"20200111-124018_1365794443","dateCreated":"2021-03-30T08:00:07-0700","dateStarted":"2021-03-30T09:09:45-0700","dateFinished":"2021-03-30T09:09:51-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:471924"},{"text":"%md\n#### SetJobGroup\nBy default a unit of Spark execution is indendent of all others. On a cluster where you have multiple Spark actions or jobs this is makes it difficult to \nfollow the application. Application programmers can use this method to group all of those actions and jobs together. This gives a group description. \nOnce set, the Spark web UI will associate such jobs with this group.\n\nNOTE The setJobGroup is not required in your labs. \n\n","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>SetJobGroup</h4>\n<p>By default a unit of Spark execution is indendent of all others. On a cluster where you have multiple Spark actions or jobs this is makes it difficult to\n<br  />follow the application. Application programmers can use this method to group all of those actions and jobs together. This gives a group description.\n<br  />Once set, the Spark web UI will associate such jobs with this group.</p>\n<p>NOTE The setJobGroup is not required in your labs.</p>\n"}]},"apps":[],"jobName":"paragraph_1617116407414_972245521","id":"20210129-181420_633802386","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471925"},{"text":"%md\n#### Creating a DataFrame with .read\nUse a value/variable, devDF, to capture a stanza of code. \n\nDataFrame uses the .read function to read data. A power of Spark is the large number of datatypes it can read. Common data types include: .avro, .csv, \n.json, and .text.\n","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Creating a DataFrame with .read</h4>\n<p>Use a value/variable, devDF, to capture a stanza of code.</p>\n<p>DataFrame uses the .read function to read data. A power of Spark is the large number of datatypes it can read. Common data types include: .avro, .csv,\n<br  />.json, and .text.</p>\n"}]},"apps":[],"jobName":"paragraph_1617116407414_-2816288","id":"20210121-120038_1278676328","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471926"},{"text":"%md\n#### Tab Compeltion\nYou can use command completion to see Spark session functions. \nType `spark.` followed by the `tab` key to list functions.","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Tab Compeltion</h4>\n<p>You can use command completion to see Spark session functions.\n<br  />Type <code>spark.</code> followed by the <code>tab</code> key to list functions.</p>\n"}]},"apps":[],"jobName":"paragraph_1617116407414_-1652762481","id":"20210202-223454_434984083","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471927"},{"text":"%pyspark\nspark.\n","user":"anonymous","dateUpdated":"2021-03-30T09:08:38-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617116407414_620896518","id":"20210202-223539_1998862046","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471928"},{"title":"4 - Print the schema of the new dataframe","text":"%pyspark\n\ndevDF.printSchema()","user":"anonymous","dateUpdated":"2021-03-30T09:09:58-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- dev_type: string (nullable = true)\n |-- devnum: long (nullable = true)\n |-- make: string (nullable = true)\n |-- model: string (nullable = true)\n |-- release_dt: string (nullable = true)"}]},"apps":[],"jobName":"paragraph_1617116407414_-405761969","id":"20200111-140004_1737621071","dateCreated":"2021-03-30T08:00:07-0700","dateStarted":"2021-03-30T09:09:58-0700","dateFinished":"2021-03-30T09:09:59-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:471929"},{"title":"5 - Display the data of the new dataframe","text":"%pyspark\n\nsc.setJobGroup(\"Exploring DataFrames\",\"Show 5 devices\")\ndevDF.show(5)","user":"anonymous","dateUpdated":"2021-03-30T09:20:18-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------+------+--------+-----+--------------------+\n|dev_type|devnum|    make|model|          release_dt|\n+--------+------+--------+-----+--------------------+\n|   phone|     1|Sorrento| F00L|2008-10-21T00:00:...|\n|   phone|     2| Titanic| 2100|2010-04-19T00:00:...|\n|   phone|     3|  MeeToo|  3.0|2011-02-18T00:00:...|\n|   phone|     4|  MeeToo|  3.1|2011-09-21T00:00:...|\n|   phone|     5|  iFruit|    1|2008-10-21T00:00:...|\n+--------+------+--------+-----+--------------------+\nonly showing top 5 rows"}]},"apps":[],"jobName":"paragraph_1617116407414_1070313735","id":"20200111-140953_499550289","dateCreated":"2021-03-30T08:00:07-0700","dateStarted":"2021-03-30T09:20:18-0700","dateFinished":"2021-03-30T09:20:19-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:471930"},{"text":"%md\n#### Displaying a DataFrame\nDisplay the data in the DataFrame using the `show` function. If you don’t pass an argument to `show`, Spark will display the first 20 rows in the DataFrame. \nFor this step, display the first five rows. \n\nThe data is displayed in tabular form, using the column names defined in the schema. This is the DataFrame.\n\nThe `show` and `printSchema` operations are actions — that is, they return a value from the distributed DataFrame to the Spark driver. Both functions \ndisplay the data in a nicely formatted table. These functions are intended for interactive use in the shell, but do not allow you to actually work with the \ndata that is returned. \n","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Displaying a DataFrame</h4>\n<p>Display the data in the DataFrame using the <code>show</code> function. If you don’t pass an argument to <code>show</code>, Spark will display the first 20 rows in the DataFrame.\n<br  />For this step, display the first five rows.</p>\n<p>The data is displayed in tabular form, using the column names defined in the schema. This is the DataFrame.</p>\n<p>The <code>show</code> and <code>printSchema</code> operations are actions — that is, they return a value from the distributed DataFrame to the Spark driver. Both functions\n<br  />display the data in a nicely formatted table. These functions are intended for interactive use in the shell, but do not allow you to actually work with the\n<br  />data that is returned.</p>\n"}]},"apps":[],"jobName":"paragraph_1617116407415_-1967735140","id":"20210121-114432_19150827","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471931"},{"title":"6- Display the data using an iteration","text":"%pyspark\n\nsc.setJobGroup(\"Exploring DataFrames\",\"Take 5 devices the print\")\nrows = devDF.take(5)\nfor row in rows: \n    print(row)","user":"anonymous","dateUpdated":"2021-03-30T09:20:48-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Row(dev_type=u'phone', devnum=1, make=u'Sorrento', model=u'F00L', release_dt=u'2008-10-21T00:00:00.000-07:00')\nRow(dev_type=u'phone', devnum=2, make=u'Titanic', model=u'2100', release_dt=u'2010-04-19T00:00:00.000-07:00')\nRow(dev_type=u'phone', devnum=3, make=u'MeeToo', model=u'3.0', release_dt=u'2011-02-18T00:00:00.000-08:00')\nRow(dev_type=u'phone', devnum=4, make=u'MeeToo', model=u'3.1', release_dt=u'2011-09-21T00:00:00.000-07:00')\nRow(dev_type=u'phone', devnum=5, make=u'iFruit', model=u'1', release_dt=u'2008-10-21T00:00:00.000-07:00')"}]},"apps":[],"jobName":"paragraph_1617116407415_-1083195478","id":"20200111-141357_1484033904","dateCreated":"2021-03-30T08:00:07-0700","dateStarted":"2021-03-30T09:20:49-0700","dateFinished":"2021-03-30T09:20:50-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:471932"},{"text":"%md\n#### Reaching the actual data requires accessing a RDD\nTo work with the data you must access the RDD. Use the action `take` function. The `take` function returns an array (Scala) or list (Python) of Row \nobjects. This collection of row objectis is actually a RDD. \n\nUse Python foreach loop syntax to display the data by iterating through the collection.\n","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Reaching the actual data requires accessing a RDD</h4>\n<p>To work with the data you must access the RDD. Use the action <code>take</code> function. The <code>take</code> function returns an array (Scala) or list (Python) of Row\n<br  />objects. This collection of row objectis is actually a RDD.</p>\n<p>Use Python foreach loop syntax to display the data by iterating through the collection.</p>\n"}]},"apps":[],"jobName":"paragraph_1617116407415_-103894512","id":"20210121-114544_1598172060","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471933"},{"text":"%md\n#### Reading the Row Object\nThe output is the actual raw record from the dataset, identified as a Row. Following JSON file format each field is assigned a field name. The \"u\" identifes \nthe data as UTF formated text. This is coming from Python not Spark.","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Reading the Row Object</h4>\n<p>The output is the actual raw record from the dataset, identified as a Row. Following JSON file format each field is assigned a field name. The &ldquo;u&rdquo; identifes\n<br  />the data as UTF formated text. This is coming from Python not Spark.</p>\n"}]},"apps":[],"jobName":"paragraph_1617116407415_3648868","id":"20210122-175604_52005625","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471934"},{"text":"%md\n### Query a DataFrame","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Query a DataFrame</h3>\n"}]},"apps":[],"jobName":"paragraph_1617116407415_-58924616","id":"20210122-185435_6388575","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471935"},{"title":"7 - Count the rows of the dataframe","text":"%pyspark\n\nsc.setJobGroup(\"Exploring DataFrames\",\"Count the devices\")\ndevDF.count()","user":"anonymous","dateUpdated":"2021-03-30T09:27:13-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"50"}]},"apps":[],"jobName":"paragraph_1617116407415_-1294611214","id":"20200111-163831_1875186973","dateCreated":"2021-03-30T08:00:07-0700","dateStarted":"2021-03-30T09:27:13-0700","dateFinished":"2021-03-30T09:27:14-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:471936"},{"text":"%md\n#### Exploring a Dataframe\nThe standard practice is to explore a DataFrame by printing out the schema, counting the total number of rows, and then showing the first few rows of data. \n\n1. printSchema() to compare column names and data types to source data\n2. count() to determine the size of the data set\n3. show(5) to sample the data and compare to the schema\n\nThe `.count` function returns the number of items in the DataFrame.\n","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Exploring a Dataframe</h4>\n<p>The standard practice is to explore a DataFrame by printing out the schema, counting the total number of rows, and then showing the first few rows of data.</p>\n<ol>\n<li>printSchema() to compare column names and data types to source data</li>\n<li>count() to determine the size of the data set</li>\n<li>show(5) to sample the data and compare to the schema</li>\n</ol>\n<p>The <code>.count</code> function returns the number of items in the DataFrame.</p>\n"}]},"apps":[],"jobName":"paragraph_1617116407415_-1475337163","id":"20210121-121441_463017055","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471937"},{"title":"8 - Create a dataframe with only the make and model columns","text":"%pyspark\n\nmakeModelDF = devDF.select(\"make\",\"model\")\nmakeModelDF.printSchema()","user":"anonymous","dateUpdated":"2021-03-30T09:32:10-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- make: string (nullable = true)\n |-- model: string (nullable = true)"}]},"apps":[],"jobName":"paragraph_1617116407416_1113254109","id":"20200111-164146_2093325511","dateCreated":"2021-03-30T08:00:07-0700","dateStarted":"2021-03-30T09:32:10-0700","dateFinished":"2021-03-30T09:32:11-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:471938"},{"text":"%md\n#### DataFrames are immutable, transformations must create new DataFrames\nDataFrame transformations typically return another DataFrame. Try using a `select` transformation to return a DataFrame with only the `make` and `model` \ncolumns, then display its schema. Note that only the selected columns are in the schema.\n","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>DataFrames are immutable, transformations must create new DataFrames</h4>\n<p>DataFrame transformations typically return another DataFrame. Try using a <code>select</code> transformation to return a DataFrame with only the <code>make</code> and <code>model</code>\n<br  />columns, then display its schema. Note that only the selected columns are in the schema.</p>\n"}]},"apps":[],"jobName":"paragraph_1617116407416_-2103613369","id":"20210121-114639_1791679204","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471939"},{"title":"9 - View the content of this new dataframe","text":"%pyspark\n\nsc.setJobGroup(\"Exploring DataFrames\",\"Show only the make and model\")\nmakeModelDF.show(5)","user":"anonymous","dateUpdated":"2021-03-30T09:32:37-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"make":"string","model":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------+-----+\n|    make|model|\n+--------+-----+\n|Sorrento| F00L|\n| Titanic| 2100|\n|  MeeToo|  3.0|\n|  MeeToo|  3.1|\n|  iFruit|    1|\n+--------+-----+\nonly showing top 5 rows"}]},"apps":[],"jobName":"paragraph_1617116407416_-1957555897","id":"20200111-164907_1112294231","dateCreated":"2021-03-30T08:00:07-0700","dateStarted":"2021-03-30T09:32:37-0700","dateFinished":"2021-03-30T09:32:38-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:471940"},{"title":"10 - Lazy Execution","text":"%pyspark\n\nsc.setJobGroup(\"Exploring DataFrames\",\"Chain the transformations and an action\")\nnewDF = devDF.select(\"devnum\",\"make\",\"model\").where(\"make = 'Ronin'\")\nnewDF.show(5)\n","user":"anonymous","dateUpdated":"2021-03-30T09:34:03-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"devnum":"string","make":"string","model":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+-----+--------------+\n|devnum| make|         model|\n+------+-----+--------------+\n|    15|Ronin|Novelty Note 1|\n|    17|Ronin|Novelty Note 3|\n|    18|Ronin|Novelty Note 2|\n|    19|Ronin|Novelty Note 4|\n|    46|Ronin|            S4|\n+------+-----+--------------+\nonly showing top 5 rows"}]},"apps":[],"jobName":"paragraph_1617116407416_-1378712269","id":"20200111-165319_156372582","dateCreated":"2021-03-30T08:00:07-0700","dateStarted":"2021-03-30T09:34:03-0700","dateFinished":"2021-03-30T09:34:04-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:471941"},{"text":"%md\n#### Lazy Execution = Transformations and Actions\nSpark uses a construct of lazy execution, meaning data is not manipulated until there is a call for an action. In this example Spark is transforming the \ndata with a `.select` and then a `.where`. No actual execution occurs until the action `.show` is called.","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Lazy Execution = Transformations and Actions</h4>\n<p>Spark uses a construct of lazy execution, meaning data is not manipulated until there is a call for an action. In this example Spark is transforming the\n<br  />data with a <code>.select</code> and then a <code>.where</code>. No actual execution occurs until the action <code>.show</code> is called.</p>\n"}]},"apps":[],"jobName":"paragraph_1617116407416_1496620812","id":"20210121-114335_1193512831","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471942"},{"title":"12 - Execute a chain of transformations","text":"%pyspark\n\ndevDF.select(\"devnum\",\"make\",\"model\").where(\"make = 'Ronin'\").show(5)\n","user":"anonymous","dateUpdated":"2021-03-30T09:34:59-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+-----+--------------+\n|devnum| make|         model|\n+------+-----+--------------+\n|    15|Ronin|Novelty Note 1|\n|    17|Ronin|Novelty Note 3|\n|    18|Ronin|Novelty Note 2|\n|    19|Ronin|Novelty Note 4|\n|    46|Ronin|            S4|\n+------+-----+--------------+\nonly showing top 5 rows"}]},"apps":[],"jobName":"paragraph_1617116407416_1525196984","id":"20210202-224655_4586603","dateCreated":"2021-03-30T08:00:07-0700","dateStarted":"2021-03-30T09:34:59-0700","dateFinished":"2021-03-30T09:35:00-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:471943"},{"text":"%md\n#### The power of Functional Programming\nThe power of Functional Programing is transformations in a query can be chained together. Execute a single command to show the results of a query using \nselect and where. The resulting DataFrame will contain only the columns `devnum`, `make`, and `model`, and only the rows where the make is `Ronin`.\n","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>The power of Functional Programming</h4>\n<p>The power of Functional Programing is transformations in a query can be chained together. Execute a single command to show the results of a query using\n<br  />select and where. The resulting DataFrame will contain only the columns <code>devnum</code>, <code>make</code>, and <code>model</code>, and only the rows where the make is <code>Ronin</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1617116407416_2129808544","id":"20210121-114813_1666360635","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471944"},{"title":"11 - Review the Apache Spark docs","text":"%md\nReview and bookmark web page for Apache Spark Data Types\nReview and bookmark web page for Apache Spark Data Sources","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Review and bookmark web page for Apache Spark Data Types\n<br  />Review and bookmark web page for Apache Spark Data Sources</p>\n"}]},"apps":[],"jobName":"paragraph_1617116407416_1307580581","id":"20210122-181034_1732174736","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471945"},{"text":"%md\n# Tear Down\n---","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Tear Down</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1617116407417_-718019392","id":"20200830-065213_1837423457","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471946"},{"title":"Delete the Livy session","text":"%sh\n\nsessionId=$(curl -s localhost:8998/sessions | jq '.sessions[0].id')\ncurl -s localhost:8998/sessions/$sessionId -X DELETE","user":"anonymous","dateUpdated":"2021-03-30T09:36:56-0700","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"{\"msg\":\"deleted\"}"}]},"apps":[],"jobName":"paragraph_1617116407417_1288491307","id":"20200830-065153_912358573","dateCreated":"2021-03-30T08:00:07-0700","dateStarted":"2021-03-30T09:36:56-0700","dateFinished":"2021-03-30T09:36:59-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:471947"},{"title":"Additional resources","text":"%md\nWe hope you've enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Cloudera Tutorials](http://cloudera.com/tutorials.html) are your natural next step where you can explore Spark in more depth.\n2. [Cloudera Community](https://community.cloudera.com) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Apache Spark Documentation](https://spark.apache.org/documentation.html) - official Spark documentation.\n4. [Apache Zeppelin Project Home Page](https://zeppelin.apache.org) - official Zeppelin web site.","user":"anonymous","dateUpdated":"2021-03-30T08:00:07-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":10,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We hope you've enjoyed this lab. Below are additional resources that you should find useful:</p>\n<ol>\n<li><a href=\"http://cloudera.com/tutorials.html\">Cloudera Tutorials</a> are your natural next step where you can explore Spark in more depth.</li>\n<li><a href=\"https://community.cloudera.com\">Cloudera Community</a> is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.</li>\n<li><a href=\"https://spark.apache.org/documentation.html\">Apache Spark Documentation</a> - official Spark documentation.</li>\n<li><a href=\"https://zeppelin.apache.org\">Apache Zeppelin Project Home Page</a> - official Zeppelin web site.</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1617116407417_-606530261","id":"20181116-135131_93712280","dateCreated":"2021-03-30T08:00:07-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471948"}],"name":"/DevSH/Pyspark/DataFrameExplore","id":"2G23YCVVF","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"angular:shared_process":[],"sh:shared_process":[],"livy:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}