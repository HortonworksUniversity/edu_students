{"paragraphs":[{"text":"%md\n# About\n**Lab:** Exploring Query Execution\n**Objective:** Explore how Spark executes RDD and DataFrame/Dataset queries. The intent is to learn the Spark History Web UI to manage Spark applications.\n**Exercise files:**\n    Exercise directory: /home/training/training_materials/devsh/exercises/query-execution\n    Data (HDFS): /warehouse/tablespace/external/hive/devsh.db/accounts\n                 /devsh_loudacre/weblogs\n                 /devsh_loudacre/devices.json\n                 /devsh_loudacre/accountdevice\n    Hive tables: devsh.accounts\n**Successful outcome:** \n**Before you begin:** \n**Related lessons:** \n\n---","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>About</h1>\n<p><strong>Lab:</strong> Exploring Query Execution\n<br  /><strong>Objective:</strong> Explore how Spark executes RDD and DataFrame/Dataset queries. The intent is to learn the Spark History Web UI to manage Spark applications.\n<br  /><strong>Exercise files:</strong></p>\n<pre><code>Exercise directory: /home/training/training_materials/devsh/exercises/query-execution\nData (HDFS): /warehouse/tablespace/external/hive/devsh.db/accounts\n             /devsh_loudacre/weblogs\n             /devsh_loudacre/devices.json\n             /devsh_loudacre/accountdevice\nHive tables: devsh.accounts\n</code></pre>\n<p><strong>Successful outcome:</strong>\n<br  /><strong>Before you begin:</strong>\n<br  /><strong>Related lessons:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1617284271720_1726917571","id":"20181126-092644_1457476546","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:289403"},{"text":"%md\n# Setup","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Setup</h1>\n"}]},"apps":[],"jobName":"paragraph_1617284271721_-1798984930","id":"20181201-044336_178705192","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289404"},{"title":"Check the HDFS directories","text":"%sh\nhdfs dfs -ls /devsh_loudacre\n#hdfs dfs -put /home/training/training_materials/devsh/data/accountdevice /devsh_loudacre/\n#hdfs dfs -put /home/training/training_materials/devsh/data/device.json /devsh_loudacre/\n#hdfs dfs -put /home/training/training_materials/devsh/data/weblogs /devsh_loudacre/\n","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 15 items\ndrwxr-xr-x   - livy     supergroup          0 2021-02-03 09:10 /devsh_loudacre/account-models\ndrwxr-xr-x   - livy     supergroup          0 2021-02-03 12:37 /devsh_loudacre/accountIPs\ndrwxr-xr-x   - zeppelin supergroup          0 2021-02-03 02:41 /devsh_loudacre/accountdevice\ndrwxr-xr-x   - zeppelin supergroup          0 2021-02-03 08:56 /devsh_loudacre/activations\n-rw-r--r--   3 zeppelin supergroup       5483 2021-02-02 20:40 /devsh_loudacre/devices.json\n-rw-r--r--   3 zeppelin supergroup   13954723 2021-02-03 08:40 /devsh_loudacre/devicestatus.txt\ndrwxr-xr-x   - livy     supergroup          0 2021-02-03 08:49 /devsh_loudacre/devicestatus_etl\n-rw-r--r--   3 zeppelin supergroup        730 2021-02-03 05:32 /devsh_loudacre/frostroad.txt\ndrwxr-xr-x   - livy     supergroup          0 2021-02-03 08:27 /devsh_loudacre/iplist\ndrwxr-xr-x   - zeppelin supergroup          0 2021-02-02 00:21 /devsh_loudacre/kb\n-rw-r--r--   3 zeppelin supergroup        102 2021-02-03 05:51 /devsh_loudacre/makes1.txt\n-rw-r--r--   3 zeppelin supergroup         95 2021-02-03 05:51 /devsh_loudacre/makes2.txt\ndrwxr-xr-x   - livy     supergroup          0 2021-02-03 02:42 /devsh_loudacre/top_devices\ndrwxr-xr-x   - livy     supergroup          0 2021-02-03 08:31 /devsh_loudacre/userips_csv\ndrwxr-xr-x   - zeppelin supergroup          0 2021-02-03 07:50 /devsh_loudacre/weblogs\n"}]},"apps":[],"jobName":"paragraph_1617284271722_1739191309","id":"20210124-091433_737057217","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289405"},{"title":"Delete HDFS file to prevent file exist errors","text":"%sh\nhdfs dfs -rm -r -skipTrash /devsh_loudacre/account_hits","user":"anonymous","dateUpdated":"2021-04-01T10:04:36-0700","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"rm: `/devsh_loudacre/account_hits': No such file or directory\n"},{"type":"TEXT","data":"ExitValue: 1"}]},"apps":[],"jobName":"paragraph_1617284271722_-1747810324","id":"20210124-205045_277908267","dateCreated":"2021-04-01T06:37:51-0700","dateStarted":"2021-04-01T10:04:36-0700","dateFinished":"2021-04-01T10:04:38-0700","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:289406"},{"title":"Set the environment variable to manage thread count","text":"%sh\n\nPYSPARK_PIN_THREAD=true\n","user":"anonymous","dateUpdated":"2021-04-01T10:21:22-0700","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1617284271723_1079783365","id":"20210121-212641_70661480","dateCreated":"2021-04-01T06:37:51-0700","dateStarted":"2021-04-01T10:21:22-0700","dateFinished":"2021-04-01T10:21:22-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:289407"},{"title":"Start the Spark Context for Livy","text":"%pyspark\n\nsc = spark.sparkContext\n","user":"anonymous","dateUpdated":"2021-04-01T10:21:28-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1617284271723_-335837594","id":"20210121-212833_540455140","dateCreated":"2021-04-01T06:37:51-0700","dateStarted":"2021-04-01T10:21:28-0700","dateFinished":"2021-04-01T10:21:29-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:289408"},{"text":"%md\n# Lab\n","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Lab</h1>\n"}]},"apps":[],"jobName":"paragraph_1617284271723_1969965651","id":"20181126-093358_358613711","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289409"},{"text":"%md\n### Explore Partitioning of File-Based RDDs","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Explore Partitioning of File-Based RDDs</h3>\n"}]},"apps":[],"jobName":"paragraph_1617284271724_-1975525939","id":"20200426-002311_1962808726","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289410"},{"title":"1 - Note the number and size of accounts data files in HDFS","text":"%md\nReview the accounts data files in HDFS. (`/warehouse/tablespace/external/hive/devsh.db/accounts`) Take note of the number and sizes of files.","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Review the accounts data files in HDFS. (<code>/warehouse/tablespace/external/hive/devsh.db/accounts</code>) Take note of the number and sizes of files.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271724_-286911468","id":"20200426-002408_1814900362","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289411"},{"text":"%sh\n\nhdfs dfs -ls /warehouse/tablespace/external/hive/devsh.db/accounts","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 5 items\n-rw-rw-rw-+  3 hive hive    4706617 2021-01-04 10:11 /warehouse/tablespace/external/hive/devsh.db/accounts/part-m-00000\n-rw-rw-rw-+  3 hive hive    4693530 2021-01-04 10:11 /warehouse/tablespace/external/hive/devsh.db/accounts/part-m-00001\n-rw-rw-rw-+  3 hive hive    4674529 2021-01-04 10:11 /warehouse/tablespace/external/hive/devsh.db/accounts/part-m-00002\n-rw-rw-rw-+  3 hive hive    4662646 2021-01-04 10:11 /warehouse/tablespace/external/hive/devsh.db/accounts/part-m-00003\n-rw-rw-rw-+  3 hive hive        129 2021-01-04 10:11 /warehouse/tablespace/external/hive/devsh.db/accounts/part-m-00004\n"}]},"apps":[],"jobName":"paragraph_1617284271725_66812271","id":"20200522-233345_410445506","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289412"},{"title":"2 - Create an RDD using the accounts data","text":"%md\nCreate an RDD called `accountsRDD` by reading the accounts data, splitting it by commas, and keying it by account ID, which is the first field of each line.","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create an RDD called <code>accountsRDD</code> by reading the accounts data, splitting it by commas, and keying it by account ID, which is the first field of each line.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271725_1829921067","id":"20200426-002628_1889296219","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289413"},{"text":"%pyspark\naccountsRDD = sc. \\\ntextFile(\"/warehouse/tablespace/external/hive/devsh.db/accounts\"). \\\nmap(lambda line: line.split(',')). \\\nmap(lambda account: (account[0],account))","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1617284271725_630282269","id":"20200426-002657_1953644645","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289414"},{"title":"3 - Find the number of partitions in the new RDD","text":"%pyspark\n\naccountsRDD.getNumPartitions()","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"5"}]},"apps":[],"jobName":"paragraph_1617284271726_-920104715","id":"20200426-002706_444177779","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289415"},{"title":"4 - Review the lineage and execution plan of accountsRDD","text":"%md\nUse `toDebugString` to view the lineage and execution plan of `accountsRDD`. How many partitions are in the resulting RDD? How may stages does the query have?","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Use <code>toDebugString</code> to view the lineage and execution plan of <code>accountsRDD</code>. How many partitions are in the resulting RDD? How may stages does the query have?</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271726_1278280287","id":"20200426-002725_436524214","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289416"},{"text":"%pyspark\n\nprint accountsRDD.toDebugString()","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(5) PythonRDD[2] at RDD at PythonRDD.scala:53 []\n |  /warehouse/tablespace/external/hive/devsh.db/accounts MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n |  /warehouse/tablespace/external/hive/devsh.db/accounts HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []"}]},"apps":[],"jobName":"paragraph_1617284271727_-1743255838","id":"20200426-002725_1871429356","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289417"},{"text":"%md\n### Explore Execution of RDD Queries","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Explore Execution of RDD Queries</h3>\n"}]},"apps":[],"jobName":"paragraph_1617284271727_-508861399","id":"20200426-002724_1932934908","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289418"},{"title":"5 - Trigger the execution of a job","text":"%md\nCall `count` on `accountsRDD` to count the number of accounts. This will trigger execution of a job.","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Call <code>count</code> on <code>accountsRDD</code> to count the number of accounts. This will trigger execution of a job.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271727_-2088090349","id":"20200426-002723_2144549000","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289419"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617284271728_-165400002","id":"20200426-002723_1061198580","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289420"},{"title":"6 - View the Spark application UI","text":"%md\nIn the browser, view the application in the YARN RM UI and click through to view the Spark Application UI.","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>In the browser, view the application in the YARN RM UI and click through to view the Spark Application UI.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271728_-1745123368","id":"20200426-002722_1296221776","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289421"},{"title":"7 - Confirm that the execution of the job matches the results from toDebugString","text":"%md\nMake sure the **Jobs** tab is selected, and review the list of completed jobs. The most recent job, which you triggered by calling count, should be at the top of the list. (Note that the job description is usually based on the action that triggered the job execution.) Confirm that the number of stages is correct, and the number of tasks completed for the job matches the number of RDD partitions you noted when you used `toDebugString`.","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Make sure the <strong>Jobs</strong> tab is selected, and review the list of completed jobs. The most recent job, which you triggered by calling count, should be at the top of the list. (Note that the job description is usually based on the action that triggered the job execution.) Confirm that the number of stages is correct, and the number of tasks completed for the job matches the number of RDD partitions you noted when you used <code>toDebugString</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271728_-628167446","id":"20200426-003259_783721808","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289422"},{"title":"8 - View the details of the job","text":"%md\nClick on the job description to view details of the job. This will list all the stages in the job, which in this case is one.","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Click on the job description to view details of the job. This will list all the stages in the job, which in this case is one.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271729_-1512082360","id":"20200426-003259_1099747477","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289423"},{"title":"9 - View the diagram of the execution plan","text":"%md\nClick on **DAG Visualization** to see a diagram of the execution plan based on the RDD's lineage. The main diagram displays only the stages, but if you \nclick on a stage, it will show you the tasks within that stage.","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Click on <strong>DAG Visualization</strong> to see a diagram of the execution plan based on the RDD's lineage. The main diagram displays only the stages, but if you\n<br  />click on a stage, it will show you the tasks within that stage.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271729_1987920499","id":"20200426-003415_1588367725","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289424"},{"title":"10 - Explore the partitioning and DAG of a more complex query","text":"%md\n*Optional:* Explore the partitioning and DAG of a more complex query like the one below. Before you view the execution plan or job details, try to figure \nout how many stages the job will have.\n\nThis query loads Loudacre's web log data, and calculates how many times each user visited. Then it joins that user count data with account data for each user.","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><em>Optional:</em> Explore the partitioning and DAG of a more complex query like the one below. Before you view the execution plan or job details, try to figure\n<br  />out how many stages the job will have.</p>\n<p>This query loads Loudacre's web log data, and calculates how many times each user visited. Then it joins that user count data with account data for each user.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271730_825499606","id":"20200426-003259_310142277","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289425"},{"text":"%pyspark\nlogsRDD = sc.textFile(\"/devsh_loudacre/weblogs\")\n\nuserReqsRDD = logsRDD. \\\nmap(lambda line: line.split(' ')). \\\nmap(lambda words: (words[2],1)). \\\nreduceByKey(lambda v1,v2: v1 + v2)\n\naccountHitsRDD = accountsRDD.join(userReqsRDD)\naccountHitsRDD.saveAsTextFile(\"/devsh_loudacre/account_hits\")","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617284271730_1200928181","id":"20200426-003258_1413992919","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289426"},{"text":"%md\n**Note:** If you execute the query multiple times, you may note that some tasks within a stage are marked as \"skipped.\"\" This is because whenever a shuffle \noperation is executed, Spark temporarily caches the data that was shuffled. Subsequent executions of the same query re-use that data if it's available to save \nsome steps and improve performance.","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><strong>Note:</strong> If you execute the query multiple times, you may note that some tasks within a stage are marked as &ldquo;skipped.&ldquo;&rdquo; This is because whenever a shuffle\n<br  />operation is executed, Spark temporarily caches the data that was shuffled. Subsequent executions of the same query re-use that data if it's available to save\n<br  />some steps and improve performance.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271730_-305812892","id":"20200426-003257_150288643","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289427"},{"text":"%md\n### Explore Execution of DataFrame Queries","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Explore Execution of DataFrame Queries</h3>\n"}]},"apps":[],"jobName":"paragraph_1617284271731_-451106294","id":"20200426-002721_938055623","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289428"},{"title":"11 - Create a DataFrame of active accounts","text":"%md\nCreate a DataFrame of active accounts from the `accounts` table in the `devsh` database.","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a DataFrame of active accounts from the <code>accounts</code> table in the <code>devsh</code> database.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271731_1835872280","id":"20200426-002721_1060215009","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289429"},{"text":"%pyspark\naccountsDF = spark.read.table(\"devsh.accounts\")\n\nactiveAccountsDF = accountsDF. \\\nselect(\"acct_num\"). \\\nwhere(accountsDF.acct_close_dt.isNull())","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617284271732_-1915733936","id":"20200426-002720_315865919","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289430"},{"title":"12 - View the full execution plan for the new DataFrame","text":"%pyspark\nactiveAccountsDF.explain(True)","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617284271732_1891353979","id":"20200426-002719_2094070605","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289431"},{"text":"%md\nCan you locate the line in the physical plan corresponding to the command to load the `devsh.accounts` table into a DataFrame?\n\nHow many stages do you think this query has?","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Can you locate the line in the physical plan corresponding to the command to load the <code>devsh.accounts</code> table into a DataFrame?</p>\n<p>How many stages do you think this query has?</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271732_-461558571","id":"20200426-002717_1757149911","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289432"},{"title":"13 - Call the DataFrame's show function to execute the query","text":"%pyspark\n","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617284271733_-2035928616","id":"20200426-002717_1959166962","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289433"},{"title":"14 - Use the Spark Application UI to view the query","text":"%md\nView the Spark Application UI and choose the **SQL** tab. This displays a list of DataFrame and Dataset queries you have executed, with the most recent query at the top.","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>View the Spark Application UI and choose the <strong>SQL</strong> tab. This displays a list of DataFrame and Dataset queries you have executed, with the most recent query at the top.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271733_-1145671085","id":"20200426-030300_272260684","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289434"},{"title":"15 - View the query's execution plan","text":"%md\nClick the description for the top query to see the visualization of the query's execution. You can also see the query's full execution plan by opening \nthe **Details** panel below the visualization graph.","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Click the description for the top query to see the visualization of the query's execution. You can also see the query's full execution plan by opening\n<br  />the <strong>Details</strong> panel below the visualization graph.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271734_1794571383","id":"20200426-030259_1650065573","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289435"},{"title":"16 - Review the query's execution plan","text":"%md\nThe first step in the execution is a `HiveTableScan`, which loaded the account data into the DataFrame. Hover your mouse over the step to show the step's \nexecution plan. Compare that to the physical plan for the query. Note that it is the same as the last line in the physical execution plan, because it is \nthe first step to execute. Did you correctly identify this line in the execution plan as the one corresponding to the `DataFrame.read.table` operation?","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The first step in the execution is a <code>HiveTableScan</code>, which loaded the account data into the DataFrame. Hover your mouse over the step to show the step's\n<br  />execution plan. Compare that to the physical plan for the query. Note that it is the same as the last line in the physical execution plan, because it is\n<br  />the first step to execute. Did you correctly identify this line in the execution plan as the one corresponding to the <code>DataFrame.read.table</code> operation?</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271734_1850354424","id":"20200426-030259_1256056256","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289436"},{"title":"17 - View the job details for the query","text":"%md\nClick the **SQL** tab to return to the main SQL query summary tab. The **Job IDs** column provides links to the jobs that executed as part of this query \nexecution. In this case, the query consisted of just a single job. Click the job's ID to view the job details. This will display a list of stages that were \ncompleted for the query. How many stages executed? Is that the number of stages you predicted it would be?","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Click the <strong>SQL</strong> tab to return to the main SQL query summary tab. The <strong>Job IDs</strong> column provides links to the jobs that executed as part of this query\n<br  />execution. In this case, the query consisted of just a single job. Click the job's ID to view the job details. This will display a list of stages that were\n<br  />completed for the query. How many stages executed? Is that the number of stages you predicted it would be?</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271734_219627155","id":"20200426-030258_341547260","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289437"},{"title":"18 - View metrics of the execution","text":"%md\n*Optional:* Click the description of the stage to view metrics on the execution of the stage and its tasks.","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><em>Optional:</em> Click the description of the stage to view metrics on the execution of the stage and its tasks.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271735_-1757610362","id":"20200426-030258_1940765022","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289438"},{"title":"19 - Joining different data sources","text":"%md\nThe previous query was very simple, involving just a single data source with a where to return only active accounts. Try executing a more complex query \nthat joins data from two different data sources.\n\nThis query reads in the `accountdevice` data file, which maps account IDs to associated device IDs. Then it joins that data with the DataFrame of active \naccounts you created above. The result is DataFrame consisting of all device IDs in use by currently active accounts.","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The previous query was very simple, involving just a single data source with a where to return only active accounts. Try executing a more complex query\n<br  />that joins data from two different data sources.</p>\n<p>This query reads in the <code>accountdevice</code> data file, which maps account IDs to associated device IDs. Then it joins that data with the DataFrame of active\n<br  />accounts you created above. The result is DataFrame consisting of all device IDs in use by currently active accounts.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271735_-1571890472","id":"20200426-030258_69484237","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289439"},{"text":"%pyspark\naccountDeviceDF = spark.read. \\\noption(\"header\",\"true\"). \\\noption(\"inferSchema\",\"true\"). \\\ncsv(\"/devsh_loudacre/accountdevice\")\n\nactiveAcctDevsDF = activeAccountsDF. \\\njoin(accountDeviceDF,\naccountDeviceDF.acct_num ==\naccountDeviceDF.account_id). \\\nselect(\"device_id\")","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617284271735_-366394504","id":"20200426-030257_1766470330","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289440"},{"title":"20 - Analyzing the execution plan","text":"%md\nReview the full execution plan using `explain`, as you did with the previous DataFrame.\n\nCan you identify which lines in the execution plan load the two different data sources?\n\nHow many stages do you think this query will execute?","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Review the full execution plan using <code>explain</code>, as you did with the previous DataFrame.</p>\n<p>Can you identify which lines in the execution plan load the two different data sources?</p>\n<p>How many stages do you think this query will execute?</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271736_-1409407072","id":"20200426-030257_1536492862","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289441"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617284271736_-1152942398","id":"20200426-031346_1665958761","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289442"},{"title":"21 - Analyzing the execution visualization","text":"%md\nExecute the query and review the execution visualization in the Spark UI.\n\nWhat differences do you see between the execution of the earlier query and this one?\n\nHow many stages executed? Is this what you expected?","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Execute the query and review the execution visualization in the Spark UI.</p>\n<p>What differences do you see between the execution of the earlier query and this one?</p>\n<p>How many stages executed? Is this what you expected?</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271737_-724001011","id":"20200426-030256_786143272","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289443"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617284271737_425807284","id":"20200602-220140_1443082639","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289444"},{"title":"22 - Explore a query with multiple joins","text":"%md\n*Optional:* Explore an even more complex query that involves multiple joins with three data sources. You can use the last query in the solutions file for this \nexercise (in the `/home/training/training_materials/devsh/exercises/query-execution/solution/ directory`). That query creates a list of device IDs,\nmakes, and models, and the number of active accounts that use that type of device, sorted in order from most popular device type to least.","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><em>Optional:</em> Explore an even more complex query that involves multiple joins with three data sources. You can use the last query in the solutions file for this\n<br  />exercise (in the <code>/home/training/training_materials/devsh/exercises/query-execution/solution/ directory</code>). That query creates a list of device IDs,\n<br  />makes, and models, and the number of active accounts that use that type of device, sorted in order from most popular device type to least.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271737_1567547386","id":"20200426-030255_969709522","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289445"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617284271738_-1102829347","id":"20200602-220145_823272845","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289446"},{"text":"%md\n# Result\n**You have now:** \n\n---","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Result</h1>\n<p><strong>You have now:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1617284271738_-742169959","id":"20181126-133507_1472573213","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289447"},{"text":"%md\n# Solution\n---","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Solution</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1617284271738_-792433829","id":"20181018-125200_1133281582","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289448"},{"text":"%md\n### Explore Partitioning of File-Based RDDs","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Explore Partitioning of File-Based RDDs</h3>\n"}]},"apps":[],"jobName":"paragraph_1617284271739_1721347916","id":"20200429-202110_912512721","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289449"},{"title":"1 - Note the number and size of accounts data files in HDFS","text":"%sh\n\nhdfs dfs -ls -h /warehouse/tablespace/external/hive/devsh.db/accounts","user":"anonymous","dateUpdated":"2021-04-01T10:25:48-0700","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 5 items\n-rw-rw-rw-+  3 hive hive      4.5 M 2021-03-27 13:29 /warehouse/tablespace/external/hive/devsh.db/accounts/part-m-00000\n-rw-rw-rw-+  3 hive hive      4.5 M 2021-03-27 13:29 /warehouse/tablespace/external/hive/devsh.db/accounts/part-m-00001\n-rw-rw-rw-+  3 hive hive      4.5 M 2021-03-27 13:29 /warehouse/tablespace/external/hive/devsh.db/accounts/part-m-00002\n-rw-rw-rw-+  3 hive hive      4.4 M 2021-03-27 13:29 /warehouse/tablespace/external/hive/devsh.db/accounts/part-m-00003\n-rw-rw-rw-+  3 hive hive        129 2021-03-27 13:29 /warehouse/tablespace/external/hive/devsh.db/accounts/part-m-00004\n"}]},"apps":[],"jobName":"paragraph_1617284271739_-341492594","id":"20200429-202127_377500485","dateCreated":"2021-04-01T06:37:51-0700","dateStarted":"2021-04-01T10:25:48-0700","dateFinished":"2021-04-01T10:25:50-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:289450"},{"title":"2 - Create an RDD using the accounts data","text":"%pyspark\n\naccountsRDD = sc.textFile(\"/warehouse/tablespace/external/hive/devsh.db/accounts\").map(lambda line: line.split(',')).map(lambda accountFields: (accountFields[0],accountFields))","user":"anonymous","dateUpdated":"2021-04-01T10:26:16-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1617284271739_-1365212216","id":"20200429-202212_2105766018","dateCreated":"2021-04-01T06:37:51-0700","dateStarted":"2021-04-01T10:26:16-0700","dateFinished":"2021-04-01T10:26:17-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:289451"},{"text":"%md\nCreate an RDD called `accountsRDD` by reading the accounts data, splitting it by commas, and keying it by account ID, which is the first field of each line.\n","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create an RDD called <code>accountsRDD</code> by reading the accounts data, splitting it by commas, and keying it by account ID, which is the first field of each line.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271740_-630160173","id":"20210124-091904_42041844","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289452"},{"title":"3 - Find the number of partitions in the new RDD","text":"%pyspark\n\naccountsRDD.getNumPartitions()","user":"anonymous","dateUpdated":"2021-04-01T10:26:43-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"5"}]},"apps":[],"jobName":"paragraph_1617284271740_1228693899","id":"20200429-202210_1469133159","dateCreated":"2021-04-01T06:37:51-0700","dateStarted":"2021-04-01T10:26:43-0700","dateFinished":"2021-04-01T10:26:44-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:289453"},{"title":"4 - Review the lineage and execution plan of accountsRDD","text":"%pyspark\n\nprint(accountsRDD.toDebugString())","user":"anonymous","dateUpdated":"2021-04-01T10:35:44-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(5) PythonRDD[3] at RDD at PythonRDD.scala:53 []\n |  /warehouse/tablespace/external/hive/devsh.db/accounts MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n |  /warehouse/tablespace/external/hive/devsh.db/accounts HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []"}]},"apps":[],"jobName":"paragraph_1617284271740_-1604908429","id":"20200429-202209_1709217881","dateCreated":"2021-04-01T06:37:51-0700","dateStarted":"2021-04-01T10:35:44-0700","dateFinished":"2021-04-01T10:35:45-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:289454"},{"text":"%md\nUse `toDebugString` to view the lineage and execution plan of `accountsRDD`. How many partitions are in the resulting RDD? How may stages does the query have?\n","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Use <code>toDebugString</code> to view the lineage and execution plan of <code>accountsRDD</code>. How many partitions are in the resulting RDD? How may stages does the query have?</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271741_992795750","id":"20210124-091954_1214842192","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289455"},{"text":"%md\n### Explore Execution of RDD Queries","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Explore Execution of RDD Queries</h3>\n"}]},"apps":[],"jobName":"paragraph_1617284271741_1387699158","id":"20200429-202207_1214551008","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289456"},{"title":"5 - Trigger the execution of a job","text":"%pyspark\n\naccountsRDD.count()","user":"anonymous","dateUpdated":"2021-04-01T10:31:59-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"129762"}]},"apps":[],"jobName":"paragraph_1617284271741_-1699271613","id":"20200429-202207_1158636728","dateCreated":"2021-04-01T06:37:51-0700","dateStarted":"2021-04-01T10:31:59-0700","dateFinished":"2021-04-01T10:32:02-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:289457"},{"text":"%md\nCall `count` on `accountsRDD` to count the number of accounts. This will trigger execution of a job.\n","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Call <code>count</code> on <code>accountsRDD</code> to count the number of accounts. This will trigger execution of a job.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271742_-155440440","id":"20210124-092102_2109156809","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289458"},{"title":"6 - View the Spark History web UI","text":"%md\nIn the browser, view the application in the YARN RM UI and click through to view the Spark History web UI.","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>In the browser, view the application in the YARN RM UI and click through to view the Spark History web UI.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271742_1260824699","id":"20200429-202205_1104714050","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289459"},{"title":"7 - Confirm that the execution of the job matches the results from toDebugString","text":"%md\nMake sure the **Jobs** tab is selected, and review the list of completed jobs. The most recent job, which you triggered by calling count, should be at the \ntop of the list. (Note that the job description is usually based on the action that triggered the job execution.) Confirm that the number of stages is correct, \nand the number of tasks completed for the job matches the number of RDD partitions you noted when you used `toDebugString`.","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Make sure the <strong>Jobs</strong> tab is selected, and review the list of completed jobs. The most recent job, which you triggered by calling count, should be at the\n<br  />top of the list. (Note that the job description is usually based on the action that triggered the job execution.) Confirm that the number of stages is correct,\n<br  />and the number of tasks completed for the job matches the number of RDD partitions you noted when you used <code>toDebugString</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271742_257270575","id":"20200429-202205_1242768058","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289460"},{"title":"8 - View the details of the job","text":"%md\nClick on the job description to view details of the job. This will list all the stages in the job, which in this case is one.","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Click on the job description to view details of the job. This will list all the stages in the job, which in this case is one.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271742_-2082148482","id":"20200429-202204_747605927","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289461"},{"title":"9 - View the diagram of the execution plan","text":"%md\nClick on **DAG Visualization** to see a diagram of the execution plan based on the RDD's lineage. The main diagram displays only the stages, but if you click \non a stage, it will show you the tasks within that stage.","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Click on <strong>DAG Visualization</strong> to see a diagram of the execution plan based on the RDD's lineage. The main diagram displays only the stages, but if you click\n<br  />on a stage, it will show you the tasks within that stage.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271743_-187014507","id":"20200429-202204_413731544","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289462"},{"title":"10 - Explore the partitioning and DAG of a more complex query","text":"%pyspark\nlogsRDD = sc.textFile(\"/devsh_loudacre/weblogs\")\nuserReqsRDD = logsRDD.map(lambda line: line.split(' ')).map(lambda words: (words[2],1)). reduceByKey(lambda v1,v2: v1 + v2)\naccountHitsRDD = accountsRDD.join(userReqsRDD)\naccountHitsRDD.saveAsTextFile(\"/devsh_loudacre/account_hits\")","user":"anonymous","dateUpdated":"2021-04-01T10:38:34-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1617284271743_71089221","id":"20200429-202203_721477472","dateCreated":"2021-04-01T06:37:51-0700","dateStarted":"2021-04-01T10:38:34-0700","dateFinished":"2021-04-01T10:39:44-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:289463"},{"text":"%md\n*Optional:* Explore the partitioning and DAG of a more complex query like the one below. Before you view the execution plan or job details, try to figure out \nhow many stages the job will have.\n\nThis query loads Loudacre's web log data, and calculates how many times each user visited. Then it joins that user count data with account data for each user.\n","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><em>Optional:</em> Explore the partitioning and DAG of a more complex query like the one below. Before you view the execution plan or job details, try to figure out\n<br  />how many stages the job will have.</p>\n<p>This query loads Loudacre's web log data, and calculates how many times each user visited. Then it joins that user count data with account data for each user.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271743_-1201817988","id":"20210124-092419_1893786229","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289464"},{"text":"%md\n### Explore Execution of DataFrame Queries","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Explore Execution of DataFrame Queries</h3>\n"}]},"apps":[],"jobName":"paragraph_1617284271744_-1462110710","id":"20200429-202806_748490264","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289465"},{"title":"11 - Create a DataFrame of active accounts","text":"%pyspark\n\naccountsDF = spark.read.table(\"devsh.accounts\")\nactiveAccountsDF = accountsDF.select(\"acct_num\").where(accountsDF.acct_close_dt.isNotNull())","user":"anonymous","dateUpdated":"2021-04-01T10:52:18-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1617284271744_-1843979299","id":"20200429-202805_170124417","dateCreated":"2021-04-01T06:37:51-0700","dateStarted":"2021-04-01T10:52:18-0700","dateFinished":"2021-04-01T10:52:21-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:289466"},{"text":"%md\nCreate a DataFrame of active accounts from the `accounts` table in the `devsh` database.\n","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a DataFrame of active accounts from the <code>accounts</code> table in the <code>devsh</code> database.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271745_1875776721","id":"20210124-092517_493706055","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289467"},{"title":"12 - View the full execution plan for the new DataFrame","text":"%pyspark\n\nactiveAccountsDF.explain(True)","user":"anonymous","dateUpdated":"2021-04-01T10:52:37-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"== Parsed Logical Plan ==\n!Filter isnotnull(acct_close_dt#2)\n+- Project [acct_num#0]\n   +- SubqueryAlias `devsh`.`accounts`\n      +- HiveTableRelation `devsh`.`accounts`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [acct_num#0, acct_create_dt#1, acct_close_dt#2, first_name#3, last_name#4, address#5, city#6, state#7, zipcode#8, phone_number#9, created#10, modified#11]\n\n== Analyzed Logical Plan ==\nacct_num: int\nProject [acct_num#0]\n+- Filter isnotnull(acct_close_dt#2)\n   +- Project [acct_num#0, acct_close_dt#2]\n      +- SubqueryAlias `devsh`.`accounts`\n         +- HiveTableRelation `devsh`.`accounts`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [acct_num#0, acct_create_dt#1, acct_close_dt#2, first_name#3, last_name#4, address#5, city#6, state#7, zipcode#8, phone_number#9, created#10, modified#11]\n\n== Optimized Logical Plan ==\nProject [acct_num#0]\n+- Filter isnotnull(acct_close_dt#2)\n   +- HiveTableRelation `devsh`.`accounts`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [acct_num#0, acct_create_dt#1, acct_close_dt#2, first_name#3, last_name#4, address#5, city#6, state#7, zipcode#8, phone_number#9, created#10, modified#11]\n\n== Physical Plan ==\n*(1) Project [acct_num#0]\n+- *(1) Filter isnotnull(acct_close_dt#2)\n   +- Scan hive devsh.accounts [acct_close_dt#2, acct_num#0], HiveTableRelation `devsh`.`accounts`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [acct_num#0, acct_create_dt#1, acct_close_dt#2, first_name#3, last_name#4, address#5, city#6, state#7, zipcode#8, phone_number#9, created#10, modified#11]"}]},"apps":[],"jobName":"paragraph_1617284271745_-832105019","id":"20200429-202803_862265902","dateCreated":"2021-04-01T06:37:51-0700","dateStarted":"2021-04-01T10:52:37-0700","dateFinished":"2021-04-01T10:52:38-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:289468"},{"text":"%md\n#### Understand the Explain Plan\nCan you locate the line in the physical plan corresponding to the command to load the `devsh.accounts` table into a DataFrame?\n\nHow many stages do you think this query has?\n","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Understand the Explain Plan</h4>\n<p>Can you locate the line in the physical plan corresponding to the command to load the <code>devsh.accounts</code> table into a DataFrame?</p>\n<p>How many stages do you think this query has?</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271745_-1460186125","id":"20210124-092606_955391884","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289469"},{"title":"13 - Execute the query","text":"%pyspark\n\nactiveAccountsDF.show(10)","user":"anonymous","dateUpdated":"2021-04-01T10:56:58-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------+\n|acct_num|\n+--------+\n|       6|\n|       7|\n|       9|\n|      16|\n|      19|\n|      27|\n|      28|\n|      34|\n|      35|\n|      40|\n+--------+\nonly showing top 10 rows"}]},"apps":[],"jobName":"paragraph_1617284271745_-830172415","id":"20200429-202802_502625491","dateCreated":"2021-04-01T06:37:51-0700","dateStarted":"2021-04-01T10:56:58-0700","dateFinished":"2021-04-01T10:57:00-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:289470"},{"title":"14 - Use the Spark History web UI to view the query","text":"%md\nView the Spark History web UI and choose the **SQL** tab. This displays a list of DataFrame and Dataset queries you have executed, with the most recent \nquery at the top.","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>View the Spark History web UI and choose the <strong>SQL</strong> tab. This displays a list of DataFrame and Dataset queries you have executed, with the most recent\n<br  />query at the top.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271746_-208608199","id":"20200429-202800_1572827814","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289471"},{"title":"15 - View the query's execution plan","text":"%md\nClick the description for the top query to see the visualization of the query's execution. You can also see the query's full execution plan by opening \nthe **Details** panel below the visualization graph.","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Click the description for the top query to see the visualization of the query's execution. You can also see the query's full execution plan by opening\n<br  />the <strong>Details</strong> panel below the visualization graph.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271746_1736522376","id":"20200429-202759_1576628165","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289472"},{"title":"16 - Review the query's execution plan","text":"%md\nThe first step in the execution is a `HiveTableScan`, which loaded the account data into the DataFrame. Hover your mouse over the step to show the step's execution \nplan. Compare that to the physical plan for the query. Note that it is the same as the last line in the physical execution plan, because it is the first step to execute.\nDid you correctly identify this line in the execution plan as the one corresponding to the `DataFrame.read.table` operation?","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The first step in the execution is a <code>HiveTableScan</code>, which loaded the account data into the DataFrame. Hover your mouse over the step to show the step's execution\n<br  />plan. Compare that to the physical plan for the query. Note that it is the same as the last line in the physical execution plan, because it is the first step to execute.\n<br  />Did you correctly identify this line in the execution plan as the one corresponding to the <code>DataFrame.read.table</code> operation?</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271746_-1550135865","id":"20200429-202759_1612498172","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289473"},{"title":"17 - View the job details for the query","text":"%md\nClick the **SQL** tab to return to the main SQL query summary tab. The **Job IDs** column provides links to the jobs that executed as part of this query execution. \nIn this case, the query consisted of just a single job. Click the job's ID to view the job details. This will display a list of stages that were completed for \nthe query. How many stages executed? Is that the number of stages you predicted it would be?","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Click the <strong>SQL</strong> tab to return to the main SQL query summary tab. The <strong>Job IDs</strong> column provides links to the jobs that executed as part of this query execution.\n<br  />In this case, the query consisted of just a single job. Click the job's ID to view the job details. This will display a list of stages that were completed for\n<br  />the query. How many stages executed? Is that the number of stages you predicted it would be?</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271747_909085009","id":"20200429-202758_2045057848","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289474"},{"title":"18 - View metrics of the execution","text":"%md\n*Optional:* Click the description of the stage to view metrics on the execution of the stage and its tasks.","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><em>Optional:</em> Click the description of the stage to view metrics on the execution of the stage and its tasks.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271747_-581480247","id":"20200429-202758_1197049485","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289475"},{"title":"19 - Joining different data sources","text":"%pyspark\n# ------ Load account device data ---------\n# Create a DataFrame from the account device data\naccountDeviceDF = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/devsh_loudacre/accountdevice\")\naccountDeviceDF.explain(True)","user":"anonymous","dateUpdated":"2021-04-01T10:57:37-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"== Parsed Logical Plan ==\nRelation[id#39,account_id#40,device_id#41,activation_date#42L,account_device_id#43] csv\n\n== Analyzed Logical Plan ==\nid: int, account_id: int, device_id: int, activation_date: bigint, account_device_id: string\nRelation[id#39,account_id#40,device_id#41,activation_date#42L,account_device_id#43] csv\n\n== Optimized Logical Plan ==\nRelation[id#39,account_id#40,device_id#41,activation_date#42L,account_device_id#43] csv\n\n== Physical Plan ==\n*(1) FileScan csv [id#39,account_id#40,device_id#41,activation_date#42L,account_device_id#43] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://localhost.localdomain:8020/devsh_loudacre/accountdevice], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,account_id:int,device_id:int,activation_date:bigint,account_device_id:string>"}]},"apps":[],"jobName":"paragraph_1617284271747_1826429116","id":"20200429-202757_232787747","dateCreated":"2021-04-01T06:37:51-0700","dateStarted":"2021-04-01T10:57:37-0700","dateFinished":"2021-04-01T10:57:40-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:289476"},{"text":"%md\nThe previous query was very simple, involving just a single data source with a where clause to return only active accounts. Try executing a more complex \nquery that joins data from two different data sources.\n\nThis query reads in the `accountdevice` data file, which maps account IDs to associated device IDs. Then it joins that data with the DataFrame of active \naccounts you created above. The result is DataFrame consisting of all device IDs in use by currently active accounts.\n","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The previous query was very simple, involving just a single data source with a where clause to return only active accounts. Try executing a more complex\n<br  />query that joins data from two different data sources.</p>\n<p>This query reads in the <code>accountdevice</code> data file, which maps account IDs to associated device IDs. Then it joins that data with the DataFrame of active\n<br  />accounts you created above. The result is DataFrame consisting of all device IDs in use by currently active accounts.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271748_472960770","id":"20210124-092945_793396535","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289477"},{"title":"Sample the first data set","text":"%pyspark\n\naccountDeviceDF.show(5)\n","user":"anonymous","dateUpdated":"2021-04-01T10:59:08-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----+----------+---------+---------------+--------------------+\n|   id|account_id|device_id|activation_date|   account_device_id|\n+-----+----------+---------+---------------+--------------------+\n|48692|     32443|       29|  1393242509000|7351fed1-f344-4cd...|\n|48693|     32444|        4|  1353649861000|6da22278-ff7a-461...|\n|48694|     32445|        9|  1331819465000|cb993b85-6775-407...|\n|48695|     32446|       43|  1336860950000|48ea2c09-a0df-4d1...|\n|48696|     32446|       29|  1383650663000|4b49c0a6-d141-42e...|\n+-----+----------+---------+---------------+--------------------+\nonly showing top 5 rows"}]},"apps":[],"jobName":"paragraph_1617284271748_-483309786","id":"20210124-101431_516914065","dateCreated":"2021-04-01T06:37:51-0700","dateStarted":"2021-04-01T10:59:08-0700","dateFinished":"2021-04-01T10:59:09-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:289478"},{"title":"Join to a second dataset","text":"%pyspark\n# -------- query for device IDs for active accounts\n# Create a DataFrame with a device model IDs for only devices \n# used by unclosed accounts\nactiveAcctDevsDF =  activeAccountsDF.join(accountDeviceDF,activeAccountsDF.acct_num == accountDeviceDF.account_id).select(\"device_id\")","user":"anonymous","dateUpdated":"2021-04-01T10:59:11-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1617284271748_1641243128","id":"20210124-101823_1589788198","dateCreated":"2021-04-01T06:37:51-0700","dateStarted":"2021-04-01T10:59:11-0700","dateFinished":"2021-04-01T10:59:12-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:289479"},{"title":"Sample the join data","text":"%pyspark\n\nactiveAcctDevsDF.show(5)","user":"anonymous","dateUpdated":"2021-04-01T10:59:17-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+\n|device_id|\n+---------+\n|       43|\n|       29|\n|       13|\n|       18|\n|       29|\n+---------+\nonly showing top 5 rows"}]},"apps":[],"jobName":"paragraph_1617284271749_-1708668632","id":"20210124-102034_1891773815","dateCreated":"2021-04-01T06:37:51-0700","dateStarted":"2021-04-01T10:59:17-0700","dateFinished":"2021-04-01T10:59:18-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:289480"},{"title":"20 - Run the full job and analyzing the execution plan","text":"%pyspark\n# ------ Load account device data ---------\n# Create a DataFrame from the account device data\naccountDeviceDF = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/devsh_loudacre/accountdevice\")\n\n# -------- query for device IDs for active accounts\n# Create a DataFrame with a device model IDs for only devices \n# used by unclosed accounts\nactiveAcctDevsDF =  activeAccountsDF.join(accountDeviceDF,activeAccountsDF.acct_num == accountDeviceDF.account_id).select(\"device_id\")\n\nactiveAcctDevsDF.explain(True)","user":"anonymous","dateUpdated":"2021-04-01T10:59:38-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"== Parsed Logical Plan ==\n'Project [unresolvedalias('device_id, None)]\n+- Join Inner, (acct_num#0 = account_id#106)\n   :- Project [acct_num#0]\n   :  +- Filter isnotnull(acct_close_dt#2)\n   :     +- Project [acct_num#0, acct_close_dt#2]\n   :        +- SubqueryAlias `devsh`.`accounts`\n   :           +- HiveTableRelation `devsh`.`accounts`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [acct_num#0, acct_create_dt#1, acct_close_dt#2, first_name#3, last_name#4, address#5, city#6, state#7, zipcode#8, phone_number#9, created#10, modified#11]\n   +- Relation[id#105,account_id#106,device_id#107,activation_date#108L,account_device_id#109] csv\n\n== Analyzed Logical Plan ==\ndevice_id: int\nProject [device_id#107]\n+- Join Inner, (acct_num#0 = account_id#106)\n   :- Project [acct_num#0]\n   :  +- Filter isnotnull(acct_close_dt#2)\n   :     +- Project [acct_num#0, acct_close_dt#2]\n   :        +- SubqueryAlias `devsh`.`accounts`\n   :           +- HiveTableRelation `devsh`.`accounts`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [acct_num#0, acct_create_dt#1, acct_close_dt#2, first_name#3, last_name#4, address#5, city#6, state#7, zipcode#8, phone_number#9, created#10, modified#11]\n   +- Relation[id#105,account_id#106,device_id#107,activation_date#108L,account_device_id#109] csv\n\n== Optimized Logical Plan ==\nProject [device_id#107]\n+- Join Inner, (acct_num#0 = account_id#106)\n   :- Project [acct_num#0]\n   :  +- Filter (isnotnull(acct_close_dt#2) && isnotnull(acct_num#0))\n   :     +- HiveTableRelation `devsh`.`accounts`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [acct_num#0, acct_create_dt#1, acct_close_dt#2, first_name#3, last_name#4, address#5, city#6, state#7, zipcode#8, phone_number#9, created#10, modified#11]\n   +- Project [account_id#106, device_id#107]\n      +- Filter isnotnull(account_id#106)\n         +- Relation[id#105,account_id#106,device_id#107,activation_date#108L,account_device_id#109] csv\n\n== Physical Plan ==\n*(2) Project [device_id#107]\n+- *(2) BroadcastHashJoin [acct_num#0], [account_id#106], Inner, BuildLeft\n   :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint))), [id=#110]\n   :  +- *(1) Project [acct_num#0]\n   :     +- *(1) Filter (isnotnull(acct_close_dt#2) && isnotnull(acct_num#0))\n   :        +- Scan hive devsh.accounts [acct_close_dt#2, acct_num#0], HiveTableRelation `devsh`.`accounts`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [acct_num#0, acct_create_dt#1, acct_close_dt#2, first_name#3, last_name#4, address#5, city#6, state#7, zipcode#8, phone_number#9, created#10, modified#11]\n   +- *(2) Project [account_id#106, device_id#107]\n      +- *(2) Filter isnotnull(account_id#106)\n         +- *(2) FileScan csv [account_id#106,device_id#107] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://localhost.localdomain:8020/devsh_loudacre/accountdevice], PartitionFilters: [], PushedFilters: [IsNotNull(account_id)], ReadSchema: struct<account_id:int,device_id:int>"}]},"apps":[],"jobName":"paragraph_1617284271749_-1135656229","id":"20200429-203545_1342409357","dateCreated":"2021-04-01T06:37:51-0700","dateStarted":"2021-04-01T10:59:38-0700","dateFinished":"2021-04-01T10:59:39-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:289481"},{"text":"%md\nReview the full execution plan using `explain`, as you did with the previous DataFrame.\n\nCan you identify which lines in the execution plan load the two different data sources?\n\nHow many stages do you think this query will execute?","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Review the full execution plan using <code>explain</code>, as you did with the previous DataFrame.</p>\n<p>Can you identify which lines in the execution plan load the two different data sources?</p>\n<p>How many stages do you think this query will execute?</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271749_1556703911","id":"20210124-093019_120370943","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289482"},{"title":"21 - Analyzing the execution visualization","text":"%pyspark\n\nactiveAcctDevsDF.write.mode(\"overwrite\").save(\"/devsh_loudacre/active_account_devices\")","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1617284271750_63073226","id":"20200429-203543_1186915220","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289483"},{"text":"%md\nExecute the query and review the execution visualization in the Spark UI.\n\nWhat differences do you see between the execution of the earlier query and this one?\n\nHow many stages executed? Is this what you expected?","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Execute the query and review the execution visualization in the Spark UI.</p>\n<p>What differences do you see between the execution of the earlier query and this one?</p>\n<p>How many stages executed? Is this what you expected?</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271750_-488227256","id":"20210124-093052_917052835","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289484"},{"title":"22 - Explore a query with multiple joins","text":"%pyspark\n# Sum up the total number of each device model \nsumDevicesDF = activeAcctDevsDF.groupBy(\"device_id\").count().withColumnRenamed(\"count\",\"active_num\")\n\n# Order by count\norderDevicesDF = sumDevicesDF.orderBy(sumDevicesDF.active_num.desc())\n\n# Join the list of device model totals with the list of devices\n# to get the make and model for each device\ndevDF = spark.read.json(\"/devsh_loudacre/devices.json\")\njoinDevicesDF = orderDevicesDF.join(devDF,sumDevicesDF.device_id == devDF.devnum)\n\n# Write out the data with the correct columns\njoinDevicesDF.select(\"device_id\",\"make\",\"model\",joinDevicesDF.active_num).write.mode(\"overwrite\").save(\"/devsh_loudacre/top_devices\")","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1617284271750_-559209242","id":"20200429-205046_1687837521","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289485"},{"text":"%md\n*Optional:* Explore an even more complex query that involves multiple joins with three data sources. You can use the last query in the solutions file for this exercise (in the `/home/training/training_materials/devsh/exercises/query-execution/solution/ directory`). That query creates a list of device IDs, makes, and models, and the number of active accounts that use that type of device, sorted in order from most popular device type to least.\n","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><em>Optional:</em> Explore an even more complex query that involves multiple joins with three data sources. You can use the last query in the solutions file for this exercise (in the <code>/home/training/training_materials/devsh/exercises/query-execution/solution/ directory</code>). That query creates a list of device IDs, makes, and models, and the number of active accounts that use that type of device, sorted in order from most popular device type to least.</p>\n"}]},"apps":[],"jobName":"paragraph_1617284271751_-71073847","id":"20210124-093129_30387689","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289486"},{"text":"%md\n# Tear Down\n---\n","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Tear Down</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1617284271751_715597155","id":"20210121-213354_1358400255","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289487"},{"title":"Delete the Livy session","text":"%sh\n\nsessionId=$(curl -s localhost:8998/sessions | jq '.sessions[0].id')\ncurl -s localhost:8998/sessions/$sessionId -X DELETE\n","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"{\"msg\":\"Session 'null' not found.\"}"}]},"apps":[],"jobName":"paragraph_1617284271751_-2042798720","id":"20210121-213413_1673244691","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289488"},{"text":"%md\nWe hope you've enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Cloudera Tutorials](http://cloudera.com/tutorials.html) are your natural next step where you can explore Spark in more depth.\n2. [Cloudera Community](https://community.cloudera.com) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Apache Spark Documentation](https://spark.apache.org/documentation.html) - official Spark documentation.\n4. [Apache Zeppelin Project Home Page](https://zeppelin.apache.org) - official Zeppelin web site.","user":"anonymous","dateUpdated":"2021-04-01T06:37:51-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We hope you've enjoyed this lab. Below are additional resources that you should find useful:</p>\n<ol>\n<li><a href=\"http://cloudera.com/tutorials.html\">Cloudera Tutorials</a> are your natural next step where you can explore Spark in more depth.</li>\n<li><a href=\"https://community.cloudera.com\">Cloudera Community</a> is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.</li>\n<li><a href=\"https://spark.apache.org/documentation.html\">Apache Spark Documentation</a> - official Spark documentation.</li>\n<li><a href=\"https://zeppelin.apache.org\">Apache Zeppelin Project Home Page</a> - official Zeppelin web site.</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1617284271751_615801446","id":"20181126-133017_244739700","dateCreated":"2021-04-01T06:37:51-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289489"}],"name":"/DevSH/Pyspark/SparkHistory","id":"2G3N3NAAG","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"angular:shared_process":[],"sh:shared_process":[],"livy:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}