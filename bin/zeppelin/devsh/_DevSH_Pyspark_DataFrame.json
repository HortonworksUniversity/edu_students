{"paragraphs":[{"text":"%md\n# About This Lab\n**Objective:** Manipulate DataFrames schemas. Create and save DataFrames using different types of data sources. Learn options for inferring and defining \nschemas. Create a DataFrame using structField and structType. Use the parquet-tools command to view parquet schemas and data.\n**File locations:**\n    Exercise files: /home/training/training_materials/devsh/exercises/dataframes\n    Data (local): /home/training/training_materials/devsh/data/devices.json\n    Data (HDFS): /devsh_loudacre/devices.json\n    Hive table: devsh.accounts\n**Successful outcome:**\n**Before you begin:**\n**Related lessons:** Working with Dataframes and Schemas\n\n---","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>About This Lab</h1>\n<p><strong>Objective:</strong> Manipulate DataFrames schemas. Create and save DataFrames using different types of data sources. Learn options for inferring and defining\n<br  />schemas. Create a DataFrame using structField and structType. Use the parquet-tools command to view parquet schemas and data.\n<br  /><strong>File locations:</strong></p>\n<pre><code>Exercise files: /home/training/training_materials/devsh/exercises/dataframes\nData (local): /home/training/training_materials/devsh/data/devices.json\nData (HDFS): /devsh_loudacre/devices.json\nHive table: devsh.accounts\n</code></pre>\n<p><strong>Successful outcome:</strong>\n<br  /><strong>Before you begin:</strong>\n<br  /><strong>Related lessons:</strong> Working with Dataframes and Schemas</p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1617123747316_-1757756444","id":"20171105-200834_1116095891","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:236255"},{"text":"%md\n# Setup\n---","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Setup</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1617123747317_1449856084","id":"20181114-164229_902436001","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236256"},{"title":"Set environment variable to manage threads","text":"%sh\n\nPYSPARK_PIN_THREAD=true","user":"anonymous","dateUpdated":"2021-03-30T11:25:57-0700","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1617123747317_599013900","id":"20200830-073644_1484939052","dateCreated":"2021-03-30T10:02:27-0700","dateStarted":"2021-03-30T11:25:57-0700","dateFinished":"2021-03-30T11:25:57-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:236257"},{"title":"Start the Spark Context for Livy","text":"%pyspark\n\nsc = spark.sparkContext","user":"anonymous","dateUpdated":"2021-03-30T11:26:00-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<font color=\"red\">Previous livy session is expired, new livy session is created. Paragraphs that depend on this paragraph need to be re-executed!</font>"}]},"apps":[],"jobName":"paragraph_1617123747317_-1858096769","id":"20200830-074054_2080534068","dateCreated":"2021-03-30T10:02:27-0700","dateStarted":"2021-03-30T11:26:00-0700","dateFinished":"2021-03-30T11:26:48-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:236258"},{"text":"%md\n# Lab\n---","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Lab</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1617123747317_-353917825","id":"20181114-164844_1661453681","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236259"},{"text":"%md\n### Create a Dataframe Based on a Hive Table","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Create a Dataframe Based on a Hive Table</h3>\n"}]},"apps":[],"jobName":"paragraph_1617123747318_1966887861","id":"20171105-200519_752831754","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236260"},{"title":"1 - Review the schema of the Hive table devsh.accounts","text":"%sql","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617123747318_352285610","id":"20171105-200623_656362182","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236261"},{"text":"%md\nThe %sql magic provides you with a Spark SQL environment that allows you to interact with Hive tables.","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The %sql magic provides you with a Spark SQL environment that allows you to interact with Hive tables.</p>\n"}]},"apps":[],"jobName":"paragraph_1617123747318_-1390888184","id":"20181115-084123_1911813743","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236262"},{"title":"2 - Create a new DataFrame using the Hive devsh.accounts table","text":"%pyspark\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617123747318_-1844061010","id":"20171105-201709_849284875","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236263"},{"text":"%md\nPrint the schema and the first 10 rows of the DataFrame, and note that the schema aligns with that of the Hive table.","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Print the schema and the first 10 rows of the DataFrame, and note that the schema aligns with that of the Hive table.</p>\n"}]},"apps":[],"jobName":"paragraph_1617123747318_1612116095","id":"20171105-201449_1118165660","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236264"},{"title":"3 - View the dataframe and confirm alignment with the Hive table","text":"%pyspark\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617123747319_-95331279","id":"20200111-175746_198257135","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236265"},{"text":"%md\nCreate a new DataFrame with rows from the accounts data where the zip code is 94913, and save the result to CSV files in the /devsh_loudacre/ accounts_zip94913 \nHDFS directory. You can do this in a single command, as shown below, or with multiple commands.","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame with rows from the accounts data where the zip code is 94913, and save the result to CSV files in the /devsh_loudacre/ accounts_zip94913\n<br  />HDFS directory. You can do this in a single command, as shown below, or with multiple commands.</p>\n"}]},"apps":[],"jobName":"paragraph_1617123747319_-264772106","id":"20200111-180055_1261073848","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236266"},{"title":"4 - Create a new dataframe and save it to HDFS","text":"%pyspark","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617123747319_-1445201627","id":"20200111-180152_647295647","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236267"},{"text":"%md\nUse hdfs to view the /devsh_loudacre/accounts_zip94913 directory in HDFS and the data in one of the saved files. Confirm that the CSV file includes a header \nline, and that only records for the selected zip code are included.","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Use hdfs to view the /devsh_loudacre/accounts_zip94913 directory in HDFS and the data in one of the saved files. Confirm that the CSV file includes a header\n<br  />line, and that only records for the selected zip code are included.</p>\n"}]},"apps":[],"jobName":"paragraph_1617123747319_-1292470410","id":"20200111-180517_2057464815","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236268"},{"title":"5 - Confirm successful write to HDFS","text":"%sh\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617123747320_-2060487845","id":"20200111-180616_878429565","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236269"},{"text":"%md\nTry creating a new DataFrame based on the CSV files you created above. Compare the schema of the original accountsDF and the new DataFrame. What’s \ndifferent? Try again, this time setting the inferSchema option to true and compare again.","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Try creating a new DataFrame based on the CSV files you created above. Compare the schema of the original accountsDF and the new DataFrame. What’s\n<br  />different? Try again, this time setting the inferSchema option to true and compare again.</p>\n"}]},"apps":[],"jobName":"paragraph_1617123747321_475289266","id":"20200111-181226_903550862","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236270"},{"title":"6 - Compare the inferSchema option","text":"%pyspark\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617123747321_1666588536","id":"20200111-181420_1328402570","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236271"},{"text":"%md\n### Define a Schema for a DataFrame\nIf you have not done so yet, review the data in the HDFS file /devsh_loudacre/devices.json.","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Define a Schema for a DataFrame</h3>\n<p>If you have not done so yet, review the data in the HDFS file /devsh_loudacre/devices.json.</p>\n"}]},"apps":[],"jobName":"paragraph_1617123747321_-1722731064","id":"20200111-182630_966116104","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236272"},{"title":"7 - Review the data in /devsh_loudacre/devices.json","text":"%sh\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617123747321_-627834493","id":"20200111-182913_1175240105","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236273"},{"text":"%md\nCreate a new DataFrame based on the devices.json file. (This command could take several seconds while it infers the schema.)","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame based on the devices.json file. (This command could take several seconds while it infers the schema.)</p>\n"}]},"apps":[],"jobName":"paragraph_1617123747322_867753759","id":"20200111-183241_1034944500","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236274"},{"title":"8 - Read the devices.json file into a devDF dataframe","text":"%pyspark\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617123747322_293218520","id":"20200111-183151_1635217379","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236275"},{"text":"%md\nView the schema of the devDF DataFrame. Note the column names and types that Spark inferred from the JSON file. In particular, note that the release_dt column \nis of type string, whereas the data in the column actually represents a timestamp.","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>View the schema of the devDF DataFrame. Note the column names and types that Spark inferred from the JSON file. In particular, note that the release_dt column\n<br  />is of type string, whereas the data in the column actually represents a timestamp.</p>\n"}]},"apps":[],"jobName":"paragraph_1617123747322_-457508457","id":"20200111-183538_253681976","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236276"},{"title":"9 - View the schema of the devDF dataframe","text":"%pyspark","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617123747322_-1718937388","id":"20200111-183606_516389013","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236277"},{"text":"%md\nDefine a schema that correctly specifies the column types for this DataFrame. Start by importing the package with the definitions of necessary classes and \ntypes. Next, create a collection of StructField objects, which represent column definitions. The release_dt column should be a timestamp.\n\n```spark\nfrom pyspark.sql.types import *\ndevColumns = [ StructField(\"devnum\",LongType()), StructField(\"make\",StringType()), StructField(\"model\",StringType()), StructField(\"release_dt\",TimestampType()), StructField(\"dev_type\",StringType())]\n```","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Define a schema that correctly specifies the column types for this DataFrame. Start by importing the package with the definitions of necessary classes and\n<br  />types. Next, create a collection of StructField objects, which represent column definitions. The release_dt column should be a timestamp.</p>\n<pre><code class=\"spark\">from pyspark.sql.types import *\ndevColumns = [ StructField(\"devnum\",LongType()), StructField(\"make\",StringType()), StructField(\"model\",StringType()), StructField(\"release_dt\",TimestampType()), StructField(\"dev_type\",StringType())]\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617123747322_-1571055985","id":"20200111-185637_433874821","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236278"},{"title":"10 - Define a schema","text":"%pyspark\n\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617123747323_1725133220","id":"20200111-185741_1002287600","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236279"},{"title":"11 - Create a schema (a StructType object) using the column definition list","text":"%pyspark\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617123747323_-79418798","id":"20200111-190339_1337771160","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236280"},{"title":"12 - Recreate the devDF DataFrame, this time using the new schema","text":"%pyspark\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617123747323_32805315","id":"20200111-190529_818511213","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236281"},{"title":"13 - View the schema and data of the new DataFrame, and confirm that the release_dt column type is now timestamp","text":"%pyspark\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617123747323_-1199396148","id":"20200111-190751_1912942450","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236282"},{"text":"%md\nNow that the device data uses the correct schema, write the data into Parquet format, which automatically embeds the schema. Save the Parquet data files into \nan HDFS directory called /devsh_loudacre/devices_parquet.","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Now that the device data uses the correct schema, write the data into Parquet format, which automatically embeds the schema. Save the Parquet data files into\n<br  />an HDFS directory called /devsh_loudacre/devices_parquet.</p>\n"}]},"apps":[],"jobName":"paragraph_1617123747323_299384188","id":"20200111-191615_1691108697","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236283"},{"title":"14 - Save the devDF dataframe to /devsh_loudacre/devices_parquet using the parquet format","text":"%pyspark\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617123747323_-424265712","id":"20200111-191643_697955659","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236284"},{"text":"%md\n### Optional: \n\nUse parquet-tools to view the schema of the saved files. First download the HDFS directory (or an individual file), then run the command.\n\n```shell\nrm -r /tmp/devices_parquet\nhdfs dfs -get /devsh_loudacre/devices_parquet /tmp/ \n\nparquet-tools schema /tmp/devices_parquet/\nparquet-tools head /tmp/devices_parquet/\n```\n\nNote that the type of the release_dt column is noted as int96; this is how Spark denotes a timestamp type in Parquet.\nFor more information about parquet-tools, run parquet-tools --help.","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Optional:</h3>\n<p>Use parquet-tools to view the schema of the saved files. First download the HDFS directory (or an individual file), then run the command.</p>\n<pre><code class=\"shell\">rm -r /tmp/devices_parquet\nhdfs dfs -get /devsh_loudacre/devices_parquet /tmp/ \n\nparquet-tools schema /tmp/devices_parquet/\nparquet-tools head /tmp/devices_parquet/\n</code></pre>\n<p>Note that the type of the release_dt column is noted as int96; this is how Spark denotes a timestamp type in Parquet.\n<br  />For more information about parquet-tools, run parquet-tools &ndash;help.</p>\n"}]},"apps":[],"jobName":"paragraph_1617123747323_499511609","id":"20200111-192204_1182605032","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236285"},{"title":"15 - Confirm the schema of the saved files using parquet-tools","text":"%sh","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617123747324_823113928","id":"20200111-192641_1103008129","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236286"},{"text":"%md\nCreate a new DataFrame using the Parquet files you saved in devices_parquet and view its schema. Note that Spark is able to correctly infer the timestamp \ntype of the release_dt column from Parquet’s embedded schema.","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame using the Parquet files you saved in devices_parquet and view its schema. Note that Spark is able to correctly infer the timestamp\n<br  />type of the release_dt column from Parquet’s embedded schema.</p>\n"}]},"apps":[],"jobName":"paragraph_1617123747324_1396615871","id":"20200111-193044_783469452","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236287"},{"title":"16 - Read the parquet files back in a dataframe to confirm the schema is well saved","text":"%pyspark\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617123747324_1576442443","id":"20200111-193146_2016640957","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236288"},{"text":"%md\n# Result\n**You have now:** explored different methods to define the schema of a dataframe.\n\n---","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Result</h1>\n<p><strong>You have now:</strong> explored different methods to define the schema of a dataframe.</p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1617123747324_-632451145","id":"20181119-142716_792318228","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236289"},{"text":"%md\n# Solution\n---","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Solution</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1617123747324_2067893386","id":"20171113-155535_1769142099","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236290"},{"text":"%md\n### Create a Dataframe Based on a Hive Table","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Create a Dataframe Based on a Hive Table</h3>\n"}]},"apps":[],"jobName":"paragraph_1617123747324_2079158892","id":"20210122-190017_1915525627","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236291"},{"title":"1 - Review the schema of the Hive table devsh.accounts","text":"%sql\n\nDESCRIBE devsh.accounts","user":"anonymous","dateUpdated":"2021-03-30T11:26:51-0700","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"col_name":"string","data_type":"string","comment":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"col_name\tdata_type\tcomment\nacct_num\tint\tnull\nacct_create_dt\ttimestamp\tnull\nacct_close_dt\ttimestamp\tnull\nfirst_name\tstring\tnull\nlast_name\tstring\tnull\naddress\tstring\tnull\ncity\tstring\tnull\nstate\tstring\tnull\nzipcode\tstring\tnull\nphone_number\tstring\tnull\ncreated\ttimestamp\tnull\nmodified\ttimestamp\tnull"}]},"apps":[],"jobName":"paragraph_1617123747325_1565202895","id":"20200111-173912_1607664098","dateCreated":"2021-03-30T10:02:27-0700","dateStarted":"2021-03-30T11:26:51-0700","dateFinished":"2021-03-30T11:26:58-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:236292"},{"text":"%md\nThe Livy interprepter provides you with a Spark SQL environment that allows you directly to interact with Hive tables. The %sql provides a SQL like environment \nallowing straight SQL commands.\n\nThis exercise uses a DataFrame based on the `account` table in the Hive database. \n\nOptionally you can use the Hive beeline CLI\n~~~\nbeeline -u jdbc:hive2://localhost:10000 -e \"DESCRIBE devsh.accounts\"\n~~~\n\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The Livy interprepter provides you with a Spark SQL environment that allows you directly to interact with Hive tables. The %sql provides a SQL like environment\n<br  />allowing straight SQL commands.</p>\n<p>This exercise uses a DataFrame based on the <code>account</code> table in the Hive database.</p>\n<p>Optionally you can use the Hive beeline CLI</p>\n<pre><code>beeline -u jdbc:hive2://localhost:10000 -e \"DESCRIBE devsh.accounts\"\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617123747325_-2013808004","id":"20210121-130036_113149906","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236293"},{"title":"2 - Create a new DataFrame using the Hive devsh.accounts table","text":"%pyspark\n\naccountsDF = spark.read.table(\"devsh.accounts\")","user":"anonymous","dateUpdated":"2021-03-30T11:31:59-0700","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1617123747325_-1680629779","id":"20200111-175424_830219293","dateCreated":"2021-03-30T10:02:27-0700","dateStarted":"2021-03-30T11:31:59-0700","dateFinished":"2021-03-30T11:32:00-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:236294"},{"text":"%md\nThe `table` function uses a jdbc connection to locates the Hive metastore to pull in schema information from the hive RDBMS database. This also passes the \nconnect string for HiveServer2 to the Spark driver. \n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The <code>table</code> function uses a jdbc connection to locates the Hive metastore to pull in schema information from the hive RDBMS database. This also passes the\n<br  />connect string for HiveServer2 to the Spark driver.</p>\n"}]},"apps":[],"jobName":"paragraph_1617123747325_-1933231410","id":"20210122-182010_1298352692","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236295"},{"title":"3 - View the dataframe and confirm alignment with the Hive table","text":"%pyspark\n\nsc.setJobGroup(\"Working with DataFrames and schemas\",\"Show the accounts data\")\naccountsDF.printSchema()","user":"anonymous","dateUpdated":"2021-03-30T11:32:14-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"acct_num":"string","acct_create_dt":"string","acct_close_dt":"string","first_name":"string","last_name":"string","address":"string","city":"string","state":"string","zipcode":"string","phone_number":"string","created":"string","modified":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- acct_num: integer (nullable = true)\n |-- acct_create_dt: timestamp (nullable = true)\n |-- acct_close_dt: timestamp (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- zipcode: string (nullable = true)\n |-- phone_number: string (nullable = true)\n |-- created: timestamp (nullable = true)\n |-- modified: timestamp (nullable = true)"}]},"apps":[],"jobName":"paragraph_1617123747325_-1754012714","id":"20200111-175849_1440047182","dateCreated":"2021-03-30T10:02:27-0700","dateStarted":"2021-03-30T11:32:14-0700","dateFinished":"2021-03-30T11:32:15-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:236296"},{"text":"%pyspark\n\naccountsDF.show(5)","user":"anonymous","dateUpdated":"2021-03-30T11:33:26-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------+-------------------+-------------+----------+---------+--------------------+-------------+-----+-------+------------+-------------------+-------------------+\n|acct_num|     acct_create_dt|acct_close_dt|first_name|last_name|             address|         city|state|zipcode|phone_number|            created|           modified|\n+--------+-------------------+-------------+----------+---------+--------------------+-------------+-----+-------+------------+-------------------+-------------------+\n|       1|2008-10-23 16:05:05|         null|    Donald|   Becton|2275 Washburn Street|      Oakland|   CA|  94660|  5100032418|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|       2|2008-11-12 03:00:01|         null|     Donna|    Jones| 3885 Elliott Street|San Francisco|   CA|  94171|  4150835799|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|       3|2008-12-21 09:19:50|         null|    Dorthy| Chalmers|    4073 Whaley Lane|    San Mateo|   CA|  94479|  6506877757|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|       4|2008-11-28 00:08:09|         null|     Leila|  Spencer|    1447 Ross Street|    San Mateo|   CA|  94444|  6503198619|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|       5|2008-11-15 23:06:06|         null|     Anita| Laughlin|    2767 Hill Street|     Richmond|   CA|  94872|  5107754354|2014-03-18 13:29:47|2014-03-18 13:29:47|\n+--------+-------------------+-------------+----------+---------+--------------------+-------------+-----+-------+------------+-------------------+-------------------+\nonly showing top 5 rows"}]},"apps":[],"jobName":"paragraph_1617123747325_419947220","id":"20210129-184513_776662024","dateCreated":"2021-03-30T10:02:27-0700","dateStarted":"2021-03-30T11:33:26-0700","dateFinished":"2021-03-30T11:33:31-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:236297"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2021-03-30T11:42:13-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617129733846_-1287472304","id":"20210330-114213_1892120575","dateCreated":"2021-03-30T11:42:13-0700","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:236298"},{"text":"%pyspark\n\naccountsDF.count()","user":"anonymous","dateUpdated":"2021-03-30T11:40:49-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"129762"}]},"apps":[],"jobName":"paragraph_1617123747326_-885620729","id":"20210127-115238_764609997","dateCreated":"2021-03-30T10:02:27-0700","dateStarted":"2021-03-30T11:40:49-0700","dateFinished":"2021-03-30T11:40:52-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:236299"},{"text":"%md\nPrint the schema and the first 10 rows of the DataFrame. Compare that the Dataframe schema aligns with that of the Hive table.\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Print the schema and the first 10 rows of the DataFrame. Compare that the Dataframe schema aligns with that of the Hive table.</p>\n"}]},"apps":[],"jobName":"paragraph_1617123747326_-356081675","id":"20210121-134836_1444803397","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236300"},{"title":"4 - Create a new dataframe and save it to HDFS","text":"%pyspark\n\nsc.setJobGroup(\"Working with DataFrames and schemas\",\"Write the filtered accounts data\")\naccountsDF.where(\"zipcode = 94913\").write.option(\"header\",\"true\").csv(\"/devsh_loudacre/accounts_zip94913\")","user":"anonymous","dateUpdated":"2021-03-30T11:44:08-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1617123747326_135037685","id":"20200111-180302_674401673","dateCreated":"2021-03-30T10:02:27-0700","dateStarted":"2021-03-30T11:44:08-0700","dateFinished":"2021-03-30T11:44:12-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:236301"},{"text":"%md\nThe `where` function is used to select rows. \n\nThe `write` function is used to write data from Spark to HDFS. Spark will write to a large number of data formats. Common data types include: .avro, .csv, \n.json, and .text\n\nThe `option` function will pass in parameters; such as `header` = true, to include the schema on the first row.\n\nCreate a new DataFrame with rows from the accounts data where the zip code is 94913, and save the result to CSV files in the /devsh_loudacre/ accounts_zip94913 \nHDFS directory.\n\n\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The <code>where</code> function is used to select rows.</p>\n<p>The <code>write</code> function is used to write data from Spark to HDFS. Spark will write to a large number of data formats. Common data types include: .avro, .csv,\n<br  />.json, and .text</p>\n<p>The <code>option</code> function will pass in parameters; such as <code>header</code> = true, to include the schema on the first row.</p>\n<p>Create a new DataFrame with rows from the accounts data where the zip code is 94913, and save the result to CSV files in the /devsh_loudacre/ accounts_zip94913\n<br  />HDFS directory.</p>\n"}]},"apps":[],"jobName":"paragraph_1617123747326_-721525101","id":"20210121-140054_655193673","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236302"},{"title":"5 - Confirm successful write to HDFS","text":"%sh\n\nhdfs dfs -ls /devsh_loudacre/accounts_zip94913\n","user":"anonymous","dateUpdated":"2021-03-30T11:45:42-0700","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 5 items\n-rw-r--r--   3 livy supergroup          0 2021-03-30 11:44 /devsh_loudacre/accounts_zip94913/_SUCCESS\n-rw-r--r--   3 livy supergroup        973 2021-03-30 11:44 /devsh_loudacre/accounts_zip94913/part-00000-71ac41e7-a364-41a9-b833-2980fb632262-c000.csv\n-rw-r--r--   3 livy supergroup       1134 2021-03-30 11:44 /devsh_loudacre/accounts_zip94913/part-00001-71ac41e7-a364-41a9-b833-2980fb632262-c000.csv\n-rw-r--r--   3 livy supergroup       1425 2021-03-30 11:44 /devsh_loudacre/accounts_zip94913/part-00002-71ac41e7-a364-41a9-b833-2980fb632262-c000.csv\n-rw-r--r--   3 livy supergroup       1676 2021-03-30 11:44 /devsh_loudacre/accounts_zip94913/part-00003-71ac41e7-a364-41a9-b833-2980fb632262-c000.csv\n"}]},"apps":[],"jobName":"paragraph_1617123747326_1570802179","id":"20200111-180700_593022127","dateCreated":"2021-03-30T10:02:27-0700","dateStarted":"2021-03-30T11:45:42-0700","dateFinished":"2021-03-30T11:45:44-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:236303"},{"text":"%sh\n\nhdfs dfs -head /devsh_loudacre/accounts_zip94913/part-00000-71ac41e7-a364-41a9-b833-2980fb632262-c000.csv","user":"anonymous","dateUpdated":"2021-03-30T11:46:28-0700","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"acct_num,acct_create_dt,acct_close_dt,first_name,last_name,address,city,state,zipcode,phone_number,created,modified\n19,2008-11-13T23:40:53.000-08:00,2014-02-26T18:44:26.000-08:00,Leona,Bray,364 Romrog Way,Santa Rosa,CA,94913,7070013038,2014-03-18T13:29:47.000-07:00,2014-03-18T13:29:47.000-07:00\n3156,2009-07-03T11:04:35.000-07:00,\"\",Stephen,Bisson,1116 Daylene Drive,Santa Rosa,CA,94913,7075647641,2014-03-18T13:29:53.000-07:00,2014-03-18T13:29:53.000-07:00\n5536,2010-05-15T14:09:32.000-07:00,\"\",Steven,Jones,4818 Copperhead Road,Santa Rosa,CA,94913,7079211079,2014-03-18T13:29:57.000-07:00,2014-03-18T13:29:57.000-07:00\n6320,2010-09-22T01:35:25.000-07:00,\"\",Victor,Cheatham,2564 Center Avenue,Santa Rosa,CA,94913,7078711417,2014-03-18T13:29:59.000-07:00,2014-03-18T13:29:59.000-07:00\n19938,2011-06-22T02:18:05.000-07:00,2014-01-27T22:10:49.000-08:00,David,Reeves,2971 Biddie Lane,Santa Rosa,CA,94913,7071048775,2014-03-18T13:30:23.000-07:00,2014-03-18T13:30:23.000-07:00\n"}]},"apps":[],"jobName":"paragraph_1617123747326_31338981","id":"20210122-182843_1309430737","dateCreated":"2021-03-30T10:02:27-0700","dateStarted":"2021-03-30T11:46:28-0700","dateFinished":"2021-03-30T11:46:31-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:236304"},{"text":"%md\nUse hdfs to view the /devsh_loudacre/accounts_zip94913 directory in HDFS and the data in one of the saved files. Copy and paste one of the files from \naccount_zip94913 directory into the hdfs dfs -head command. Confirm that the CSV file includes a header line, and that only records for the selected zip \ncode are included.\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Use hdfs to view the /devsh_loudacre/accounts_zip94913 directory in HDFS and the data in one of the saved files. Copy and paste one of the files from\n<br  />account_zip94913 directory into the hdfs dfs -head command. Confirm that the CSV file includes a header line, and that only records for the selected zip\n<br  />code are included.</p>\n"}]},"apps":[],"jobName":"paragraph_1617123747327_-2120726736","id":"20210121-140625_1512204011","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236305"},{"title":"6 - Compare the inferSchema option","text":"%pyspark\n\nsc.setJobGroup(\"Working with DataFrames and schemas\",\"Read a csv file: no options\")\nspark.read.csv(\"/devsh_loudacre/accounts_zip94913\").printSchema()\n\nsc.setJobGroup(\"Working with DataFrames and schemas\",\"Read a csv file: header=true\")\nspark.read.option(\"header\",\"true\").csv(\"/devsh_loudacre/accounts_zip94913\").printSchema()\n\nsc.setJobGroup(\"Working with DataFrames and schemas\",\"Read a csv file: header=true inferSchema=true\")\nspark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/devsh_loudacre/accounts_zip94913\").printSchema()\n","user":"anonymous","dateUpdated":"2021-03-30T11:51:45-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n |-- _c3: string (nullable = true)\n |-- _c4: string (nullable = true)\n |-- _c5: string (nullable = true)\n |-- _c6: string (nullable = true)\n |-- _c7: string (nullable = true)\n |-- _c8: string (nullable = true)\n |-- _c9: string (nullable = true)\n |-- _c10: string (nullable = true)\n |-- _c11: string (nullable = true)\n\nroot\n |-- acct_num: string (nullable = true)\n |-- acct_create_dt: string (nullable = true)\n |-- acct_close_dt: string (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- zipcode: string (nullable = true)\n |-- phone_number: string (nullable = true)\n |-- created: string (nullable = true)\n |-- modified: string (nullable = true)\n\nroot\n |-- acct_num: integer (nullable = true)\n |-- acct_create_dt: timestamp (nullable = true)\n |-- acct_close_dt: timestamp (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- zipcode: integer (nullable = true)\n |-- phone_number: long (nullable = true)\n |-- created: timestamp (nullable = true)\n |-- modified: timestamp (nullable = true)"}]},"apps":[],"jobName":"paragraph_1617123747327_-1004541052","id":"20200111-182021_1532001378","dateCreated":"2021-03-30T10:02:27-0700","dateStarted":"2021-03-30T11:51:45-0700","dateFinished":"2021-03-30T11:51:48-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:236306"},{"text":"%md\n#### The options for inferring schemas\nNo options assigns an alphanumeric sequence to all fields and casts all fields as data type string.\n\nHeader = true will assign field names but will cast all fields as data type string.\n\nHeader = true, Infer Schema = true will assign filed names and assign data types. \n\n\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>The options for inferring schemas</h4>\n<p>No options assigns an alphanumeric sequence to all fields and casts all fields as data type string.</p>\n<p>Header = true will assign field names but will cast all fields as data type string.</p>\n<p>Header = true, Infer Schema = true will assign filed names and assign data types.</p>\n"}]},"apps":[],"jobName":"paragraph_1617123747327_508415967","id":"20210122-183034_1388155243","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236307"},{"text":"%md\n### Define a Schema for a DataFrame\nIf you have not done so yet, review the data in the HDFS file /devsh_loudacre/devices.json\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Define a Schema for a DataFrame</h3>\n<p>If you have not done so yet, review the data in the HDFS file /devsh_loudacre/devices.json</p>\n"}]},"apps":[],"jobName":"paragraph_1617123747327_-1580172692","id":"20210122-190110_2062914929","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236308"},{"title":"7 - Review the data in /devsh_loudacre/devices.json","text":"%sh\n\nhdfs dfs -head /devsh_loudacre/devices.json","user":"anonymous","dateUpdated":"2021-03-30T12:05:04-0700","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"{\"devnum\":1,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"Sorrento\",\"model\":\"F00L\",\"dev_type\":\"phone\"}\n{\"devnum\":2,\"release_dt\":\"2010-04-19T00:00:00.000-07:00\",\"make\":\"Titanic\",\"model\":\"2100\",\"dev_type\":\"phone\"}\n{\"devnum\":3,\"release_dt\":\"2011-02-18T00:00:00.000-08:00\",\"make\":\"MeeToo\",\"model\":\"3.0\",\"dev_type\":\"phone\"}\n{\"devnum\":4,\"release_dt\":\"2011-09-21T00:00:00.000-07:00\",\"make\":\"MeeToo\",\"model\":\"3.1\",\"dev_type\":\"phone\"}\n{\"devnum\":5,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"1\",\"dev_type\":\"phone\"}\n{\"devnum\":6,\"release_dt\":\"2011-11-02T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"3\",\"dev_type\":\"phone\"}\n{\"devnum\":7,\"release_dt\":\"2010-05-20T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"2\",\"dev_type\":\"phone\"}\n{\"devnum\":8,\"release_dt\":\"2013-07-02T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"5\",\"dev_type\":\"phone\"}\n{\"devnum\":9,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"Titanic\",\"model\":\"1000\",\"dev_type\":\"phone\"}\n{\"devnum\":10,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"mak"}]},"apps":[],"jobName":"paragraph_1617123747327_1526366081","id":"20200111-183034_19858163","dateCreated":"2021-03-30T10:02:27-0700","dateStarted":"2021-03-30T12:05:04-0700","dateFinished":"2021-03-30T12:05:06-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:236309"},{"title":"8 - Read the devices.json file into a devDF dataframe","text":"%pyspark\n\nsc.setJobGroup(\"Working with DataFrames and schemas\",\"Read the devices.json file\")\ndevDF = spark.read.json(\"/devsh_loudacre/devices.json\")","user":"anonymous","dateUpdated":"2021-03-30T12:05:27-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1617123747328_705102110","id":"20200111-183406_507235503","dateCreated":"2021-03-30T10:02:27-0700","dateStarted":"2021-03-30T12:05:27-0700","dateFinished":"2021-03-30T12:05:28-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:236310"},{"text":"%md\nCreate a new DataFrame based on the devices.json file. (This command could take several seconds while it infers the schema.)\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame based on the devices.json file. (This command could take several seconds while it infers the schema.)</p>\n"}]},"apps":[],"jobName":"paragraph_1617123747328_-1579011616","id":"20210121-140752_339312179","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236311"},{"title":"9- View the schema of the devDF dataframe","text":"%pyspark\n\ndevDF.printSchema()\ndevDF.show(5)","user":"anonymous","dateUpdated":"2021-03-30T12:05:46-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- dev_type: string (nullable = true)\n |-- devnum: long (nullable = true)\n |-- make: string (nullable = true)\n |-- model: string (nullable = true)\n |-- release_dt: string (nullable = true)\n\n+--------+------+--------+-----+--------------------+\n|dev_type|devnum|    make|model|          release_dt|\n+--------+------+--------+-----+--------------------+\n|   phone|     1|Sorrento| F00L|2008-10-21T00:00:...|\n|   phone|     2| Titanic| 2100|2010-04-19T00:00:...|\n|   phone|     3|  MeeToo|  3.0|2011-02-18T00:00:...|\n|   phone|     4|  MeeToo|  3.1|2011-09-21T00:00:...|\n|   phone|     5|  iFruit|    1|2008-10-21T00:00:...|\n+--------+------+--------+-----+--------------------+\nonly showing top 5 rows"}]},"apps":[],"jobName":"paragraph_1617123747328_1881315589","id":"20200111-185504_2076885456","dateCreated":"2021-03-30T10:02:27-0700","dateStarted":"2021-03-30T12:05:46-0700","dateFinished":"2021-03-30T12:05:47-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:236312"},{"text":"%md\nView the schema of the `devDF` DataFrame. Note the column names and types that Spark inferred from the JSON file. In particular, note that the `release_dt` \ncolumn is of type `string`, whereas the data in the column actually represents a timestamp.\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>View the schema of the <code>devDF</code> DataFrame. Note the column names and types that Spark inferred from the JSON file. In particular, note that the <code>release_dt</code>\n<br  />column is of type <code>string</code>, whereas the data in the column actually represents a timestamp.</p>\n"}]},"apps":[],"jobName":"paragraph_1617123747328_1095349882","id":"20210121-142717_1842020453","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236313"},{"title":"10 - Define a schema","text":"%pyspark\n\nfrom pyspark.sql.types import *\n\ndevColumns = [ \n    StructField(\"devnum\",LongType()), \n    StructField(\"make\",StringType()), \n    StructField(\"model\",StringType()), \n    StructField(\"release_dt\",TimestampType()), \n    StructField(\"dev_type\",StringType())\n    ]\n","user":"anonymous","dateUpdated":"2021-03-30T12:09:39-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1617123747328_-402769843","id":"20200111-190012_576363507","dateCreated":"2021-03-30T10:02:27-0700","dateStarted":"2021-03-30T12:09:39-0700","dateFinished":"2021-03-30T12:09:40-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:236314"},{"text":"%md\n#### Creating a column object\nDefine a schema that correctly specifies the data types for each column of this DataFrame. \n\nStart by importing the package with the definitions of necessary classes and types.\n\nA `StructField` object is used to define the field name and the data type. A schema is defined as a collection of `StructFields`.\n\nNext, create a collection of `StructField` objects, which represent column definitions. The `release_dt` column should be a timestamp.\n\n```spark\nfrom pyspark.sql.types import *\n\ndevColumns = [ StructField(\"devnum\",LongType()), \n    StructField(\"make\",StringType()), \n    StructField(\"model\",StringType()), \n    StructField(\"release_dt\",TimestampType()), \n    StructField(\"dev_type\",StringType())]\n```\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Creating a column object</h4>\n<p>Define a schema that correctly specifies the data types for each column of this DataFrame.</p>\n<p>Start by importing the package with the definitions of necessary classes and types.</p>\n<p>A <code>StructField</code> object is used to define the field name and the data type. A schema is defined as a collection of <code>StructFields</code>.</p>\n<p>Next, create a collection of <code>StructField</code> objects, which represent column definitions. The <code>release_dt</code> column should be a timestamp.</p>\n<pre><code class=\"spark\">from pyspark.sql.types import *\n\ndevColumns = [ StructField(\"devnum\",LongType()), \n    StructField(\"make\",StringType()), \n    StructField(\"model\",StringType()), \n    StructField(\"release_dt\",TimestampType()), \n    StructField(\"dev_type\",StringType())]\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1617123747329_475367193","id":"20210121-140904_185066472","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236315"},{"title":"11 - Create a schema (a StructType object) using the column definition list","text":"%pyspark\n\ndevSchema = StructType(devColumns)","user":"anonymous","dateUpdated":"2021-03-30T12:10:33-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1617123747329_740375218","id":"20200111-190405_1345983159","dateCreated":"2021-03-30T10:02:27-0700","dateStarted":"2021-03-30T12:10:33-0700","dateFinished":"2021-03-30T12:10:35-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:236316"},{"text":"%md\n#### Creating a schema object\nCreate a schema (a `StructType` object) using the column definition list.\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Creating a schema object</h4>\n<p>Create a schema (a <code>StructType</code> object) using the column definition list.</p>\n"}]},"apps":[],"jobName":"paragraph_1617123747329_1093813444","id":"20210121-143155_1071743868","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236317"},{"title":"12 - Recreate the devDF DataFrame, this time using the new schema","text":"%pyspark\n\nsc.setJobGroup(\"Working with DataFrames and schemas\",\"Read the devices.json file with a schema\")\ndevDF = spark.read.schema(devSchema).json(\"/devsh_loudacre/devices.json\")\n","user":"anonymous","dateUpdated":"2021-03-30T12:12:09-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1617123747329_170502552","id":"20200111-190932_1097085748","dateCreated":"2021-03-30T10:02:27-0700","dateStarted":"2021-03-30T12:12:09-0700","dateFinished":"2021-03-30T12:12:10-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:236318"},{"title":"13 - View the schema and data of the new DataFrame, and confirm that the release_dt column type is now timestamp","text":"%pyspark\n\nsc.setJobGroup(\"Working with DataFrames and schemas\",\"Show the devices DataFrame\")\ndevDF.printSchema()\ndevDF.show(5)","user":"anonymous","dateUpdated":"2021-03-30T12:12:18-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{"1":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"devnum":"string","make":"string","model":"string","release_dt":"string","dev_type":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- devnum: long (nullable = true)\n |-- make: string (nullable = true)\n |-- model: string (nullable = true)\n |-- release_dt: timestamp (nullable = true)\n |-- dev_type: string (nullable = true)\n\n+------+--------+-----+-------------------+--------+\n|devnum|    make|model|         release_dt|dev_type|\n+------+--------+-----+-------------------+--------+\n|     1|Sorrento| F00L|2008-10-21 00:00:00|   phone|\n|     2| Titanic| 2100|2010-04-19 00:00:00|   phone|\n|     3|  MeeToo|  3.0|2011-02-18 00:00:00|   phone|\n|     4|  MeeToo|  3.1|2011-09-21 00:00:00|   phone|\n|     5|  iFruit|    1|2008-10-21 00:00:00|   phone|\n+------+--------+-----+-------------------+--------+\nonly showing top 5 rows"}]},"apps":[],"jobName":"paragraph_1617123747330_1101756620","id":"20200111-190849_1182876565","dateCreated":"2021-03-30T10:02:27-0700","dateStarted":"2021-03-30T12:12:18-0700","dateFinished":"2021-03-30T12:12:19-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:236319"},{"text":"%md\nView the schema and data of the new DataFrame, and confirm that the `release_dt` column type is now of type `timestamp`.\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>View the schema and data of the new DataFrame, and confirm that the <code>release_dt</code> column type is now of type <code>timestamp</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1617123747330_-1684502582","id":"20210121-143308_2141220186","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236320"},{"title":"14 - Save the devDF dataframe to /devsh_loudacre/devices_parquet using the parquet format","text":"%pyspark\n\nsc.setJobGroup(\"Working with DataFrames and schemas\",\"Write the devices DataFrame in parquet format\")\ndevDF.write.parquet(\"/devsh_loudacre/devices_parquet\")","user":"anonymous","dateUpdated":"2021-03-30T12:17:24-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1617123747330_-1552960530","id":"20200111-191831_2119240237","dateCreated":"2021-03-30T10:02:27-0700","dateStarted":"2021-03-30T12:17:24-0700","dateFinished":"2021-03-30T12:17:26-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:236321"},{"text":"%md\nNow that the device data uses the correct schema, write the data into Parquet format, which automatically embeds the schema. Save the Parquet data files \ninto an HDFS directory called /devsh_loudacre/devices_parquet.\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Now that the device data uses the correct schema, write the data into Parquet format, which automatically embeds the schema. Save the Parquet data files\n<br  />into an HDFS directory called /devsh_loudacre/devices_parquet.</p>\n"}]},"apps":[],"jobName":"paragraph_1617123747330_1356063548","id":"20210121-141421_1547432313","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236322"},{"title":"15 - Confirm the schema of the saved files using parquet-tools","text":"%sh\n\nrm -r /tmp/devices_parquet\nhdfs dfs -get /devsh_loudacre/devices_parquet /tmp/\n\nparquet-tools schema /tmp/devices_parquet/\nparquet-tools head /tmp/devices_parquet/","user":"anonymous","dateUpdated":"2021-03-30T12:19:16-0700","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"rm: cannot remove ‘/tmp/devices_parquet’: No such file or directory\nJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0\nJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0\nmessage spark_schema {\n  optional int64 devnum;\n  optional binary make (STRING);\n  optional binary model (STRING);\n  optional int96 release_dt;\n  optional binary dev_type (STRING);\n}\n\nJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0\nJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0\ndevnum = 1\nmake = Sorrento\nmodel = F00L\nrelease_dt = AGAMVesWAADpdCUA\ndev_type = phone\n\ndevnum = 2\nmake = Titanic\nmodel = 2100\nrelease_dt = AGAMVesWAAAKdyUA\ndev_type = phone\n\ndevnum = 3\nmake = MeeToo\nmodel = 3.0\nrelease_dt = AADFhTEaAAA7eCUA\ndev_type = phone\n\ndevnum = 4\nmake = MeeToo\nmodel = 3.1\nrelease_dt = AGAMVesWAAASeSUA\ndev_type = phone\n\ndevnum = 5\nmake = iFruit\nmodel = 1\nrelease_dt = AGAMVesWAADpdCUA\ndev_type = phone\n\n"}]},"apps":[],"jobName":"paragraph_1617123747330_-1070014857","id":"20200111-192819_852011514","dateCreated":"2021-03-30T10:02:27-0700","dateStarted":"2021-03-30T12:19:16-0700","dateFinished":"2021-03-30T12:19:21-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:236323"},{"text":"%md\n### Optional\n\nUse `parquet-tools` to view the schema of the saved files. First download the HDFS directory (or an individual file), then run the command.\n\n```shell\nhdfs dfs -get /devsh_loudacre/devices_parquet /tmp/ \nparquet-tools schema /tmp/devices_parquet/\nparquet-tools head /tmp/devices_parquet/\n```\n\nNote that the type of the `release_dt` column is noted as `int96`; this is how Spark denotes a timestamp type in Parquet. For more information about \n`parquet-tools`, run `parquet-tools --help`.\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Optional</h3>\n<p>Use <code>parquet-tools</code> to view the schema of the saved files. First download the HDFS directory (or an individual file), then run the command.</p>\n<pre><code class=\"shell\">hdfs dfs -get /devsh_loudacre/devices_parquet /tmp/ \nparquet-tools schema /tmp/devices_parquet/\nparquet-tools head /tmp/devices_parquet/\n</code></pre>\n<p>Note that the type of the <code>release_dt</code> column is noted as <code>int96</code>; this is how Spark denotes a timestamp type in Parquet. For more information about\n<br  /><code>parquet-tools</code>, run <code>parquet-tools --help</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1617123747330_1824175609","id":"20210121-141607_1157000296","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236324"},{"title":"16 - Read the parquet files back in a dataframe to confirm the schema is well saved","text":"%pyspark\n\nsc.setJobGroup(\"Working with DataFrames and schemas\",\"Read the devices DataFrame in parquet format\")\nspark.read.parquet(\"/devsh_loudacre/devices_parquet\").printSchema()","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- devnum: long (nullable = true)\n |-- make: string (nullable = true)\n |-- model: string (nullable = true)\n |-- release_dt: timestamp (nullable = true)\n |-- dev_type: string (nullable = true)"}]},"apps":[],"jobName":"paragraph_1617123747331_-686206223","id":"20200111-193316_622744995","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236325"},{"text":"%md\nCreate a new DataFrame using the Parquet files you saved in devices_parquet and view its schema. Note that Spark is able to correctly infer the timestamp \ntype of the release_dt column from Parquet’s embedded schema.\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame using the Parquet files you saved in devices_parquet and view its schema. Note that Spark is able to correctly infer the timestamp\n<br  />type of the release_dt column from Parquet’s embedded schema.</p>\n"}]},"apps":[],"jobName":"paragraph_1617123747331_916212817","id":"20210121-141733_613796356","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236326"},{"text":"%md\n# Tear Down\n---","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Tear Down</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1617123747331_-145269069","id":"20200830-075248_1182047608","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236327"},{"title":"Delete existing HDFS files to prevent file exists errors","text":"%sh\n\nhdfs dfs -rm -r -skipTrash /devsh_loudacre/accounts_zip94913\nhdfs dfs -rm -r -skipTrash /devsh_loudacre/devices_parquet\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Deleted /devsh_loudacre/accounts_zip94913\nDeleted /devsh_loudacre/devices_parquet\n"}]},"apps":[],"jobName":"paragraph_1617123747331_746157260","id":"20210202-234612_319924165","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236328"},{"title":"Delete the Livy session","text":"%sh\n\nsessionId=$(curl -s localhost:8998/sessions | jq '.sessions[0].id')\ncurl -s localhost:8998/sessions/$sessionId -X DELETE","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"{\"msg\":\"deleted\"}"}]},"apps":[],"jobName":"paragraph_1617123747331_1574983770","id":"20200830-075258_565376786","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236329"},{"title":"Additional resources","text":"%md\nWe hope you've enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Cloudera Tutorials](http://cloudera.com/tutorials.html) are your natural next step where you can explore Spark in more depth.\n2. [Cloudera Community](https://community.cloudera.com) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Apache Spark Documentation](https://spark.apache.org/documentation.html) - official Spark documentation.\n4. [Apache Zeppelin Project Home Page](https://zeppelin.apache.org) - official Zeppelin web site.","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":10,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We hope you've enjoyed this lab. Below are additional resources that you should find useful:</p>\n<ol>\n<li><a href=\"http://cloudera.com/tutorials.html\">Cloudera Tutorials</a> are your natural next step where you can explore Spark in more depth.</li>\n<li><a href=\"https://community.cloudera.com\">Cloudera Community</a> is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.</li>\n<li><a href=\"https://spark.apache.org/documentation.html\">Apache Spark Documentation</a> - official Spark documentation.</li>\n<li><a href=\"https://zeppelin.apache.org\">Apache Zeppelin Project Home Page</a> - official Zeppelin web site.</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1617123747332_633910329","id":"20181116-135131_93712280","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236330"},{"text":"%angular\n</br>\n</br>\n</br>\n</br>\n<center>\n<a href=\"https://www.cloudera.com/about/training/courses.html\">\n  <img src=\"https://www.cloudera.com/content/dam/www/marketing/media-kit/logo-assets/cloudera_logo_darkorange.png\" alt=\"Cloudera University\" style=\"width:280px;height:40px;border:0;\" align=\"middle\">\n</a>\n</center>\n</br>\n</br>","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":2,"editorMode":"ace/mode/undefined","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"ANGULAR","data":"</br>\n</br>\n</br>\n</br>\n<center>\n<a href=\"https://www.cloudera.com/about/training/courses.html\">\n  <img src=\"https://www.cloudera.com/content/dam/www/marketing/media-kit/logo-assets/cloudera_logo_darkorange.png\" alt=\"Cloudera University\" style=\"width:280px;height:40px;border:0;\" align=\"middle\">\n</a>\n</center>\n</br>\n</br>"}]},"apps":[],"jobName":"paragraph_1617123747332_-1905453491","id":"20200110-154537_1406191376","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236331"},{"text":"%angular\n","user":"anonymous","dateUpdated":"2021-03-30T10:02:27-0700","config":{"editorSetting":{"language":"scala","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/undefined","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1617123747332_-1499020561","id":"20200110-162013_302547143","dateCreated":"2021-03-30T10:02:27-0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236332"}],"name":"/DevSH/Pyspark/DataFrame","id":"2G4KYCS2H","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"angular:shared_process":[],"sh:shared_process":[],"livy:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}