{"paragraphs":[{"text":"%md\n# About This Lab\n**Objective:** To become familiar with how files are added to and removed from HDFS and how to view files in HDFS.\n**File locations:**\n**Successful outcome:** You will have added and deleted several files and folders in HDFS.\n**Before you begin:**\n**Related lessons:** The Hadoop Distributed File System (HDFS)\n\n---","dateUpdated":"2020-01-05T19:34:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578252831656_-1181034597","id":"20171105-200834_1116095891","dateCreated":"2020-01-05T19:33:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:69046","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>About This Lab</h1>\n<p><strong>Objective:</strong> To become familiar with how files are added to and removed from HDFS and how to view files in HDFS.\n<br  /><strong>File locations:</strong>\n<br  /><strong>Successful outcome:</strong> You will have added and deleted several files and folders in HDFS.\n<br  /><strong>Before you begin:</strong>\n<br  /><strong>Related lessons:</strong> The Hadoop Distributed File System (HDFS)</p>\n<hr />\n"}]}},{"text":"%md\n# Setup\n---","dateUpdated":"2020-01-05T19:34:15+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578252831656_-1776915576","id":"20181114-164229_902436001","dateCreated":"2020-01-05T19:33:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69047","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Setup</h1>\n<hr />\n"}]}},{"title":"Retrieve the data from S3","text":"%sh\nrm -f Rev6.zip\nrm -f data.txt\nrm -f small_blocks.txt\nrm -f -R DevPH_Labs-Rev6\nwget https://s3-us-west-1.amazonaws.com/sci-241/HortonworksUniversity-DevPH_Labs-archive/Rev6.zip\nunzip Rev6.zip\nmv DevPH_Labs-Rev6/labs/Lab2.1/data.txt .\nmv DevPH_Labs-Rev6/labs/demos/small_blocks.txt .\nhdfs dfs -test -d test && hdfs dfs -rm -R test || echo \"\"","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381851_-1748032487","id":"20181206-194903_1771375579","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69048"},{"text":"%md\n# Lab\n---","dateUpdated":"2020-01-05T19:34:21+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578252831656_1314264978","id":"20181114-164844_1661453681","dateCreated":"2020-01-05T19:33:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69049","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Lab</h1>\n<hr />\n"}]}},{"title":"1 - View the hdfs dfs Command","text":"","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381852_979489436","id":"20171105-200623_656362182","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69050"},{"text":"%md\nNotice that the usage contains options for performing file system tasks in HDFS, like copying files from a local folder into HDFS, retrieving a file from HDFS,\ncopying and moving files around, and making and removing directories. \nIn this lab, you will perform these commands, and many others, to help you become comfortable with working with HDFS.\n\nEnter the following command to view the contents of the user’s zeppelin directory in HDFS, which is /user/zeppelin\n~~~\nhdfs dfs -ls\n~~~","dateUpdated":"2020-01-05T19:34:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578252831656_1853130598","id":"20171105-201923_413365117","dateCreated":"2020-01-05T19:33:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69051","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Notice that the usage contains options for performing file system tasks in HDFS, like copying files from a local folder into HDFS, retrieving a file from HDFS,\n<br  />copying and moving files around, and making and removing directories.\n<br  />In this lab, you will perform these commands, and many others, to help you become comfortable with working with HDFS.</p>\n<p>Enter the following command to view the contents of the user’s zeppelin directory in HDFS, which is /user/zeppelin</p>\n<pre><code>hdfs dfs -ls\n</code></pre>\n"}]}},{"title":"2 - View the content of the zeppelin user directory in HDFS","text":"","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381852_1462480624","id":"20181206-185011_517182192","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69052"},{"title":"3 - View the content of the root directory in HDFS","text":"","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381853_-1325419960","id":"20171105-201449_1118165660","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69053"},{"text":"%md\n**Important**\n\nNotice how adding the / in the -ls command caused the contents of the root folder to display, but leaving off the / showed the contents of /user/zeppelin, which is the default prefix if you leave off the leading / on any of the hadoop commands (assuming the command is run by the “zeppelin” user).","dateUpdated":"2020-01-05T19:34:30+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578252831656_1874532036","id":"20181206-190311_1187608924","dateCreated":"2020-01-05T19:33:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69054","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><strong>Important</strong></p>\n<p>Notice how adding the / in the -ls command caused the contents of the root folder to display, but leaving off the / showed the contents of /user/zeppelin, which is the default prefix if you leave off the leading / on any of the hadoop commands (assuming the command is run by the “zeppelin” user).</p>\n"}]}},{"text":"%md\nEnter the following command to create a directory named test in HDFS:\n~~~\nhdfs dfs -mkdir test\n~~~\n","dateUpdated":"2020-01-05T19:34:32+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578252831656_-223616971","id":"20181206-190911_1689211983","dateCreated":"2020-01-05T19:33:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69055","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Enter the following command to create a directory named test in HDFS:</p>\n<pre><code>hdfs dfs -mkdir test\n</code></pre>\n"}]}},{"title":"4 - Create a directory named test in HDFS","text":"","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381854_-1446286242","id":"20181206-191002_2092689030","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69056"},{"title":"5 - Verify that the folder was created successfully","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381854_1387490095","id":"20181206-191314_68186558","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69057"},{"text":"%md\nCreate a couple of subdirectories for test:\n~~~\nhdfs dfs -mkdir test/test1\nhdfs dfs -mkdir -p test/test2/test3\n~~~","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381854_-1316883294","id":"20181206-191742_505751640","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69058"},{"title":"6 - Create a couple of subdirectories for test","text":"","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381855_1511460816","id":"20181206-191851_1257798774","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69059"},{"text":"%md\nNotice how the -p command can be used to create multiple directories. The second command above will fail if you omit the -p. ","dateUpdated":"2020-01-05T19:34:38+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578252831656_-1919803813","id":"20181206-192241_956108146","dateCreated":"2020-01-05T19:33:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69060","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Notice how the -p command can be used to create multiple directories. The second command above will fail if you omit the -p.</p>\n"}]}},{"title":"7 - View the contents of /user/zeppelin","text":"","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381855_576613859","id":"20181206-192411_1380093636","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69061"},{"text":"%md\nNotice you only see the test directory. To recursively view the contents of a folder, use -ls -R:\n~~~\nhdfs dfs -ls -R\n~~~","dateUpdated":"2020-01-05T19:34:41+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578252831656_-1940849876","id":"20181206-192619_1552653873","dateCreated":"2020-01-05T19:33:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69062","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Notice you only see the test directory. To recursively view the contents of a folder, use -ls -R:</p>\n<pre><code>hdfs dfs -ls -R\n</code></pre>\n"}]}},{"title":"8 - View the contents of /user/zeppelin recursively","text":"","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381856_-714317924","id":"20181206-193014_652851327","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69063"},{"text":"%md\nDelete the test2 folder (and recursively, its sub-contents) using the -rm -R command:\n~~~\nhdfs dfs -rm -R test/test2\n~~~","dateUpdated":"2020-01-05T19:34:44+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578252831656_-653158147","id":"20181206-193123_338727181","dateCreated":"2020-01-05T19:33:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69064","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Delete the test2 folder (and recursively, its sub-contents) using the -rm -R command:</p>\n<pre><code>hdfs dfs -rm -R test/test2\n</code></pre>\n"}]}},{"title":"9 - Delete a directory","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381857_1224789795","id":"20181206-193133_1179547109","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69065"},{"title":"10 - Confirm the deletion of the directory","text":"","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381857_105224790","id":"20181206-193509_128012816","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69066"},{"text":"%md\n**Note**\n\nNotice Hadoop created a .Trash folder for the zeppelin user and moved the deleted content there. The .Trash folder empties automatically after a configured amount of time.\n","dateUpdated":"2020-01-05T19:34:48+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578252831656_1551159788","id":"20181206-193936_185574902","dateCreated":"2020-01-05T19:33:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69067","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><strong>Note</strong></p>\n<p>Notice Hadoop created a .Trash folder for the zeppelin user and moved the deleted content there. The .Trash folder empties automatically after a configured amount of time.</p>\n"}]}},{"text":"%md\nWe have downloaded some files on your local file system as part of this lab setup.\nWe fill use them to test the upload to HDFS.","dateUpdated":"2020-01-05T19:34:50+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578252831656_1882176026","id":"20181206-195530_690183528","dateCreated":"2020-01-05T19:33:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69068","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We have downloaded some files on your local file system as part of this lab setup.\n<br  />We fill use them to test the upload to HDFS.</p>\n"}]}},{"title":"11 - Check the content of  data.txt","text":"","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381858_535565739","id":"20181206-194444_379567354","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69069"},{"text":"%md\nRun the following -put command to copy data.txt into the test folder in HDFS:\n~~~\nhdfs dfs -put data.txt test/\n~~~","dateUpdated":"2020-01-05T19:34:52+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578252831656_-693522367","id":"20181207-090129_42099507","dateCreated":"2020-01-05T19:33:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69070","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Run the following -put command to copy data.txt into the test folder in HDFS:</p>\n<pre><code>hdfs dfs -put data.txt test/\n</code></pre>\n"}]}},{"title":"12 - Copy data.txt into the test folder in HDFS","text":"","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381859_344781797","id":"20181207-090159_1881525100","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69071"},{"title":"13 - Verify that the file is in HDFS by listing the contents of test","text":"","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381859_-1608597479","id":"20181207-090448_724615975","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69072"},{"text":"%md\nNow copy the data.txt file in test to another folder in HDFS using the -cp command:\n~~~\nhdfs dfs -cp test/data.txt test/test1/data2.txt\n~~~","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381859_-904499360","id":"20181207-090629_1994988169","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69073"},{"title":"14 - Copy a File in HDFS","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381859_-770780894","id":"20181207-090635_962560763","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69074"},{"title":"15 - Verify that the file is in both places by using the -ls -R command on test","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381860_-2068159102","id":"20181207-090901_660195567","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69075"},{"title":"16 - Now delete the data2.txt file using the -rm command","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381860_1561128184","id":"20181207-091257_171743570","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69076"},{"text":"%md\nYou can use the -cat command to view text files in HDFS. \nEnter the following command to view the contents of data.txt:\n~~~\nhdfs dfs -cat test/data.txt\n~~~","dateUpdated":"2020-01-05T19:34:56+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578252831656_-171926349","id":"20181207-091427_1914915948","dateCreated":"2020-01-05T19:33:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69077","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>You can use the -cat command to view text files in HDFS.\n<br  />Enter the following command to view the contents of data.txt:</p>\n<pre><code>hdfs dfs -cat test/data.txt\n</code></pre>\n"}]}},{"title":"17 - View the Contents of a File in HDFS","text":"","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381861_-1336427461","id":"20181207-091432_959695280","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69078"},{"title":"18 - You can also use the -tail command to view the end of a file","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381861_134968283","id":"20181207-091914_737258730","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69079"},{"text":"%md\nSee if you can figure out how to use the get command to copy test/data.txt from HDFS into your local /tmp folder.","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381861_-1628719693","id":"20181207-092219_493612600","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69080"},{"title":"19 - Getting a File from HDFS","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381862_-230860305","id":"20181207-092228_1916756618","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69081"},{"title":"20 - Put the file small_blocks.txt into the test folder in HDFS","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381862_1548171106","id":"20181207-092724_1778755795","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69082"},{"text":"%md\nYou should now have two files in test: data.txt and small_blocks.txt.\nRun the following getmerge command:\n~~~\nhdfs dfs -getmerge test /tmp/merged.txt\n~~~","dateUpdated":"2020-01-05T19:35:02+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578252831656_-2136979510","id":"20181207-092809_977227762","dateCreated":"2020-01-05T19:33:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69083","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>You should now have two files in test: data.txt and small_blocks.txt.\n<br  />Run the following getmerge command:</p>\n<pre><code>hdfs dfs -getmerge test /tmp/merged.txt\n</code></pre>\n"}]}},{"title":"21 - Run a getmerge command","text":"","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381863_-488951811","id":"20181207-092932_1972899950","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69084"},{"text":"%md\nWhat did the previous command do? Did you open the file merged.txt to see what happened?","dateUpdated":"2020-01-05T19:35:05+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578252831656_1616386990","id":"20181207-093139_1734357344","dateCreated":"2020-01-05T19:33:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69085","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>What did the previous command do? Did you open the file merged.txt to see what happened?</p>\n"}]}},{"text":"%md\nPut data.txt into /user/root in HDFS, giving it a blocksize of 1,048,576 bytes.  \n**Hint:** The blocksize is defined using the dfs.blocksize property on the command line.","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381863_150093908","id":"20181207-093358_1077743009","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69086"},{"title":"22 - Specify the Block Size and Replication Factor","text":"","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381864_137544963","id":"20181207-094228_621112162","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69087"},{"text":"%md\nRun the following fsck command on data.txt:\n~~~\nhdfs fsck test/data3.txt\n~~~","dateUpdated":"2020-01-05T19:35:10+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578252831656_-1313098231","id":"20181207-094737_1505245371","dateCreated":"2020-01-05T19:33:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69088","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Run the following fsck command on data.txt:</p>\n<pre><code>hdfs fsck test/data3.txt\n</code></pre>\n"}]}},{"title":"23 - Run fsck on the last file","text":"","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381864_159452486","id":"20181207-094901_728482837","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69089"},{"text":"%md\nHow many blocks are there for this file?","dateUpdated":"2020-01-05T19:35:14+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578252831656_-1725648392","id":"20181207-094759_1354476994","dateCreated":"2020-01-05T19:33:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69090","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>How many blocks are there for this file?</p>\n"}]}},{"text":"%md\n# Result\n**You have now:** You should now be comfortable with executing the various HDFS commands, including creating directories, putting files into HDFS, copying files out of HDFS, and deleting files and folders.\n\n---","dateUpdated":"2020-01-05T19:35:16+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578252831656_1910784986","id":"20181119-142716_792318228","dateCreated":"2020-01-05T19:33:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69091","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Result</h1>\n<p><strong>You have now:</strong> You should now be comfortable with executing the various HDFS commands, including creating directories, putting files into HDFS, copying files out of HDFS, and deleting files and folders.</p>\n<hr />\n"}]}},{"text":"%md\n# Solution\n---","dateUpdated":"2020-01-05T19:35:18+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578252831656_-298313331","id":"20171113-155535_1769142099","dateCreated":"2020-01-05T19:33:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69092","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Solution</h1>\n<hr />\n"}]}},{"title":"1 - View the hdfs dfs Command","text":"%md\n~~~\nhdfs dfs\nUsage: hadoop fs [generic options]\n\t[-appendToFile <localsrc> ... <dst>]\n\t[-cat [-ignoreCrc] <src> ...]\n\t[-checksum <src> ...]\n\t[-chgrp [-R] GROUP PATH...]\n\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n\t[-copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst>]\n\t[-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n\t[-count [-q] [-h] <path> ...]\n\t[-cp [-f] [-p | -p[topax]] <src> ... <dst>]\n\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n\t[-df [-h] [<path> ...]]\n\t[-du [-s] [-h] <path> ...]\n\t[-expunge]\n\t[-find <path> ... <expression> ...]\n\t[-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n\t[-getfacl [-R] <path>]\n\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n\t[-getmerge [-nl] <src> <localdst>]\n\t[-help [cmd ...]]\n\t[-ls [-d] [-h] [-R] [<path> ...]]\n\t[-mkdir [-p] <path> ...]\n\t[-moveFromLocal <localsrc> ... <dst>]\n\t[-moveToLocal <src> <localdst>]\n\t[-mv <src> ... <dst>]\n\t[-put [-f] [-p] [-l] <localsrc> ... <dst>]\n\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n\t[-rm [-f] [-r|-R] [-skipTrash] <src> ...]\n\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n\t[-setfattr {-n name [-v value] | -x name} <path>]\n\t[-setrep [-R] [-w] <rep> <path> ...]\n\t[-stat [format] <path> ...]\n\t[-tail [-f] <file>]\n\t[-test -[defsz] <path>]\n\t[-text [-ignoreCrc] <src> ...]\n\t[-touchz <path> ...]\n\t[-truncate [-w] <length> <path> ...]\n\t[-usage [cmd ...]]\n\nGeneric options supported are\n-conf <configuration file>     specify an application configuration file\n-D <property=value>            use value for given property\n-fs <local|namenode:port>      specify a namenode\n-jt <local|resourcemanager:port>    specify a ResourceManager\n-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster\n-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.\n-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.\n\nThe general command line syntax is\nbin/hadoop command [genericOptions] [commandOptions]\n~~~","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381866_-835519469","id":"20181115-084422_413255504","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69093"},{"title":"2 - View the content of the zeppelin user directory in HDFS","text":"%sh\nhdfs dfs -ls","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381866_1785975591","id":"20171105-201709_849284875","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69094"},{"title":"3 - View the content of the root directory in HDFS","text":"%sh\nhdfs dfs -ls /","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381866_-911426436","id":"20181115-092436_561920580","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69095"},{"title":"4 - Create a directory named test in HDFS","text":"%sh\nhdfs dfs -mkdir test","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381867_-560391453","id":"20181206-191125_1088920073","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69096"},{"title":"5 - Verify that the folder was created successfully","text":"%sh\nhdfs dfs -ls","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381867_-667635884","id":"20181206-191334_112719772","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69097"},{"title":"6 - Create a couple of subdirectories for test","text":"%sh\nhdfs dfs -mkdir test/test1\nhdfs dfs -mkdir -p test/test2/test3","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381867_-1872488390","id":"20181206-191954_647413614","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69098"},{"title":"7 - View the contents of /user/zeppelin","text":"%sh\nhdfs dfs -ls\n","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381868_-1317926360","id":"20181206-192515_732280654","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69099"},{"title":"8 - View the contents of /user/zeppelin recursively","text":"%sh\nhdfs dfs -ls -R","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381868_-1102749509","id":"20181206-192631_944912258","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69100"},{"title":"9 - Delete a directory","text":"%sh\nhdfs dfs -rm -R test/test2","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381869_-75226420","id":"20181206-193207_1501145013","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69101"},{"title":"10 - Confirm the deletion of the directory","text":"%sh\nhdfs dfs -ls -R","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381869_-54301837","id":"20181206-193556_1797349403","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69102"},{"title":"11 - Check the content of  data.txt","text":"%sh\ntail data.txt","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381869_-921569233","id":"20181206-194526_1818312136","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69103"},{"title":"12 - Copy data.txt into the test folder in HDFS","text":"%sh\nhdfs dfs -put data.txt test/","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381869_1471734116","id":"20181207-090253_811056162","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69104"},{"title":"13 - Verify that the file is in HDFS by listing the contents of test","text":"%sh\n\nhdfs dfs -ls test\n","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381870_676898886","id":"20181207-090350_175393479","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69105"},{"title":"14 - Copy a File in HDFS","text":"%sh\nhdfs dfs -cp test/data.txt test/test1/data2.txt","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381870_-619889999","id":"20181207-090746_1001047818","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69106"},{"title":"15 - Verify that the file is in both places by using the -ls -R command on test","text":"%sh\nhdfs dfs -ls -R test","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381870_-1219276342","id":"20181207-090921_400751635","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69107"},{"title":"16 - Now delete the data2.txt file using the -rm command","text":"%sh\nhdfs dfs -rm test/test1/data2.txt","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381871_1195926219","id":"20181207-091144_307887061","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69108"},{"title":"17 - View the Contents of a File in HDFS","text":"%sh\nhdfs dfs -cat test/data.txt","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381871_-1832938839","id":"20181207-091709_1330621601","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69109"},{"title":"18 - You can also use the -tail command to view the end of a file","text":"%sh\nhdfs dfs -tail test/data.txt","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381871_1855795734","id":"20181207-091143_768068633","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69110"},{"title":"19 - Getting a File from HDFS","text":"%sh\nhdfs dfs -get test/data.txt /tmp/\nls /tmp/data*","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381872_197843718","id":"20181207-092302_721441300","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69111"},{"title":"20 - Put the file small_blocks.txt into the test folder in HDFS","text":"%sh\nhdfs dfs -put small_blocks.txt test/","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381872_-896407993","id":"20181207-092651_26372656","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69112"},{"title":"21 - Run a getmerge command","text":"%sh\nhdfs dfs -getmerge test /tmp/merged.txt","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381872_-1405816670","id":"20181207-093017_1470900285","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69113"},{"text":"%md\n**Answer:** The two files that were in the test folder in HDFS were merged into a single file and stored on the local file system.","dateUpdated":"2020-01-05T19:35:29+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578252831657_-1407348014","id":"20181207-093303_1860965498","dateCreated":"2020-01-05T19:33:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69114","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><strong>Answer:</strong> The two files that were in the test folder in HDFS were merged into a single file and stored on the local file system.</p>\n"}]}},{"title":"22 - Specify the Block Size and Replication Factor","text":"%sh\nhdfs dfs -D dfs.blocksize=1048576 -put data.txt test/data3.txt","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381873_-389080004","id":"20181207-094346_63615299","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69115"},{"title":"23 - Run fsck on the last file","text":"%sh\nhdfs fsck test/data3.txt","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381873_318643873","id":"20181207-095014_88500569","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69116"},{"text":"%md\n**Answer:** The file should be broken down into two blocks","dateUpdated":"2020-01-05T19:35:32+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578252831657_1949480742","id":"20181207-095151_1392085652","dateCreated":"2020-01-05T19:33:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69117","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><strong>Answer:</strong> The file should be broken down into two blocks</p>\n"}]}},{"title":"Additional resources","text":"%md\nWe hope you've enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Hortonworks Apache Spark Tutorials](http://hortonworks.com/tutorials/#tuts-developers) are your natural next step where you can explore Spark in more depth.\n2. [Hortonworks Community Connection (HCC)](https://community.hortonworks.com/spaces/85/data-science.html?type=question) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Hortonworks Apache Spark Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html) - official Spark documentation.\n4. [Hortonworks Apache Zeppelin Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html) - official Zeppelin documentation.","dateUpdated":"2020-01-05T19:35:35+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":10,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578252831657_819550517","id":"20181116-135131_93712280","dateCreated":"2020-01-05T19:33:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69118","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We hope you've enjoyed this lab. Below are additional resources that you should find useful:</p>\n<ol>\n<li><a href=\"http://hortonworks.com/tutorials/#tuts-developers\">Hortonworks Apache Spark Tutorials</a> are your natural next step where you can explore Spark in more depth.</li>\n<li><a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\">Hortonworks Community Connection (HCC)</a> is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html\">Hortonworks Apache Spark Docs</a> - official Spark documentation.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html\">Hortonworks Apache Zeppelin Docs</a> - official Zeppelin documentation.</li>\n</ol>\n"}]}},{"text":"%angular\n</br>\n<center>\n<a href=\"https://learn.hortonworks.com/\">\n  <img src=\"https://everpath-course-content.s3-accelerate.amazonaws.com/instructor%2Ftbroas_hortonworks_c_bsns5k%2Fpublic%2Flogo-training-1.1494860458677.png\" alt=\"Hortonworks University\" style=\"width:125px;height:125px;border:0;\" align=\"middle\">\n</a>\n</center>","dateUpdated":"2020-01-05T19:35:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":2,"editorMode":"ace/mode/undefined","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578252831657_-862667293","id":"20181115-083751_61749794","dateCreated":"2020-01-05T19:33:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69119","results":{"code":"SUCCESS","msg":[{"type":"ANGULAR","data":"</br>\n<center>\n<a href=\"https://learn.hortonworks.com/\">\n  <img src=\"https://everpath-course-content.s3-accelerate.amazonaws.com/instructor%2Ftbroas_hortonworks_c_bsns5k%2Fpublic%2Flogo-training-1.1494860458677.png\" alt=\"Hortonworks University\" style=\"width:125px;height:125px;border:0;\" align=\"middle\">\n</a>\n</center>"}]}},{"text":"%angular\n","user":"devuser","dateUpdated":"2019-12-04T04:23:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/undefined","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575433381875_745410661","id":"20181207-095756_1112890944","dateCreated":"2019-12-04T04:23:01+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69120"}],"name":"HDFS/HDFSCommands","id":"2EVYD1G22","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"angular:shared_process":[],"sh:shared_process":[],"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"personalizedMode":"true","looknfeel":"default"},"info":{}}
