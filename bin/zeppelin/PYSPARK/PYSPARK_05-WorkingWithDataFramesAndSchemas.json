{"paragraphs":[{"text":"%md\n# About\n**Lab:** Working with DataFrames and Schemas\n**Objective:** Practice working with structured account data and mobile device data using DataFrames\n**File locations:**\n- Data files (HDFS): /user/zeppelin/devices.json\n- Hive Tables: telco.accounts\n\n**Successful outcome:** Create and save DataFrames using different types of data sources, and infer and define schemas\n**Before you begin:** \n**Related lessons:** \n\n---","user":"sysadmin","dateUpdated":"2020-06-02T18:41:50+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>About</h1>\n<p><strong>Lab:</strong> Working with DataFrames and Schemas\n<br  /><strong>Objective:</strong> Practice working with structured account data and mobile device data using DataFrames\n<br  /><strong>File locations:</strong></p>\n<ul>\n<li>Data files (HDFS): /devsh_loudacre/devices.json</li>\n<li>Hive Tables: devsh.accounts</li>\n</ul>\n<p><strong>Successful outcome:</strong> Create and save DataFrames using different types of data sources, and infer and define schemas\n<br  /><strong>Before you begin:</strong>\n<br  /><strong>Related lessons:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1590353111767_-1952390947","id":"20181126-092644_1457476546","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:133230"},{"text":"%md\n# Setup\nThis exercise requires the `telco.accounts` table, check that it exists and containts data by running the `SELECT` statement below. If the table has not already been created and loaded you can do so by completing exercise `20 - Supplemental: Creating and Loading Hive Database`.\n\n**Important:** This assume you are familiar with DataFrames, you can practice by completing exercise `04 - Exploring DataFrames Using the Apache Spark Shell`.","user":"sysadmin","dateUpdated":"2020-06-02T18:54:20+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Setup</h1>\n<p>This exercise requires the <code>telco.accounts</code> table, check that it exists and containts data by running the <code>SELECT</code> statement below. If the table has not already been created and loaded you can do so by completing exercise <code>20 - Supplemental: Creating and Loading Hive Database</code>.</p>\n<p><strong>Important:</strong> This assume you are familiar with DataFrames, you can practice by completing exercise 04 - Exploring DataFrames Using the Apache Spark Shell.</p>\n"}]},"apps":[],"jobName":"paragraph_1590353111768_1412172271","id":"20181201-044336_178705192","dateCreated":"2020-05-24T20:45:11+0000","dateStarted":"2020-06-02T18:49:57+0000","dateFinished":"2020-06-02T18:49:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:133231"},{"text":"%jdbc(hive)\nSELECT * FROM telco.accounts LIMIT 5;","user":"sysadmin","dateUpdated":"2020-06-02T18:49:07+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"accounts.acct_num":"string","accounts.acct_create_dt":"string","accounts.acct_close_dt":"string","accounts.first_name":"string","accounts.last_name":"string","accounts.address":"string","accounts.city":"string","accounts.state":"string","accounts.zipcode":"string","accounts.phone_number":"string","accounts.created":"string","accounts.modified":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"accounts.acct_num\taccounts.acct_create_dt\taccounts.acct_close_dt\taccounts.first_name\taccounts.last_name\taccounts.address\taccounts.city\taccounts.state\taccounts.zipcode\taccounts.phone_number\taccounts.created\taccounts.modified\n1\t2008-10-23 16:05:05.0\tnull\tDonald\tBecton\t2275 Washburn Street\tOakland\tCA\t94660\t5100032418\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n2\t2008-11-12 03:00:01.0\tnull\tDonna\tJones\t3885 Elliott Street\tSan Francisco\tCA\t94171\t4150835799\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n3\t2008-12-21 09:19:50.0\tnull\tDorthy\tChalmers\t4073 Whaley Lane\tSan Mateo\tCA\t94479\t6506877757\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n4\t2008-11-28 00:08:09.0\tnull\tLeila\tSpencer\t1447 Ross Street\tSan Mateo\tCA\t94444\t6503198619\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n5\t2008-11-15 23:06:06.0\tnull\tAnita\tLaughlin\t2767 Hill Street\tRichmond\tCA\t94872\t5107754354\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n"}]},"apps":[],"jobName":"paragraph_1591123705609_193299807","id":"20200602-184825_95278537","dateCreated":"2020-06-02T18:48:25+0000","dateStarted":"2020-06-02T18:48:57+0000","dateFinished":"2020-06-02T18:48:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:133232"},{"text":"%md\n# Lab\n","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Lab</h1>\n"}]},"apps":[],"jobName":"paragraph_1590353111768_-1640534716","id":"20181126-093358_358613711","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133233"},{"text":"%md\n### Create a DataFrame Based on a Hive Table","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Create a DataFrame Based on a Hive Table</h3>\n"}]},"apps":[],"jobName":"paragraph_1590353111768_-432061630","id":"20200427-233238_1449553071","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133234"},{"title":"1 - Review the accounts table in the Hive database","text":"%md\nThis exercise uses a DataFrame based on the `accounts` table in the `telco` Hive database. You can review the schema using the `jdbc` interpreter to access Hive.","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>This exercise uses a DataFrame based on the <code>accounts</code> table in the <code>devsh</code> Hive\n<br  />database. You can review the schema using the Beeline SQL command line to access\n<br  />Hive.</p>\n<p>In a terminal session (not one that is running the Spark shell), enter the following\n<br  />command:</p>\n<pre><code>$ beeline -u jdbc:hive2://localhost:10000 -e \"DESCRIBE devsh.accounts\"\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1590353111769_-54605198","id":"20200424-193917_1691179324","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133235"},{"text":"%jdbc(hive)\nDESCRIBE telco.accounts;","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"col_name":"string","data_type":"string","comment":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"col_name\tdata_type\tcomment\nacct_num\tint\t\nacct_create_dt\ttimestamp\t\nacct_close_dt\ttimestamp\t\nfirst_name\tvarchar(255)\t\nlast_name\tvarchar(255)\t\naddress\tvarchar(255)\t\ncity\tvarchar(255)\t\nstate\tvarchar(255)\t\nzipcode\tvarchar(255)\t\nphone_number\tvarchar(255)\t\ncreated\ttimestamp\t\nmodified\ttimestamp\t\n"}]},"apps":[],"jobName":"paragraph_1590353111769_-724261568","id":"20200521-194130_648124297","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133236"},{"title":"2 - Create a new DataFrame","text":"%md\nCreate a new DataFrame using the Hive `telco.accounts` table.","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame using the Hive <code>telco.accounts</code> table.</p>\n"}]},"apps":[],"jobName":"paragraph_1590353111769_194820013","id":"20200424-194206_1083041321","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133237"},{"text":"%pyspark\naccountsDF = spark.read.table(\"telco.accounts\")","user":"sysadmin","dateUpdated":"2020-06-02T18:54:35+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590353111770_2007954013","id":"20200424-194301_536696070","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133238"},{"title":"3 - Compare the DataFrame and the Hive table","text":"%md\nPrint the schema and the first few rows of the DataFrame, and note that the schema aligns with that of the Hive table.","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Print the schema and the first few rows of the DataFrame, and note that the schema aligns with that of the Hive table.</p>\n"}]},"apps":[],"jobName":"paragraph_1590353111770_1362854679","id":"20200424-194700_1657078805","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133239"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590353111770_1755720502","id":"20200522-201531_943828593","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133240"},{"title":"4 - Create a new DataFrame based on a condition","text":"%md\nCreate a new DataFrame with rows from the accounts data where the zip code is 94913, and save the result to CSV files in the `/user/zeppelin/accounts_zip94913` HDFS directory. You can do this in a single command, as shown below, or with multiple commands.","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"table","height":60,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame with rows from the accounts data where the zip\n<br  />code is 94913, and save the result to CSV files in the <code>/devsh_loudacre/accounts_zip94913</code> HDFS directory. You can do this in a single command, as shown below, or with multiple commands.</p>\n"}]},"apps":[],"jobName":"paragraph_1590353111770_446490899","id":"20200424-194924_522335325","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133241"},{"text":"%pyspark\naccountsDF.where(\"zipcode = 94913\"). \\\nwrite.option(\"header\",\"true\"). \\\ncsv(\"/user/zeppelin/accounts_zip94913\")","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590353111771_44495484","id":"20200424-195112_623415846","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133242"},{"title":"5 - Confirm the operation was executed correctly","text":"%md\nUse `hdfs` to view the `/user/zeppelin/accounts_zip94913` directory in HDFS and the data in one of the saved files. Confirm that the CSV file includes a header line, and that only records for the selected zip code are included.","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Use <code>hdfs</code> in a separate terminal window to view the\n<br  /><code>/devsh_loudacre/accounts_zip94913</code> directory in HDFS and the data in\n<br  />one of the saved files. Confirm that the CSV file includes a header line, and that only\n<br  />records for the selected zip code are included.</p>\n"}]},"apps":[],"jobName":"paragraph_1590353111771_-359583997","id":"20200424-195251_938833734","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133243"},{"text":"%sh\n","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590353111771_435089064","id":"20200522-201740_1472082247","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133244"},{"title":"6 - Create a DataFrame from a CSV","text":"%md\n*Optional:* Try creating a new DataFrame based on the CSV files you created above. Compare the schema of the original `accountsDF` and the new DataFrame. What's different? Try again, this time setting the `inferSchema` option to true and compare again.","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><em>Optional:</em> Try creating a new DataFrame based on the CSV files you created above.\n<br  />Compare the schema of the original <code>accountsDF</code> and the new DataFrame. What's\n<br  />different? Try again, this time setting the <code>inferSchema</code> option to true and\n<br  />compare again.</p>\n"}]},"apps":[],"jobName":"paragraph_1590353111771_285348043","id":"20200424-195331_1399907429","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133245"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590353111772_726971147","id":"20200522-201757_1455279920","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133246"},{"text":"%md\n### Define a Schema for a DataFrame","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Define a Schema for a DataFrame</h3>\n"}]},"apps":[],"jobName":"paragraph_1590353111772_541228861","id":"20200427-234619_2069213136","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133247"},{"title":"7 - Review the data file","text":"%md\nIf you have not done so yet, review the data in the HDFS file `/user/zeppelin/devices.json`.","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>If you have not done so yet, review the data in the HDFS file\n<br  /><code>/devsh_loudacre/devices.json</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1590353111772_1951400544","id":"20200424-195454_1861585933","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133248"},{"text":"%sh\n","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590353111772_-1668588657","id":"20200430-003645_261089763","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133249"},{"title":"8 - Create a DataFrame based on the devices.json file","text":"%md\nCreate a new DataFrame based on the `devices.json` file. (This command could take several seconds while it infers the schema.)","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame based on the <code>devices.json</code> file. (This command could take several seconds while it infers the schema.)</p>\n"}]},"apps":[],"jobName":"paragraph_1590353111773_1048366749","id":"20200424-201503_1618492364","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133250"},{"text":"%pyspark\ndevDF = spark.read.json(\"/user/zeppelin/devices.json\")","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590353111773_-1791905130","id":"20200424-201433_776130777","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133251"},{"title":"9 - Review the schema of the devDF DataFrame","text":"%md\nView the schema of the `devDF` DataFrame. Note the column names and types that Spark inferred from the JSON file. In particular, note that the `release_dt` column is of type `string`, whereas the data in the column actually represents a timestamp.","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>View the schema of the <code>devDF</code> DataFrame. Note the column names and types that\n<br  />Spark inferred from the JSON file. In particular, note that the <code>release_dt</code> column\n<br  />is of type <code>string</code>, whereas the data in the column actually represents a timestamp.</p>\n"}]},"apps":[],"jobName":"paragraph_1590353111773_-1725273016","id":"20200424-201330_1854493694","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133252"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590353111773_-1849124469","id":"20200522-201923_1097540138","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133253"},{"title":"10 - Specify the column types of the DataFrame","text":"%md\nDefine a schema that correctly specifies the column types for this DataFrame. Start by importing the package with the definitions of necessary classes and types.","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Define a schema that correctly specifies the column types for this DataFrame. Start\n<br  />by importing the package with the definitions of necessary classes and types.</p>\n"}]},"apps":[],"jobName":"paragraph_1590353111774_470098493","id":"20200424-201310_819844900","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133254"},{"text":"%pyspark\nfrom pyspark.sql.types import *","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590353111774_-1322270439","id":"20200424-201240_160502927","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133255"},{"title":"11 - Represent the column definitions with StructField objects","text":"%md\nNext, create a collection of `StructField` objects, which represent column definitions. The `release_dt` column should be a timestamp.","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Next, create a collection of <code>StructField</code> objects, which represent column\n<br  />definitions. The <code>release_dt</code> column should be a timestamp.</p>\n"}]},"apps":[],"jobName":"paragraph_1590353111774_-1589687380","id":"20200424-201153_1739325405","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133256"},{"text":"%pyspark\ndevColumns = [\nStructField(\"devnum\",LongType()),\nStructField(\"make\",StringType()),\nStructField(\"model\",StringType()),\nStructField(\"release_dt\",TimestampType()),\nStructField(\"dev_type\",StringType())]","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590353111774_-1412526623","id":"20200424-201128_583503462","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133257"},{"title":"12 - Create a schema using the column definition list","text":"%md\nCreate a schema (a `StructType` object) using the column definition list.","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a schema (a <code>StructType</code> object) using the column definition list.</p>\n"}]},"apps":[],"jobName":"paragraph_1590353111775_-1301346195","id":"20200424-201020_2045594003","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133258"},{"text":"%pyspark\ndevSchema = StructType(devColumns)","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590353111775_1036763149","id":"20200424-200751_1167632895","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133259"},{"title":"13 - Recreate the devDF DataFrame using the new schema","text":"%md\nRecreate the `devDF` DataFrame, this time using the new schema.","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Recreate the <code>devDF</code> DataFrame, this time using the new schema.</p>\n"}]},"apps":[],"jobName":"paragraph_1590353111775_-1834305705","id":"20200424-200207_703556537","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133260"},{"text":"%pyspark\ndevDF = spark.read.schema(devSchema).json(\"/user/zeppelin/devices.json\")","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590353111775_357193154","id":"20200424-200134_290658193","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133261"},{"title":"14 - Confirm that the release_dt column is now of type timestamp","text":"%md\nView the schema and data of the new DataFrame, and confirm that the `release_dt` column type is now `timestamp`.","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>View the schema and data of the new DataFrame, and confirm that the\n<br  /><code>release_dt</code> column type is now <code>timestamp</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1590353111776_-1645115504","id":"20200424-200027_404284113","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133262"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590353111776_-1981311114","id":"20200522-202136_1188742309","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133263"},{"title":"15 - Save the DataFrame in Parquet format","text":"%md\nNow that the device data uses the correct schema, write the data in Parquet format, which automatically embeds the schema. Save the Parquet data files into an HDFS directory called `/user/zeppelin/devices_parquet`.","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Now that the device data uses the correct schema, write the data in Parquet format,\n<br  />which automatically embeds the schema. Save the Parquet data files into an HDFS\n<br  />directory called <code>/devsh_loudacre/devices_parquet</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1590353111776_395734354","id":"20200424-195942_1783798900","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133264"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590353111776_-277607155","id":"20200522-202142_98727568","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133265"},{"title":"16 - View the schema of the Parquet file","text":"%md\n*Optional:* Use `parquet-tools` to view the schema of the saved files. First download the HDFS directory (or an individual file), then run the command.\n\n```\n    $ hdfs dfs -get /user/zeppelin/devices_parquet /tmp/\n    $ parquet-tools schema /tmp/devices_parquet\n```\n\nNote that the type of the `release_dt` column is noted as `int96`; this is how Spark denotes a timestamp type in Parquet.\n\nFor more information about `parquet-tools`, run `parquet-tools --help`.","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><em>Optional:</em> In a separate terminal window, use <code>parquet-tools</code> to view the schema\n<br  />of the saved files. First download the HDFS directory (or an individual file), then run\n<br  />the command.</p>\n<pre><code>$ hdfs dfs -get /devsh_loudacre/devices_parquet /tmp/\n$ parquet-tools schema /tmp/devices_parquet/\n</code></pre>\n<p>Note that the type of the <code>release_dt</code> column is noted as <code>int96</code>; this is how Spark\n<br  />denotes a timestamp type in Parquet.</p>\n<p>For more information about <code>parquet-tools</code>, run <code>parquet-tools --help</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1590353111777_762982686","id":"20200424-195815_1058807478","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133266"},{"text":"%sh\n","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590353111777_-2061992272","id":"20200522-202202_1659863288","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133267"},{"title":"17 - Create a DataFrame from the Parquet files","text":"%md\nCreate a new DataFrame using the Parquet files you saved in `devices_parquet` and view its schema. Note that Spark is able to correctly infer the timestamp type of the `release_dt` column from Parquet's embedded schema.","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame using the Parquet files you saved in <code>devices_parquet</code>\n<br  />and view its schema. Note that Spark is able to correctly infer the timestamp type\n<br  />of the <code>release_dt</code> column from Parquet's embedded schema.</p>\n"}]},"apps":[],"jobName":"paragraph_1590353111777_708246968","id":"20200424-195648_983273468","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133268"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590353111777_29485524","id":"20200522-202219_851657177","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133269"},{"text":"%md\n# Result\n**You have now:** \n\n---","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Result</h1>\n<p><strong>You have now:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1590353111777_1688317592","id":"20181126-133507_1472573213","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133270"},{"text":"%md\n# Solution\n---","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Solution</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1590353111778_1366151808","id":"20181018-125200_1133281582","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133271"},{"text":"%md\n### Create a DataFrame Based on a Hive Table","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Create a DataFrame Based on a Hive Table</h3>\n"}]},"apps":[],"jobName":"paragraph_1590353111778_-1501985487","id":"20200428-225106_184286167","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133272"},{"title":"1 - Review the accounts table in the Hive database","text":"%jdbc(hive)\nDESCRIBE telco.accounts;","user":"sysadmin","dateUpdated":"2020-05-26T03:39:29+0000","config":{"tableHide":false,"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"col_name":"string","data_type":"string","comment":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"col_name\tdata_type\tcomment\nacct_num\tint\t\nacct_create_dt\ttimestamp\t\nacct_close_dt\ttimestamp\t\nfirst_name\tvarchar(255)\t\nlast_name\tvarchar(255)\t\naddress\tvarchar(255)\t\ncity\tvarchar(255)\t\nstate\tvarchar(255)\t\nzipcode\tvarchar(255)\t\nphone_number\tvarchar(255)\t\ncreated\ttimestamp\t\nmodified\ttimestamp\t\n"}]},"apps":[],"jobName":"paragraph_1590353111778_-2066461487","id":"20200428-225139_1888690996","dateCreated":"2020-05-24T20:45:11+0000","dateStarted":"2020-05-26T03:39:29+0000","dateFinished":"2020-05-26T03:39:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:133273"},{"title":"2 - Create a new DataFrame","text":"%pyspark\naccountsDF = spark.read.table(\"telco.accounts\")","user":"sysadmin","dateUpdated":"2020-05-26T03:39:39+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1590353111779_279633970","id":"20200428-225540_857086049","dateCreated":"2020-05-24T20:45:11+0000","dateStarted":"2020-05-26T03:39:39+0000","dateFinished":"2020-05-26T03:39:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:133274"},{"title":"3 - Compare the DataFrame and the Hive table","text":"%pyspark\naccountsDF.printSchema()","user":"sysadmin","dateUpdated":"2020-05-26T03:39:41+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- acct_num: integer (nullable = true)\n |-- acct_create_dt: timestamp (nullable = true)\n |-- acct_close_dt: timestamp (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- zipcode: string (nullable = true)\n |-- phone_number: string (nullable = true)\n |-- created: timestamp (nullable = true)\n |-- modified: timestamp (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1590353111779_-1757947629","id":"20200428-225533_1026098327","dateCreated":"2020-05-24T20:45:11+0000","dateStarted":"2020-05-26T03:39:41+0000","dateFinished":"2020-05-26T03:39:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:133275"},{"title":"4 - Create a new DataFrame based on a condition","text":"%pyspark\naccountsDF.where(\"zipcode = 94913\").write.option(\"header\",\"true\").csv(\"/user/zeppelin/accounts_zip94913\")","user":"sysadmin","dateUpdated":"2020-05-26T03:42:11+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1590353111779_-1431165695","id":"20200428-225650_1329297880","dateCreated":"2020-05-24T20:45:11+0000","dateStarted":"2020-05-26T03:42:12+0000","dateFinished":"2020-05-26T03:42:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:133276"},{"title":"5 - Confirm the operation was executed correctly","text":"%sh\nhdfs dfs -ls /user/zeppelin/accounts_zip94913\n\necho\nhdfs dfs -head /user/zeppelin/accounts_zip94913/<part-file.csv>","user":"sysadmin","dateUpdated":"2020-05-26T03:43:10+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Found 5 items\n-rw-r--r--   3 zeppelin hdfs          0 2020-05-26 03:42 /user/zeppelin/accounts_zip94913/_SUCCESS\n-rw-r--r--   3 zeppelin hdfs        882 2020-05-26 03:42 /user/zeppelin/accounts_zip94913/part-00000-d360ab0f-6d8d-4a21-b515-d60ea16cc7a0-c000.csv\n-rw-r--r--   3 zeppelin hdfs       1026 2020-05-26 03:42 /user/zeppelin/accounts_zip94913/part-00001-d360ab0f-6d8d-4a21-b515-d60ea16cc7a0-c000.csv\n-rw-r--r--   3 zeppelin hdfs       1289 2020-05-26 03:42 /user/zeppelin/accounts_zip94913/part-00002-d360ab0f-6d8d-4a21-b515-d60ea16cc7a0-c000.csv\n-rw-r--r--   3 zeppelin hdfs       1514 2020-05-26 03:42 /user/zeppelin/accounts_zip94913/part-00003-d360ab0f-6d8d-4a21-b515-d60ea16cc7a0-c000.csv\n\nhead: `/user/zeppelin/accounts_zip94913/part-00000-efa4f310-a43b-4e6f-9798-1caf35e97702-c000.csv': No such file or directory\n"},{"type":"TEXT","data":"ExitValue: 1"}]},"apps":[],"jobName":"paragraph_1590353111779_-1131333839","id":"20200428-225649_867963440","dateCreated":"2020-05-24T20:45:11+0000","dateStarted":"2020-05-26T03:42:17+0000","dateFinished":"2020-05-26T03:42:20+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:133277"},{"title":"6 - Create a DataFrame from a CSV","text":"%pyspark\ntest1DF = spark.read.option(\"header\",\"true\").csv(\"/user/zeppelin/accounts_zip94913\")\ntest2DF = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/user/zeppelin/accounts_zip94913\")\n\ntest1DF.printSchema()\ntest2DF.printSchema()","user":"sysadmin","dateUpdated":"2020-05-26T03:43:10+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- acct_num: string (nullable = true)\n |-- acct_create_dt: string (nullable = true)\n |-- acct_close_dt: string (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- zipcode: string (nullable = true)\n |-- phone_number: string (nullable = true)\n |-- created: string (nullable = true)\n |-- modified: string (nullable = true)\n\nroot\n |-- acct_num: integer (nullable = true)\n |-- acct_create_dt: timestamp (nullable = true)\n |-- acct_close_dt: timestamp (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- zipcode: integer (nullable = true)\n |-- phone_number: long (nullable = true)\n |-- created: timestamp (nullable = true)\n |-- modified: timestamp (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1590353111780_1437214113","id":"20200428-225649_314698363","dateCreated":"2020-05-24T20:45:11+0000","dateStarted":"2020-05-26T03:43:10+0000","dateFinished":"2020-05-26T03:43:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:133278"},{"text":"%md\n### Define a Schema for a DataFrame","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Define a Schema for a DataFrame</h3>\n"}]},"apps":[],"jobName":"paragraph_1590353111780_143678343","id":"20200428-225648_2130864594","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133279"},{"title":"7 - Review the data file","text":"%sh\nhdfs dfs -head /user/zeppelin/devices.json","user":"sysadmin","dateUpdated":"2020-05-26T03:43:18+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"{\"devnum\":1,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"Sorrento\",\"model\":\"F00L\",\"dev_type\":\"phone\"}\n{\"devnum\":2,\"release_dt\":\"2010-04-19T00:00:00.000-07:00\",\"make\":\"Titanic\",\"model\":\"2100\",\"dev_type\":\"phone\"}\n{\"devnum\":3,\"release_dt\":\"2011-02-18T00:00:00.000-08:00\",\"make\":\"MeeToo\",\"model\":\"3.0\",\"dev_type\":\"phone\"}\n{\"devnum\":4,\"release_dt\":\"2011-09-21T00:00:00.000-07:00\",\"make\":\"MeeToo\",\"model\":\"3.1\",\"dev_type\":\"phone\"}\n{\"devnum\":5,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"1\",\"dev_type\":\"phone\"}\n{\"devnum\":6,\"release_dt\":\"2011-11-02T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"3\",\"dev_type\":\"phone\"}\n{\"devnum\":7,\"release_dt\":\"2010-05-20T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"2\",\"dev_type\":\"phone\"}\n{\"devnum\":8,\"release_dt\":\"2013-07-02T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"5\",\"dev_type\":\"phone\"}\n{\"devnum\":9,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"Titanic\",\"model\":\"1000\",\"dev_type\":\"phone\"}\n{\"devnum\":10,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"mak"}]},"apps":[],"jobName":"paragraph_1590353111780_1226752863","id":"20200428-225648_313296430","dateCreated":"2020-05-24T20:45:11+0000","dateStarted":"2020-05-26T03:43:19+0000","dateFinished":"2020-05-26T03:43:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:133280"},{"title":"8 - Create a DataFrame based on the devices.json file","text":"%pyspark\ndevDF = spark.read.json(\"/user/zeppelin/devices.json\")","user":"sysadmin","dateUpdated":"2020-05-26T03:43:23+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1590353111780_-982623357","id":"20200428-225648_1886643259","dateCreated":"2020-05-24T20:45:11+0000","dateStarted":"2020-05-26T03:43:23+0000","dateFinished":"2020-05-26T03:43:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:133281"},{"title":"9 - Review the schema of the devDF DataFrame","text":"%pyspark\ndevDF.printSchema()","user":"sysadmin","dateUpdated":"2020-05-26T03:43:32+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- dev_type: string (nullable = true)\n |-- devnum: long (nullable = true)\n |-- make: string (nullable = true)\n |-- model: string (nullable = true)\n |-- release_dt: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1590353111781_-104426113","id":"20200428-225647_1646122487","dateCreated":"2020-05-24T20:45:11+0000","dateStarted":"2020-05-26T03:43:32+0000","dateFinished":"2020-05-26T03:43:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:133282"},{"title":"10 - Specify the column types of the DataFrame","text":"%pyspark\nfrom pyspark.sql.types import *","user":"sysadmin","dateUpdated":"2020-05-26T03:43:37+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1590353111781_403516914","id":"20200428-225647_1682536791","dateCreated":"2020-05-24T20:45:11+0000","dateStarted":"2020-05-26T03:43:37+0000","dateFinished":"2020-05-26T03:43:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:133283"},{"title":"11 - Represent the column definitions with StructField objects","text":"%pyspark\ndevColumns = [\nStructField(\"devnum\",LongType()),\nStructField(\"make\",StringType()),\nStructField(\"model\",StringType()),\nStructField(\"release_dt\",TimestampType()),\nStructField(\"dev_type\",StringType())]","user":"sysadmin","dateUpdated":"2020-05-26T03:43:38+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1590353111781_-224800417","id":"20200428-225646_614334074","dateCreated":"2020-05-24T20:45:11+0000","dateStarted":"2020-05-26T03:43:38+0000","dateFinished":"2020-05-26T03:43:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:133284"},{"title":"12 - Create a schema using the column definitions","text":"%pyspark\ndevSchema = StructType(devColumns)","user":"sysadmin","dateUpdated":"2020-05-26T03:43:40+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1590353111781_-1085255452","id":"20200428-225645_364347750","dateCreated":"2020-05-24T20:45:11+0000","dateStarted":"2020-05-26T03:43:40+0000","dateFinished":"2020-05-26T03:43:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:133285"},{"title":"13 - Recreate the devDF DataFrame using the new schema","text":"%pyspark\ndevDF = spark.read.schema(devSchema).json(\"/user/zeppelin/devices.json\")\ndevDF.printSchema()","user":"sysadmin","dateUpdated":"2020-05-26T03:43:43+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- devnum: long (nullable = true)\n |-- make: string (nullable = true)\n |-- model: string (nullable = true)\n |-- release_dt: timestamp (nullable = true)\n |-- dev_type: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1590353111782_-945082276","id":"20200428-225644_1723966751","dateCreated":"2020-05-24T20:45:11+0000","dateStarted":"2020-05-26T03:43:43+0000","dateFinished":"2020-05-26T03:43:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:133286"},{"title":"14 - Confirm that the release_dt column is now of type timestamp","text":"%pyspark\ndevDF.show()","user":"sysadmin","dateUpdated":"2020-05-26T03:43:48+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+--------+--------------+-------------------+--------+\n|devnum|    make|         model|         release_dt|dev_type|\n+------+--------+--------------+-------------------+--------+\n|     1|Sorrento|          F00L|2008-10-21 07:00:00|   phone|\n|     2| Titanic|          2100|2010-04-19 07:00:00|   phone|\n|     3|  MeeToo|           3.0|2011-02-18 08:00:00|   phone|\n|     4|  MeeToo|           3.1|2011-09-21 07:00:00|   phone|\n|     5|  iFruit|             1|2008-10-21 07:00:00|   phone|\n|     6|  iFruit|             3|2011-11-02 07:00:00|   phone|\n|     7|  iFruit|             2|2010-05-20 07:00:00|   phone|\n|     8|  iFruit|             5|2013-07-02 07:00:00|   phone|\n|     9| Titanic|          1000|2008-10-21 07:00:00|   phone|\n|    10|  MeeToo|           1.0|2008-10-21 07:00:00|   phone|\n|    11|Sorrento|          F21L|2011-02-28 08:00:00|   phone|\n|    12|  iFruit|             4|2012-10-25 07:00:00|   phone|\n|    13|Sorrento|          F23L|2011-11-21 08:00:00|   phone|\n|    14| Titanic|          2200|2010-05-25 07:00:00|   phone|\n|    15|   Ronin|Novelty Note 1|2010-06-20 07:00:00|   phone|\n|    16| Titanic|          2500|2012-07-21 07:00:00|   phone|\n|    17|   Ronin|Novelty Note 3|2013-04-11 07:00:00|   phone|\n|    18|   Ronin|Novelty Note 2|2011-10-02 07:00:00|   phone|\n|    19|   Ronin|Novelty Note 4|2013-07-02 07:00:00|   phone|\n|    20|  iFruit|            3A|2012-07-21 07:00:00|   phone|\n+------+--------+--------------+-------------------+--------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1590353111782_1612020806","id":"20200428-230856_1582268955","dateCreated":"2020-05-24T20:45:11+0000","dateStarted":"2020-05-26T03:43:48+0000","dateFinished":"2020-05-26T03:43:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:133287"},{"title":"15 - Save the DataFrame in Parquet format","text":"%pyspark\ndevDF.write.parquet(\"/user/zeppelin/devices_parquet\") ","user":"sysadmin","dateUpdated":"2020-06-02T03:32:16+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1590353111782_1106776908","id":"20200428-230854_1798105788","dateCreated":"2020-05-24T20:45:11+0000","dateStarted":"2020-05-26T03:43:51+0000","dateFinished":"2020-05-26T03:43:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:133288"},{"title":"16 - View the schema of the Parquet file","text":"%sh\nhdfs dfs -get /user/zeppelin/devices_parquet /tmp/\nparquet-tools schema /tmp/devices_parquet","user":"sysadmin","dateUpdated":"2020-06-02T03:32:21+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"bash: line 1: parquet-tools: command not found\n"},{"type":"TEXT","data":"ExitValue: 127"}]},"apps":[],"jobName":"paragraph_1590353111783_-364691695","id":"20200428-230853_1175884827","dateCreated":"2020-05-24T20:45:11+0000","dateStarted":"2020-05-24T21:53:00+0000","dateFinished":"2020-05-24T21:53:03+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:133289"},{"title":"17 - Create a DataFrame from the Parquet files","text":"%pyspark\nspark.read.parquet(\"/user/zeppelin/devices_parquet\").printSchema()","user":"sysadmin","dateUpdated":"2020-05-24T20:45:11+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590353111783_746550418","id":"20200428-230853_1544838296","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133290"},{"text":"%md\nWe hope you've enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Hortonworks Apache Spark Tutorials](http://hortonworks.com/tutorials/#tuts-developers) are your natural next step where you can explore Spark in more depth.\n2. [Hortonworks Community Connection (HCC)](https://community.hortonworks.com/spaces/85/data-science.html?type=question) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Hortonworks Apache Spark Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html) - official Spark documentation.\n4. [Hortonworks Apache Zeppelin Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html) - official Zeppelin documentation.\n","user":"sysadmin","dateUpdated":"2020-05-31T20:44:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We hope you've enjoyed this lab. Below are additional resources that you should find useful:</p>\n<ol>\n<li><a href=\"http://hortonworks.com/tutorials/#tuts-developers\">Hortonworks Apache Spark Tutorials</a> are your natural next step where you can explore Spark in more depth.</li>\n<li><a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\">Hortonworks Community Connection (HCC)</a> is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html\">Hortonworks Apache Spark Docs</a> - official Spark documentation.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html\">Hortonworks Apache Zeppelin Docs</a> - official Zeppelin documentation.</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1590353111783_2025210624","id":"20181126-133017_244739700","dateCreated":"2020-05-24T20:45:11+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133291"}],"name":"PYSPARK/05-WorkingWithDataFramesAndSchemas","id":"2F8A8H3BR","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"angular:shared_process":[],"sh:shared_process":[],"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}