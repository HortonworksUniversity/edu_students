{"paragraphs":[{"text":"%md\n# About\n**Lab:** Exploring Query Execution\n**Objective:** Explore how Spark executes RDD and DataFrame/Dataset queries.\n**File locations:**\n    HDFS: /user/zeppelin/hive/telco.db/accounts\n          /user/zeppelin/weblogs\n          /user/zeppelin/devices.json\n          /user/zeppelin/accountdevice\n    Hive tables: telco.accounts\n**Successful outcome:** \n**Before you begin:** \n**Related lessons:** \n\n---","user":"sysadmin","dateUpdated":"2020-06-05T00:14:31+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>About</h1>\n<p><strong>Lab:</strong> Exploring Query Execution\n<br  /><strong>Objective:</strong> Explore how Spark executes RDD and DataFrame/Dataset queries.\n<br  /><strong>File locations:</strong></p>\n<pre><code>HDFS: /user/hive/warehouse/devsh.db/accounts\n      /devsh_loudacre/weblogs\n      /devsh_loudacre/devices.json\n      /devsh_loudacre/accountdevice\nHive tables: devsh.accounts\n</code></pre>\n<p><strong>Successful outcome:</strong>\n<br  /><strong>Before you begin:</strong>\n<br  /><strong>Related lessons:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1590950875590_841478254","id":"20181126-092644_1457476546","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:224087"},{"text":"%md\n# Setup","user":"sysadmin","dateUpdated":"2020-06-02T21:56:00+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Setup</h1>\n<p><strong>Important:</strong> This exercise depends on <strong><em> ***Insert previous exercise title here (with link?)*** </em></strong>. If you did not complete that exercise, run the course catch-up script and advance to the current exercise:</p>\n<pre><code>$ $DEVSH/scripts/catchup.sh\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1590950875590_-1885234015","id":"20181201-044336_178705192","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224088"},{"text":"%md\n# Lab\n","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Lab</h1>\n"}]},"apps":[],"jobName":"paragraph_1590950875591_493259130","id":"20181126-093358_358613711","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224089"},{"text":"%md\n### Explore Partitioning of File-Based RDDs","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Explore Partitioning of File-Based RDDs</h3>\n"}]},"apps":[],"jobName":"paragraph_1590950875591_2029036162","id":"20200426-002311_1962808726","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224090"},{"title":"1 - Note the number and size of accounts data files in HDFS","text":"%md\nReview the accounts data files in HDFS. (`user/zeppelin/accounts`) Take note of the number and sizes of files.","user":"sysadmin","dateUpdated":"2020-05-31T18:51:47+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875591_-1539874829","id":"20200426-002408_1814900362","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224091"},{"text":"%sh\nhdfs dfs -ls /user/zeppelin/hive/telco.db/accounts","user":"sysadmin","dateUpdated":"2020-06-02T21:56:20+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 13 items\ndrwx------   - zeppelin hdfs          0 2020-05-21 12:02 /user/zeppelin/.Trash\ndrwxr-xr-x   - zeppelin hdfs          0 2020-05-21 17:42 /user/zeppelin/.sparkStaging\ndrwxr-xr-x   - zeppelin hdfs          0 2020-05-21 20:36 /user/zeppelin/accountdevice\ndrwxr-xr-x   - zeppelin hdfs          0 2020-05-22 20:49 /user/zeppelin/accounts_zip94913\ndrwxrwxrwx   - zeppelin hdfs          0 2020-05-22 23:30 /user/zeppelin/conf\n-rw-r--r--   3 zeppelin hdfs       5483 2020-05-21 18:32 /user/zeppelin/devices.json\ndrwxr-xr-x   - zeppelin hdfs          0 2020-05-22 21:50 /user/zeppelin/devices_parquet\ndrwxr-xr-x   - zeppelin hdfs          0 2020-05-22 04:14 /user/zeppelin/iplist\ndrwxr-xr-x   - zeppelin hdfs          0 2020-05-21 05:55 /user/zeppelin/kb\ndrwxr-xr-x   - zeppelin hdfs          0 2020-05-22 22:16 /user/zeppelin/name_dev\ndrwxrwxrwx   - zeppelin hdfs          0 2020-05-22 23:30 /user/zeppelin/notebook\ndrwxrwxrwx   - zeppelin hdfs          0 2020-04-29 00:53 /user/zeppelin/test\ndrwxr-xr-x   - zeppelin hdfs          0 2020-05-22 01:49 /user/zeppelin/weblogs\n"}]},"apps":[],"jobName":"paragraph_1590950875591_1372203552","id":"20200522-233345_410445506","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224092"},{"title":"2 - Create an RDD using the accounts data","text":"%md\nCreate an RDD called `accountsRDD` by reading the accounts data, splitting it by commas, and keying it by account ID, which is the first field of each line.","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875591_-420136932","id":"20200426-002628_1889296219","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224093"},{"text":"%pyspark\naccountsRDD = sc. \\\ntextFile(\"/user/zeppelin/hive/telco.db/accounts\"). \\\nmap(lambda line: line.split(',')). \\\nmap(lambda account: (account[0],account))","user":"sysadmin","dateUpdated":"2020-06-02T21:56:40+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1590950875591_93359653","id":"20200426-002657_1953644645","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224094"},{"title":"3 - Find the number of partitions in the new RDD","text":"%pyspark\naccountsRDD.getNumPartitions()","user":"sysadmin","dateUpdated":"2020-06-02T21:56:48+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875592_-1121578501","id":"20200426-002706_444177779","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224095"},{"title":"4 - Review the lineage and execution plan of accountsRDD","text":"%md\nUse `toDebugString` to view the lineage and execution plan of `accountsRDD`. How many partitions are in the resulting RDD? How may stages does the query have?","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875592_506188312","id":"20200426-002725_436524214","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224096"},{"text":"%pyspark\nprint accountsRDD.toDebugString()","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875592_1450986614","id":"20200426-002725_1871429356","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224097"},{"text":"%md\n### Explore Execution of RDD Queries","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Explore Execution of RDD Queries</h3>\n"}]},"apps":[],"jobName":"paragraph_1590950875592_1766246892","id":"20200426-002724_1932934908","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224098"},{"title":"5 - Trigger the execution of a job","text":"%md\nCall `count` on `accountsRDD` to count the number of accounts. This will trigger execution of a job.","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875592_758772209","id":"20200426-002723_2144549000","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224099"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875592_889181036","id":"20200426-002723_1061198580","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224100"},{"title":"6 - View the Spark application UI","text":"%md\nIn the browser, view the application in the YARN RM UI and click through to view the Spark Application UI.","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>In the browser, view the application in the YARN RM UI using the provided bookmark (or <a href=\"http://localhost:8088\">http://localhost:8088</a> and click through to view the Spark Application UI.</p>\n<p><strong><em> !!!! Does this link work? !!! </em></strong></p>\n"}]},"apps":[],"jobName":"paragraph_1590950875592_-297160480","id":"20200426-002722_1296221776","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224101"},{"title":"7 - Confirm that the execution of the job matches the results from toDebugString","text":"%md\nMake sure the **Jobs** tab is selected, and review the list of completed jobs. The most recent job, which you triggered by calling count, should be at the top of the list. (Note that the job description is usually based on the action that triggered the job execution.) Confirm that the number of stages is correct, and the number of tasks completed for the job matches the number of RDD partitions you noted when you used `toDebugString`.","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875593_-69209281","id":"20200426-003259_783721808","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224102"},{"title":"8 - View the details of the job","text":"%md\nClick on the job description to view details of the job. This will list all the stages in the job, which in this case is one.","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875593_-611053577","id":"20200426-003259_1099747477","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224103"},{"title":"9 - View the diagram of the execution plan","text":"%md\nClick on **DAG Visualization** to see a diagram of the execution plan based on the RDD's lineage. The main diagram displays only the stages, but if you click on a stage, it will show you the tasks within that stage.","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875593_-1648434891","id":"20200426-003415_1588367725","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224104"},{"title":"10 - Explore the partitioning and DAG of a more complex query","text":"%md\n*Optional:* Explore the partitioning and DAG of a more complex query like the one below. Before you view the execution plan or job details, try to figure out how many stages the job will have.\n\nThis query loads Loudacre's web log data, and calculates how many times each user visited. Then it joins that user count data with account data for each user.","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875593_-1574805701","id":"20200426-003259_310142277","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224105"},{"text":"%pyspark\nlogsRDD = sc.textFile(\"/user/zeppelin/weblogs\")\n\nuserReqsRDD = logsRDD. \\\nmap(lambda line: line.split(' ')). \\\nmap(lambda words: (words[2],1)). \\\nreduceByKey(lambda v1,v2: v1 + v2)\n\naccountHitsRDD = accountsRDD.join(userReqsRDD)\naccountHitsRDD.saveAsTextFile(\"/user/zeppelin/account_hits\")","user":"sysadmin","dateUpdated":"2020-06-02T21:59:25+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875593_214263231","id":"20200426-003258_1413992919","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224106"},{"text":"%md\n**Note:** If you execute the query multiple times, you may note that some tasks within a stage are marked as \"skipped.\"\" This is because whenever a shuffle operation is executed, Spark temporarily caches the data that was shuffled. Subsequent executions of the same query re-use that data if it's available to save some steps and improve performance.","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><strong>Note:</strong> If you execute the query multiple times, you may note that some tasks within\n<br  />a stage are marked as &ldquo;skipped.&ldquo;&rdquo; This is because whenever a shuffle operation\n<br  />is executed, Spark temporarily caches the data that was shuffled. Subsequent\n<br  />executions of the same query re-use that data if it's available to save some steps and\n<br  />improve performance.</p>\n"}]},"apps":[],"jobName":"paragraph_1590950875593_-1659883833","id":"20200426-003257_150288643","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224107"},{"text":"%md\n### Explore Execution of DataFrame Queries","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Explore Execution of DataFrame Queries</h3>\n"}]},"apps":[],"jobName":"paragraph_1590950875594_-764194222","id":"20200426-002721_938055623","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224108"},{"title":"11 - Create a DataFrame of active accounts","text":"%md\nCreate a DataFrame of active accounts from the `accounts` table in the `telco` database.","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875594_-1861270331","id":"20200426-002721_1060215009","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224109"},{"text":"%pyspark\naccountsDF = spark.read.table(\"telco.accounts\")\n\nactiveAccountsDF = accountsDF. \\\nselect(\"acct_num\"). \\\nwhere(accountsDF.acct_close_dt.isNull())","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875594_1270674255","id":"20200426-002720_315865919","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224110"},{"title":"12 - View the full execution plan for the new DataFrame","text":"%pyspark\nactiveAccountsDF.explain(True)","user":"sysadmin","dateUpdated":"2020-06-02T21:59:41+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875594_-116090503","id":"20200426-002719_2094070605","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224111"},{"text":"%md\nCan you locate the line in the physical plan corresponding to the command to load the `telco.accounts` table into a DataFrame?\n\nHow many stages do you think this query has?","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Can you locate the line in the physical plan corresponding to the command to load\n<br  />the <code>devsh.accounts</code> table into a DataFrame?</p>\n<p>How many stages do you think this query has?</p>\n"}]},"apps":[],"jobName":"paragraph_1590950875595_-701439297","id":"20200426-002717_1757149911","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224112"},{"title":"13 - Call the DataFrame's show function to execute the query","text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-06-02T22:00:20+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875595_-1238502029","id":"20200426-002717_1959166962","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224113"},{"title":"14 - Use the Spark Application UI to view the query","text":"%md\nView the Spark Application UI and choose the **SQL** tab. This displays a list of DataFrame and Dataset queries you have executed, with the most recent query at the top.","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875595_292801699","id":"20200426-030300_272260684","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224114"},{"title":"15 - View the query's execution plan","text":"%md\nClick the description for the top query to see the visualization of the query's execution. You can also see the query's full execution plan by opening the **Details** panel below the visualization graph.","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875595_-328661613","id":"20200426-030259_1650065573","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224115"},{"title":"16 - Review the query's execution plan","text":"%md\nThe first step in the execution is a `HiveTableScan`, which loaded the account data into the DataFrame. Hover your mouse over the step to show the step's execution plan. Compare that to the physical plan for the query. Note that it is the same as the last line in the physical execution plan, because it is the first step to execute. Did you correctly identify this line in the execution plan as the one corresponding to the `DataFrame.read.table` operation?","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875595_1010935781","id":"20200426-030259_1256056256","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224116"},{"title":"17 - View the job details for the query","text":"%md\nClick the **SQL** tab to return to the main SQL query summary tab. The **Job IDs** column provides links to the jobs that executed as part of this query execution. In this case, the query consisted of just a single job. Click the job's ID to view the job details. This will display a list of stages that were completed for the query. How many stages executed? Is that the number of stages you predicted it would be?","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875595_-1803407033","id":"20200426-030258_341547260","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224117"},{"title":"18 - View metrics of the execution","text":"%md\n*Optional:* Click the description of the stage to view metrics on the execution of the stage and its tasks.","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875596_-1491177585","id":"20200426-030258_1940765022","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224118"},{"title":"19 - Joining different data sources","text":"%md\nThe previous query was very simple, involving just a single data source with a where to return only active accounts. Try executing a more complex query that joins data from two different data sources.\n\nThis query reads in the `accountdevice` data file, which maps account IDs to associated device IDs. Then it joins that data with the DataFrame of active accounts you created above. The result is DataFrame consisting of all device IDs in use by currently active accounts.","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The previous query was very simple, involving just a single data source with a\n<br  />where to return only active accounts. Try executing a more complex query that\n<br  />joins data from two different data sources.</p>\n<p>This query reads in the <code>accountdevice</code> data file, which maps account IDs to\n<br  />associated device IDs. Then it joins that data with the DataFrame of active accounts\n<br  />you created above. The result is DataFrame consisting of all device IDs in use by\n<br  />currently active accounts.</p>\n"}]},"apps":[],"jobName":"paragraph_1590950875596_-1044468834","id":"20200426-030258_69484237","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224119"},{"text":"%pyspark\naccountDeviceDF = spark.read. \\\noption(\"header\",\"true\"). \\\noption(\"inferSchema\",\"true\"). \\\ncsv(\"/user/zeppelin/accountdevice\")\n\nactiveAcctDevsDF = activeAccountsDF. \\\njoin(accountDeviceDF,\naccountDeviceDF.acct_num ==\naccountDeviceDF.account_id). \\\nselect(\"device_id\")","user":"sysadmin","dateUpdated":"2020-06-02T22:01:31+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875596_2103050854","id":"20200426-030257_1766470330","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224120"},{"title":"20 - Analyzing the execution plan","text":"%md\nReview the full execution plan using `explain`, as you did with the previous DataFrame.\n\nCan you identify which lines in the execution plan load the two different data sources?\n\nHow many stages do you think this query will execute?","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Review the full execution plan using <code>explain</code>, as you did with the previous\n<br  />DataFrame.</p>\n<p>Can you identify which lines in the execution plan load the two different data\n<br  />sources?</p>\n<p>How many stages do you think this query will execute?</p>\n"}]},"apps":[],"jobName":"paragraph_1590950875596_1906369779","id":"20200426-030257_1536492862","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224121"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875596_-1147942993","id":"20200426-031346_1665958761","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224122"},{"title":"21 - Analyzing the execution visualization","text":"%md\nExecute the query and review the execution visualization in the Spark UI.\n\nWhat differences do you see between the execution of the earlier query and this one?\n\nHow many stages executed? Is this what you expected?","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Execute the query and review the execution visualization in the Spark UI.</p>\n<p>What differences do you see between the execution of the earlier query and this\n<br  />one?</p>\n<p>How many stages executed? Is this what you expected?</p>\n"}]},"apps":[],"jobName":"paragraph_1590950875596_-252408623","id":"20200426-030256_786143272","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224123"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-06-02T22:02:04+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591135300205_-1833025864","id":"20200602-220140_1443082639","dateCreated":"2020-06-02T22:01:40+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:224124"},{"title":"22 - Explore a query with multiple joins","text":"%md\n*Optional:* Explore an even more complex query that involves multiple joins with three data sources. You can use the last query in the solutions file for this exercise (in the `/home/devuser/bin/devsh/exercises/query-execution/solution/ directory`). That query creates a list of device IDs, makes, and models, and the number of active accounts that use that type of device, sorted in order from most popular device type to least.","user":"sysadmin","dateUpdated":"2020-06-02T22:02:39+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>!!! Solution doesn't answer this question !!!</h2>\n"}]},"apps":[],"jobName":"paragraph_1590950875596_1682771332","id":"20200426-030255_969709522","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224125"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-06-02T22:02:04+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591135305480_-1181690782","id":"20200602-220145_823272845","dateCreated":"2020-06-02T22:01:45+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:224126"},{"text":"%md\n# Result\n**You have now:** \n\n---","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Result</h1>\n<p><strong>You have now:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1590950875597_1676396414","id":"20181126-133507_1472573213","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224127"},{"text":"%md\n# Solution\n---","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Solution</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1590950875597_759278882","id":"20181018-125200_1133281582","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224128"},{"text":"%md\n### Explore Partitioning of File-Based RDDs","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Explore Partitioning of File-Based RDDs</h3>\n"}]},"apps":[],"jobName":"paragraph_1590950875597_-422833610","id":"20200429-202110_912512721","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224129"},{"title":"1 - Note the number and size of accounts data files in HDFS","text":"%sh\nhdfs dfs -ls /user/zeppelin/hive/telco.db/accounts/","user":"sysadmin","dateUpdated":"2020-06-02T03:46:13+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 5 items\n-rw-r--r--   3 hive hdfs    4706617 2020-05-31 18:28 /warehouse/tablespace/external/hive/telco_stage.db/accounts_stage/part-m-00000\n-rw-r--r--   3 hive hdfs    4693530 2020-05-31 18:28 /warehouse/tablespace/external/hive/telco_stage.db/accounts_stage/part-m-00001\n-rw-r--r--   3 hive hdfs    4674529 2020-05-31 18:28 /warehouse/tablespace/external/hive/telco_stage.db/accounts_stage/part-m-00002\n-rw-r--r--   3 hive hdfs    4662646 2020-05-31 18:28 /warehouse/tablespace/external/hive/telco_stage.db/accounts_stage/part-m-00003\n-rw-r--r--   3 hive hdfs        129 2020-05-31 18:28 /warehouse/tablespace/external/hive/telco_stage.db/accounts_stage/part-m-00004\n"}]},"apps":[],"jobName":"paragraph_1590950875597_350409513","id":"20200429-202127_377500485","dateCreated":"2020-05-31T18:47:55+0000","dateStarted":"2020-05-31T18:51:37+0000","dateFinished":"2020-05-31T18:51:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:224130"},{"title":"2 - Create an RDD using the accounts data","text":"%pyspark\naccountsRDD = sc.textFile(\"/user/zeppelin/hive/telco.db/accounts\").map(lambda line: line.split(',')).map(lambda accountFields: (accountFields[0],accountFields))","user":"sysadmin","dateUpdated":"2020-06-02T03:46:37+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1590950875597_1913260177","id":"20200429-202212_2105766018","dateCreated":"2020-05-31T18:47:55+0000","dateStarted":"2020-05-31T18:53:30+0000","dateFinished":"2020-05-31T18:53:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:224131"},{"title":"3 - Find the number of partitions in the new RDD","text":"%pyspark\naccountsRDD.getNumPartitions()","user":"sysadmin","dateUpdated":"2020-05-31T18:53:35+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"5\n"}]},"apps":[],"jobName":"paragraph_1590950875598_-1698209506","id":"20200429-202210_1469133159","dateCreated":"2020-05-31T18:47:55+0000","dateStarted":"2020-05-31T18:53:35+0000","dateFinished":"2020-05-31T18:53:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:224132"},{"title":"4 - Review the lineage and execution plan of accountsRDD","text":"%pyspark\nprint(accountsRDD.toDebugString())","user":"sysadmin","dateUpdated":"2020-05-31T18:53:49+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(5) PythonRDD[165] at RDD at PythonRDD.scala:52 []\n |  /warehouse/tablespace/external/hive/telco_stage.db/accounts_stage MapPartitionsRDD[164] at textFile at NativeMethodAccessorImpl.java:0 []\n |  /warehouse/tablespace/external/hive/telco_stage.db/accounts_stage HadoopRDD[163] at textFile at NativeMethodAccessorImpl.java:0 []\n"}]},"apps":[],"jobName":"paragraph_1590950875598_1529987529","id":"20200429-202209_1709217881","dateCreated":"2020-05-31T18:47:55+0000","dateStarted":"2020-05-31T18:53:49+0000","dateFinished":"2020-05-31T18:53:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:224133"},{"text":"%md\n### Explore Execution of RDD Queries","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Explore Execution of RDD Queries</h3>\n"}]},"apps":[],"jobName":"paragraph_1590950875598_1229191872","id":"20200429-202207_1214551008","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224134"},{"title":"5 - Trigger the execution of a job","text":"%pyspark\naccountsRDD.count()","user":"sysadmin","dateUpdated":"2020-05-31T18:54:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"129762\n"}]},"apps":[],"jobName":"paragraph_1590950875599_-145432090","id":"20200429-202207_1158636728","dateCreated":"2020-05-31T18:47:55+0000","dateStarted":"2020-05-31T18:54:05+0000","dateFinished":"2020-05-31T18:54:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:224135"},{"title":"6 - View the Spark application UI","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875599_488598904","id":"20200429-202205_1104714050","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224136"},{"title":"7 - Confirm that the execution of the job matches the results from toDebugString","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875599_-86460906","id":"20200429-202205_1242768058","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224137"},{"title":"8 - View the details of the job","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875599_847702193","id":"20200429-202204_747605927","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224138"},{"title":"9 - View the diagram of the execution plan","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875599_277822285","id":"20200429-202204_413731544","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224139"},{"title":"10 - Explore the partitioning and DAG of a more complex query","text":"%pyspark\nlogsRDD = sc.textFile(\"/user/zeppelin/weblogs\")\nuserReqsRDD = logsRDD.map(lambda line: line.split(' ')).map(lambda words: (words[2],1)). reduceByKey(lambda v1,v2: v1 + v2)\naccountHitsRDD = accountsRDD.join(userReqsRDD)\naccountHitsRDD.saveAsTextFile(\"/user/zeppelin/account_hits\")","user":"sysadmin","dateUpdated":"2020-05-31T18:56:03+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1590950875599_-1637423342","id":"20200429-202203_721477472","dateCreated":"2020-05-31T18:47:55+0000","dateStarted":"2020-05-31T18:55:43+0000","dateFinished":"2020-05-31T18:56:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:224140"},{"text":"%md\n### Explore Execution of DataFrame Queries","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875600_-1388225194","id":"20200429-202806_748490264","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224141"},{"title":"11 - Create a DataFrame of active accounts","text":"%pyspark\naccountsDF = spark.read.table(\"telco.accounts\")\nactiveAccountsDF = accountsDF.select(\"acct_num\").where(accountsDF.acct_close_dt.isNotNull())","user":"sysadmin","dateUpdated":"2020-06-02T03:46:55+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1590950875600_297435969","id":"20200429-202805_170124417","dateCreated":"2020-05-31T18:47:55+0000","dateStarted":"2020-05-31T18:59:09+0000","dateFinished":"2020-05-31T18:59:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:224142"},{"title":"12 - View the full execution plan for the new DataFrame","text":"%pyspark\nactiveAccountsDF.explain(True)","user":"sysadmin","dateUpdated":"2020-05-31T18:59:15+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"== Parsed Logical Plan ==\n!Filter isnotnull(acct_close_dt#241)\n+- AnalysisBarrier\n      +- Project [acct_num#239]\n         +- SubqueryAlias accounts_stage\n            +- HiveTableRelation `telco`.`accounts_stage`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [acct_num#239, acct_create_dt#240, acct_close_dt#241, first_name#242, last_name#243, address#244, city#245, state#246, zipcode#247, phone_number#248, created#249, modified#250]\n\n== Analyzed Logical Plan ==\nacct_num: int\nProject [acct_num#239]\n+- Filter isnotnull(acct_close_dt#241)\n   +- Project [acct_num#239, acct_close_dt#241]\n      +- SubqueryAlias accounts_stage\n         +- HiveTableRelation `telco`.`accounts_stage`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [acct_num#239, acct_create_dt#240, acct_close_dt#241, first_name#242, last_name#243, address#244, city#245, state#246, zipcode#247, phone_number#248, created#249, modified#250]\n\n== Optimized Logical Plan ==\nProject [acct_num#239]\n+- Filter isnotnull(acct_close_dt#241)\n   +- HiveTableRelation `telco`.`accounts_stage`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [acct_num#239, acct_create_dt#240, acct_close_dt#241, first_name#242, last_name#243, address#244, city#245, state#246, zipcode#247, phone_number#248, created#249, modified#250]\n\n== Physical Plan ==\n*(1) Project [acct_num#239]\n+- *(1) Filter isnotnull(acct_close_dt#241)\n   +- HiveTableScan [acct_close_dt#241, acct_num#239], HiveTableRelation `telco`.`accounts_stage`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [acct_num#239, acct_create_dt#240, acct_close_dt#241, first_name#242, last_name#243, address#244, city#245, state#246, zipcode#247, phone_number#248, created#249, modified#250]\n"}]},"apps":[],"jobName":"paragraph_1590950875600_-1485895626","id":"20200429-202803_862265902","dateCreated":"2020-05-31T18:47:55+0000","dateStarted":"2020-05-31T18:59:15+0000","dateFinished":"2020-05-31T18:59:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:224143"},{"title":"13 - Execute the query","text":"%pyspark\nactiveAccountsDF.show()","user":"sysadmin","dateUpdated":"2020-05-31T18:59:29+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------+\n|acct_num|\n+--------+\n|       6|\n|       7|\n|       9|\n|      16|\n|      19|\n|      27|\n|      28|\n|      34|\n|      35|\n|      40|\n|      44|\n|      45|\n|      46|\n|      51|\n|      53|\n|      59|\n|      62|\n|      63|\n|      66|\n|      68|\n+--------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1590950875601_2147479561","id":"20200429-202802_502625491","dateCreated":"2020-05-31T18:47:55+0000","dateStarted":"2020-05-31T18:59:29+0000","dateFinished":"2020-05-31T18:59:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:224144"},{"title":"14 - Use the Spark Application UI to view the query","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875601_-494073183","id":"20200429-202800_1572827814","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224145"},{"title":"15 - View the query's execution plan","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875601_-1186473767","id":"20200429-202759_1576628165","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224146"},{"title":"16 - Review the query's execution plan","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875601_267819414","id":"20200429-202759_1612498172","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224147"},{"title":"17 - View the job details for the query","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875601_190902039","id":"20200429-202758_2045057848","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224148"},{"title":"18 - View metrics of the execution","user":"sysadmin","dateUpdated":"2020-05-31T18:47:55+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1590950875602_-1696352342","id":"20200429-202758_1197049485","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224149"},{"title":"19 - Joining different data sources","text":"%pyspark\n# ------ Load account device data ---------\n# Create a DataFrame from the account device data\naccountDeviceDF = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/user/zeppelin/accountdevice\")\naccountDeviceDF.explain(True)\naccountDeviceDF.show(5)\n\n# -------- query for device IDs for active accounts\n# Create a DataFrame with a device model IDs for only devices \n# used by unclosed accounts\nactiveAcctDevsDF =  activeAccountsDF.join(accountDeviceDF,activeAccountsDF.acct_num == accountDeviceDF.account_id).select(\"device_id\")","user":"sysadmin","dateUpdated":"2020-05-31T19:01:02+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"== Parsed Logical Plan ==\nRelation[id#280,account_id#281,device_id#282,activation_date#283L,account_device_id#284] csv\n\n== Analyzed Logical Plan ==\nid: int, account_id: int, device_id: int, activation_date: bigint, account_device_id: string\nRelation[id#280,account_id#281,device_id#282,activation_date#283L,account_device_id#284] csv\n\n== Optimized Logical Plan ==\nRelation[id#280,account_id#281,device_id#282,activation_date#283L,account_device_id#284] csv\n\n== Physical Plan ==\n*(1) FileScan csv [id#280,account_id#281,device_id#282,activation_date#283L,account_device_id#284] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://cloudairha/user/zeppelin/accountdevice], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,account_id:int,device_id:int,activation_date:bigint,account_device_id:string>\n+-----+----------+---------+---------------+--------------------+\n|   id|account_id|device_id|activation_date|   account_device_id|\n+-----+----------+---------+---------------+--------------------+\n|48692|     32443|       29|  1393242509000|7351fed1-f344-4cd...|\n|48693|     32444|        4|  1353649861000|6da22278-ff7a-461...|\n|48694|     32445|        9|  1331819465000|cb993b85-6775-407...|\n|48695|     32446|       43|  1336860950000|48ea2c09-a0df-4d1...|\n|48696|     32446|       29|  1383650663000|4b49c0a6-d141-42e...|\n+-----+----------+---------+---------------+--------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1590950875602_-1079287537","id":"20200429-202757_232787747","dateCreated":"2020-05-31T18:47:55+0000","dateStarted":"2020-05-31T19:01:02+0000","dateFinished":"2020-05-31T19:01:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:224150"},{"title":"20 - Analyzing the execution plan","text":"%pyspark\nactiveAcctDevsDF.explain(True)","user":"sysadmin","dateUpdated":"2020-05-31T19:01:49+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"== Parsed Logical Plan ==\n'Project [unresolvedalias('device_id, None)]\n+- AnalysisBarrier\n      +- Join Inner, (acct_num#239 = account_id#281)\n         :- Project [acct_num#239]\n         :  +- Filter isnotnull(acct_close_dt#241)\n         :     +- Project [acct_num#239, acct_close_dt#241]\n         :        +- SubqueryAlias accounts_stage\n         :           +- HiveTableRelation `telco`.`accounts_stage`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [acct_num#239, acct_create_dt#240, acct_close_dt#241, first_name#242, last_name#243, address#244, city#245, state#246, zipcode#247, phone_number#248, created#249, modified#250]\n         +- Relation[id#280,account_id#281,device_id#282,activation_date#283L,account_device_id#284] csv\n\n== Analyzed Logical Plan ==\ndevice_id: int\nProject [device_id#282]\n+- Join Inner, (acct_num#239 = account_id#281)\n   :- Project [acct_num#239]\n   :  +- Filter isnotnull(acct_close_dt#241)\n   :     +- Project [acct_num#239, acct_close_dt#241]\n   :        +- SubqueryAlias accounts_stage\n   :           +- HiveTableRelation `telco`.`accounts_stage`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [acct_num#239, acct_create_dt#240, acct_close_dt#241, first_name#242, last_name#243, address#244, city#245, state#246, zipcode#247, phone_number#248, created#249, modified#250]\n   +- Relation[id#280,account_id#281,device_id#282,activation_date#283L,account_device_id#284] csv\n\n== Optimized Logical Plan ==\nProject [device_id#282]\n+- Join Inner, (acct_num#239 = account_id#281)\n   :- Project [acct_num#239]\n   :  +- Filter (isnotnull(acct_close_dt#241) && isnotnull(acct_num#239))\n   :     +- HiveTableRelation `telco`.`accounts_stage`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [acct_num#239, acct_create_dt#240, acct_close_dt#241, first_name#242, last_name#243, address#244, city#245, state#246, zipcode#247, phone_number#248, created#249, modified#250]\n   +- Project [account_id#281, device_id#282]\n      +- Filter isnotnull(account_id#281)\n         +- Relation[id#280,account_id#281,device_id#282,activation_date#283L,account_device_id#284] csv\n\n== Physical Plan ==\n*(2) Project [device_id#282]\n+- *(2) BroadcastHashJoin [acct_num#239], [account_id#281], Inner, BuildLeft\n   :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n   :  +- *(1) Project [acct_num#239]\n   :     +- *(1) Filter (isnotnull(acct_close_dt#241) && isnotnull(acct_num#239))\n   :        +- HiveTableScan [acct_close_dt#241, acct_num#239], HiveTableRelation `telco`.`accounts_stage`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [acct_num#239, acct_create_dt#240, acct_close_dt#241, first_name#242, last_name#243, address#244, city#245, state#246, zipcode#247, phone_number#248, created#249, modified#250]\n   +- *(2) Project [account_id#281, device_id#282]\n      +- *(2) Filter isnotnull(account_id#281)\n         +- *(2) FileScan csv [account_id#281,device_id#282] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://cloudairha/user/zeppelin/accountdevice], PartitionFilters: [], PushedFilters: [IsNotNull(account_id)], ReadSchema: struct<account_id:int,device_id:int>\n"}]},"apps":[],"jobName":"paragraph_1590950875602_-1871187072","id":"20200429-203545_1342409357","dateCreated":"2020-05-31T18:47:55+0000","dateStarted":"2020-05-31T19:01:49+0000","dateFinished":"2020-05-31T19:01:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:224151"},{"title":"21 - Analyzing the execution visualization","text":"%pyspark\nactiveAcctDevsDF.write.mode(\"overwrite\").save(\"/user/zeppelin/active_account_devices\")","user":"sysadmin","dateUpdated":"2020-05-31T19:01:56+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1590950875602_1968586656","id":"20200429-203543_1186915220","dateCreated":"2020-05-31T18:47:55+0000","dateStarted":"2020-05-31T19:01:56+0000","dateFinished":"2020-05-31T19:01:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:224152"},{"title":"22 - Explore a query with multiple joins","text":"%pyspark\n# Sum up the total number of each device model \nsumDevicesDF = activeAcctDevsDF.groupBy(\"device_id\").count().withColumnRenamed(\"count\",\"active_num\")\n\n# Order by count\norderDevicesDF = sumDevicesDF.orderBy(sumDevicesDF.active_num.desc())\n\n# Join the list of device model totals with the list of devices\n# to get the make and model for each device\ndevDF = spark.read.json(\"/user/zeppelin/devices.json\")\njoinDevicesDF = orderDevicesDF.join(devDF,sumDevicesDF.device_id == devDF.devnum)\n\n# Write out the data with the correct columns\njoinDevicesDF.select(\"device_id\",\"make\",\"model\",joinDevicesDF.active_num).write.mode(\"overwrite\").save(\"/user/zeppelin/top_devices\")","user":"sysadmin","dateUpdated":"2020-05-31T19:07:03+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1590950875603_-4127168","id":"20200429-205046_1687837521","dateCreated":"2020-05-31T18:47:55+0000","dateStarted":"2020-05-31T19:07:03+0000","dateFinished":"2020-05-31T19:07:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:224153"},{"text":"%md\nWe hope you've enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Hortonworks Apache Spark Tutorials](http://hortonworks.com/tutorials/#tuts-developers) are your natural next step where you can explore Spark in more depth.\n2. [Hortonworks Community Connection (HCC)](https://community.hortonworks.com/spaces/85/data-science.html?type=question) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Hortonworks Apache Spark Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html) - official Spark documentation.\n4. [Hortonworks Apache Zeppelin Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html) - official Zeppelin documentation.\n","user":"sysadmin","dateUpdated":"2020-05-31T19:05:51+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We hope you've enjoyed this lab. Below are additional resources that you should find useful:</p>\n<ol>\n<li><a href=\"http://hortonworks.com/tutorials/#tuts-developers\">Hortonworks Apache Spark Tutorials</a> are your natural next step where you can explore Spark in more depth.</li>\n<li><a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\">Hortonworks Community Connection (HCC)</a> is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html\">Hortonworks Apache Spark Docs</a> - official Spark documentation.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html\">Hortonworks Apache Zeppelin Docs</a> - official Zeppelin documentation.</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1590950875603_-2059969402","id":"20181126-133017_244739700","dateCreated":"2020-05-31T18:47:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224154"}],"name":"PYSPARK/13-ExploringQueryExecution","id":"2FBZAMBSZ","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}