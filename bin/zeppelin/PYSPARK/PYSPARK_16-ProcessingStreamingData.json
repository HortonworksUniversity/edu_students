{"paragraphs":[{"text":"%md\n# About\n**Lab:** Processing Streaming Data\n**Objective:** Read and process streaming data from a set of files.\n**File locations:**\n    Test data (local): /home/devuser/data/telco/activations_stream/\n    Test script: /home/devuser/bin/devsh/scripts/streamtest-file.sh\n**Successful outcome:** \n**Before you begin:** \n**Related lessons:** \n\n---","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>About</h1>\n<p><strong>Lab:</strong> Processing Streaming Data\n<br  /><strong>Objective:</strong> Read and process streaming data from a set of files.\n<br  /><strong>File locations:</strong></p>\n<pre><code>Test data (local): $DEVDATA/activations_stream/\nTest script: $DEVSH/scripts/streamtest-file.sh\n</code></pre>\n<p><strong>Successful outcome:</strong>\n<br  /><strong>Before you begin:</strong>\n<br  /><strong>Related lessons:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591316154968_-1962541407","id":"20181126-092644_1457476546","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:229990"},{"text":"%md\n# Setup","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Setup</h1>\n<p><strong>Important:</strong> This exercise depends on <strong><em> ***Insert previous exercise title here (with link?)*** </em></strong>. If you did not complete that exercise, run the course catch-up script and advance to the current exercise:</p>\n<pre><code>$ $DEVSH/scripts/catchup.sh\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591316154969_-2066811788","id":"20181201-044336_178705192","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:229991"},{"text":"%md\n# Lab\n","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Lab</h1>\n"}]},"apps":[],"jobName":"paragraph_1591316154970_1134517264","id":"20181126-093358_358613711","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:229992"},{"text":"%md\n### Display Streaming Data to the Console\n\nIn this section, you will read data from a file-based stream and display the results to the console. The query in this section is very simple -- it does not transform the data, and simply outputs the data it receives \"as-is.\"\"","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Display Streaming Data to the Console</h3>\n<p>In this section, you will read data from a file-based stream and display the results to the\n<br  />console. The query in this section is very simple &ndash; it does not transform the data, and\n<br  />simply outputs the data it receives &ldquo;as-is.&ldquo;&rdquo;</p>\n"}]},"apps":[],"jobName":"paragraph_1591316154971_399524198","id":"20200426-194011_597040821","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:229993"},{"title":"1 - Review the test data you will be using in this exercise","text":"%md\nIt contains information about device activations on Loudacre's cellular network in JSON format.\nThe data is in `/home/devuser/data/telco/activations_stream/`.\nYou will use these files later to simulate a stream of JSON data by running a script that copies the files in one at a time.","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>It contains information about device activations on Loudacre's cellular network in JSON format.\n<br  />The data is in <code>$DEVDATA/activations_stream/</code>.\n<br  />You will use these files later to simulate a stream of JSON data by running a script\n<br  />that copies the files in one at a time.</p>\n"}]},"apps":[],"jobName":"paragraph_1591316154972_-1898690168","id":"20200426-194056_111072930","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:229994"},{"title":"2 - In a terminal, set up a directory to contain the data files that Spark will read","text":"%md\nSet the file permissions to allow your application to access the files.\nDo not copy any data into the directory yet.\n\n```\nmkdir -p /tmp/telco-streaming\nchmod +wr /tmp/telco-streaming\n```\n\n**Note:** The directory from which Spark will load data must exist before the you create the DataFrame based on the data.","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154973_1358011373","id":"20200426-194111_1243893916","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:229995"},{"title":"3 - Prepare a shell to run a streaming application","text":"%md\nIf you currenty have a Spark shell running in a terminal session, exit it.\n\nStart a new Python or Scala Spark shell running on the cluster.\n\n```\npyspark --master local[2]\n```\n\nEnter the code in the following paragraphs into your Spark shell.","user":"sysadmin","dateUpdated":"2020-06-05T00:16:44+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>If you currently have a Spark shell running in a terminal session, exit it.\n<br  />The course exercise environment's resources are not sufficient to run a streaming\n<br  />application, so start a new Python or Scala Spark shell running locally instead of on\n<br  />the cluster.</p>\n<pre><code>$ pyspark --master local[2]\n</code></pre>\n<!-- -->\n<pre><code>$ spark-shell --master local[2]\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591316154974_-1610982238","id":"20200426-194110_1702903882","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:229996"},{"title":"4 - Define a schema for the structure of the input data","text":"%md\n```\ninputDir = \"file:/tmp/telco-streaming/\"\n\nfrom pyspark.sql.types import *\n\nactivationsSchema = StructType([\nStructField(\"acct_num\", IntegerType()),\nStructField(\"dev_id\", StringType()),\nStructField(\"phone\", StringType()),\nStructField(\"model\", StringType())])\n```","user":"sysadmin","dateUpdated":"2020-06-05T00:24:31+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154975_1213641646","id":"20200426-194109_271714022","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:229997"},{"title":"5 - Create a streaming DataFrame by reading the data you reviewed above","text":"%md\n```\nactivationsDF = spark.readStream. \\\nschema(activationsSchema). \\\njson(\"file:/tmp/telco-streaming/\")\n```","user":"sysadmin","dateUpdated":"2020-06-05T00:17:09+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154975_174600404","id":"20200426-194108_1321483966","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:229998"},{"title":"6 - Display the streaming DataFrame's schema to confirm that it is set up correctly","text":"","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154976_438205769","id":"20200426-194106_1144996716","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:229999"},{"title":"7 - Confirm that the DataFrame's isStreaming property is set","text":"","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154977_414180717","id":"20200426-194105_131508129","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230000"},{"title":"8 - Start a streaming query","text":"%md\nStart a streaming query that displays results to the console. Use append mode to display the first several records in each new input stream micro-batch.\n\nSet the `truncate` option so that you will be able to see all the data in each record.\n\n```\nactivationsQuery = activationsDF.writeStream. \\\noutputMode(\"append\"). \\\nformat(\"console\").option(\"truncate\",\"false\"). \\\nstart()\n```\n\nThe query will not display any output yet, because no files are available to read yet.","user":"sysadmin","dateUpdated":"2020-06-05T00:17:26+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Start a streaming query that displays results to the console. Use append mode to\n<br  />display the first several records in each new input stream micro-batch.</p>\n<p>Set the <code>truncate</code> option so that you will be able to see all the data in each record.</p>\n"}]},"apps":[],"jobName":"paragraph_1591316154977_977052799","id":"20200426-194103_454623960","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230001"},{"title":"9 - Begin streaming data","text":"%md\nOpen a new terminal window. Run the test script to copy the test data files into the streaming directory at a rate of one per second.\n\n```\n/home/devuser/bin/devsh/scripts/streamtest-file.sh /home/devuser/data/telco/activations_stream/ /tmp/telco-streaming\n```\n\nThe script will display the names of the files as it copies them.\n\n**Note:** Spark keeps track of files that have been previously read by each query. If you need to re-run the script later to test the same query, Spark will ignore any files that were previously copied.","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Open a new terminal window. Run a test script to copy the test data files into the\n<br  />streaming directory at a rate of one per second.</p>\n<pre><code>$ $DEVSH/scripts/streamtest-file.sh \\\n$DEVDATA/activations_stream/ /tmp/devsh-streaming\n</code></pre>\n<p>The script will display the names of the files as it copies them.</p>\n<p><strong>Note:</strong> Spark keeps track of files that have been previously read by each query. If you\n<br  />need to re-run the script later to test the same query, Spark will ignore any files that\n<br  />were previously copied.</p>\n"}]},"apps":[],"jobName":"paragraph_1591316154978_464364240","id":"20200426-194101_1798043560","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230002"},{"title":"10 - Confirm that the query is displaying data","text":"%md\nReturn to your Spark shell and confirm that the query is displaying data from each batch.\nNote that the first batch processed and displayed will always be empty.","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>!!! Edit needed - &ldquo;displaying console output displaying data from each batch&rdquo; !!!</h4>\n<p>Return to your Spark shell and confirm that the query is displaying console output\n<br  />displaying data from each batch.\n<br  />Note that the first batch processed and displayed will always be empty.</p>\n"}]},"apps":[],"jobName":"paragraph_1591316154978_-179819290","id":"20200426-195419_757368827","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230003"},{"title":"11 - Stopping the stream","text":"%md\nWhen you are done, stop the stream by entering \n\n```\nactivationsQuery.stop()\n```\n\n**Note:** You will not see a prompt while the shell is displaying output, but you can enter commands in the shell anyway.","user":"sysadmin","dateUpdated":"2020-06-05T00:17:49+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>When you are done, stop the stream by entering activationsQuery.stop().</p>\n<p><strong>Note:</strong> You will not see a prompt while the shell is displaying output, but you can\n<br  />enter commands in the shell anyway.</p>\n"}]},"apps":[],"jobName":"paragraph_1591316154979_-2047309274","id":"20200426-195419_1156531890","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230004"},{"title":"12 - Stopping the test script","text":"%md\nTerminate the test script in the second terminal window using `Ctrl+C`.","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154980_686022959","id":"20200426-195418_1358698244","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230005"},{"text":"%md\n### Display Aggregated Streaming Data to the Console\n\nIn this section, you will perform a simple aggregation -- counting devices by model -- and display the results to the console.","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Display Aggregated Streaming Data to the Console</h3>\n<p>In this section, you will perform a simple aggregation &ndash; counting devices by model &ndash; and display the results to the console.</p>\n"}]},"apps":[],"jobName":"paragraph_1591316154980_-1053800118","id":"20200426-195418_1158851408","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230006"},{"title":"13 - Clean the streaming directory","text":"%md\nIn your test script terminal window (not the Spark shell), remove the data that was copied into the streaming directory in the last section.\n\n```\nrm -rf /tmp/telco-streaming/*\n```","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>In your test script terminal window (not the Spark shell), remove data from the\n<br  />streaming directory copied in the last section.</p>\n<pre><code>$ rm -rf /tmp/devsh-streaming/*\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591316154981_1707860485","id":"20200426-195417_1588990866","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230007"},{"title":"14 - Create a DataFrame to count each device model","text":"%md\nGo to your Spark shell. Starting with the `activationsDF` DataFrame you created above, create a second DataFrame containing the count of each device model.\n\n```\nactivationCountDF = activationsDF. \\\ngroupBy(\"model\").count()\n```","user":"sysadmin","dateUpdated":"2020-06-05T00:18:07+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154982_1627537205","id":"20200426-195417_926762270","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230008"},{"title":"15 - Start a streaming query that aggregates the data","text":"%md\nStart a streaming query that displays the model name and count for activated devices. Use `complete` mode so that the data will be aggregated across all the data in all the batches received so far for each interval, rather than just each individual batch.\n\n```\nactivationCountQuery = activationCountDF. \\\nwriteStream.outputMode(\"complete\"). \\\nformat(\"console\").start()\n```","user":"sysadmin","dateUpdated":"2020-06-05T00:18:18+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154982_1479178370","id":"20200426-195415_1492878269","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230009"},{"title":"16 - Run the streamtest-file.sh script again","text":"%md\nReturn to your second terminal window and re-run the `streamtest-file.sh` script you ran in the previous section.","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154983_-919830875","id":"20200426-195414_562992384","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230010"},{"title":"17 - Confirm that complete output mode is activated","text":"%md\nIn your Spark shell, make sure that the models and their counts are being displayed. Confirm that the counts are going up in each batch because you used `complete` output mode.","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154983_1327403616","id":"20200426-195413_572889012","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230011"},{"title":"18 - Stop the query and terminate the test script","text":"%md\nWhen you are done, stop the `activationCountQuery` query and terminate the test script.","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154984_47007432","id":"20200426-195412_2056098062","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230012"},{"text":"%md\n### Output Data to Files\n\nIn this section, youw ill run a query that outputs to a set of files.","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Output Data to Files</h3>\n<p>In this section, youw ill run a query that outputs to a set of files.</p>\n"}]},"apps":[],"jobName":"paragraph_1591316154985_-981063930","id":"20200426-194100_1694382703","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230013"},{"title":"19 - Clean the streaming directory","text":"%md\nIn your test script terminal window (not the Spark shell), remove the data copied into the streaming directory in the last section.\n\n```\nrm -rf /tmp/telco-streaming/*\n```","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154985_1025023769","id":"20200426-201404_1653661450","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230014"},{"title":"20 - Create a new streaming DataFrame","text":"%md\nGo to your Spark shell. Starting with the `activationsDF` DataFrame you created previously, create a new streaming DataFrame with just the rows for the \"Titanic 1000\" devices and the `dev_id` and `acct_num` columns.\n\n```\ntitanic1000DF = activationsDF. \\\nwhere(\"model = 'Titanic 1000'\"). \\\nselect(\"dev_id\",\"acct_num\")\n\ntitanic1000DF.printSchema()\n```","user":"sysadmin","dateUpdated":"2020-06-05T00:23:40+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154986_415060","id":"20200426-201404_1292067866","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230015"},{"title":"21 - Start a query that saves to HDFS","text":"%md\nStart a query that saves the data in the streaming DataFrame you just created to the `/user/zeppelin/titanic1000/` directory in HDFS. Set the query to trigger every three seconds.\n\n**a.** Checkpointing must be enabled to ensure fault tolerance when saving files. Set the checkpoint HDFS directory for the Spark session.\n\n```\nspark.conf.set(\n\"spark.sql.streaming.checkpointLocation\",\n\"/tmp/streaming-checkpoint\")\n```\n\n**b.** Start the query\n\n```\ntitanic1000Query = titanic1000DF. \\\nwriteStream.trigger(processingTime=\"3 seconds\"). \\\noutputMode(\"append\").format(\"csv\"). \\\noption(\"path\",\"/user/zeppelin/titanic1000/\"). \\\nstart()\n```","user":"sysadmin","dateUpdated":"2020-06-05T00:19:12+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Start a query that saves the data in the streaming DataFrame you just created to the <code>/user/zeppelin/titanic1000/`</code> directory in HDFS. Set the query to trigger every three seconds.</p>\n<p><strong>a.</strong> Checkpointing must be enabled to ensure fault tolerance when saving files. Set the checkpoint HDFS directory for the Spark session.</p>\n"}]},"apps":[],"jobName":"paragraph_1591316154987_1557335447","id":"20200426-201402_528992491","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230016"},{"title":"22 - Re-run the stream test script","text":"%md\nReturn to your second terminal window and re-run the `streamtest-file.sh` test script.","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Return to your second terminal window and re-run the <code>streamtest-file.sh</code>\n<br  />test script.</p>\n"}]},"apps":[],"jobName":"paragraph_1591316154988_1418704773","id":"20200426-202548_611263496","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230017"},{"title":"23 - Confirm the target directory is receiving the files","text":"%md\nOpen a new (third) terminal window and list the contents of the HDFS target directory: `/user/zeppelin/titanic1000`. Take note of the number of files.\n\nList the contents again after a few seconds and notice that the number of files is growing as Spark adds new data files to the directory for each incoming microbatch received.","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Open a new (third) terminal window and list the contents of the HDFS target\n<br  />directory: <code>/devsh_loudacre/titanic1000</code>. Take note of the number of files.</p>\n<p>List the contents again after a few seconds and notice that the number of files is\n<br  />growing as Spark adds new data files to the directory for each incoming microbatch\n<br  />received.</p>\n"}]},"apps":[],"jobName":"paragraph_1591316154989_-735065515","id":"20200426-202547_2080286423","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230018"},{"title":"24 - Stopping the test","text":"%md\nWhen you are done, stop the query, terminate the test script, and exit your Spark shell.","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154990_-141133663","id":"20200426-202547_1010479452","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230019"},{"text":"%md\n# Result\n**You have now:** \n\n---","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Result</h1>\n<p><strong>You have now:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591316154990_-2116652576","id":"20181126-133507_1472573213","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230020"},{"text":"%md\n# Solution\n---","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Solution</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591316154991_1526018043","id":"20181018-125200_1133281582","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230021"},{"text":"%md\n### Display Streaming Data to the Console","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154992_948433268","id":"20200429-213425_223954687","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230022"},{"title":"1   - Review the test data you will be using in this exercise","text":"%sh\nhead /home/devuser/data/telco/activations_stream/part-00000-ad954932-b2d7-426d-a7c1-429cfc1ebb33-c000.json","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"{\"acct_num\":128931,\"dev_id\":\"247abaa7-6c85-4f2c-a96f-c11450250506\",\"phone\":\"6190399439\",\"model\":\"Sorrento F41L\"}\n{\"acct_num\":97620,\"dev_id\":\"16512c4a-b3d3-4f92-97b7-80f9df6a487e\",\"phone\":\"7072821950\",\"model\":\"Sorrento F24L\"}\n{\"acct_num\":74940,\"dev_id\":\"361cbf31-cc6c-4233-ad4e-28d10ab09568\",\"phone\":\"5623783063\",\"model\":\"MeeToo 3.0\"}\n{\"acct_num\":125327,\"dev_id\":\"6db1bbf9-3903-47f8-b150-388dcfd8d0d3\",\"phone\":\"9168710424\",\"model\":\"Titanic 2000\"}\n{\"acct_num\":32782,\"dev_id\":\"e97f4769-13f7-4a28-9843-827f49e767dd\",\"phone\":\"8056881980\",\"model\":\"Sorrento F41L\"}\n{\"acct_num\":94440,\"dev_id\":\"1512a38e-721b-4712-afd4-79fd94ec2b56\",\"phone\":\"9513506516\",\"model\":\"MeeToo 2.0\"}\n{\"acct_num\":4337,\"dev_id\":\"6f495bc4-d6a9-43bb-90fc-650cbd6da073\",\"phone\":\"8056442972\",\"model\":\"Titanic 2200\"}\n{\"acct_num\":28646,\"dev_id\":\"eee8add4-1740-4a39-9f23-c9ee8b4e04ac\",\"phone\":\"5039316268\",\"model\":\"Titanic 2000\"}\n{\"acct_num\":44835,\"dev_id\":\"aed40d19-d5fc-4e11-a04a-f68094b38d73\",\"phone\":\"9289444545\",\"model\":\"Sorrento F41L\"}\n{\"acct_num\":121026,\"dev_id\":\"e67cd2c4-7358-4796-81ea-bc8a7bf91212\",\"phone\":\"5419395477\",\"model\":\"Sorrento F32L\"}\n"}]},"apps":[],"jobName":"paragraph_1591316154992_1837986041","id":"20200429-213439_92529216","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230023"},{"title":"2 - Open a terminal and set up a directory to contain the data files that Spark will read","text":"%md\n```\nmkdir -p /tmp/telco-streaming\nchmod +wr /tmp/telco-streaming\n```","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1591316154993_-995740357","id":"20200429-213508_983964336","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230024"},{"title":"3 - Prepare a shell to run a streaming application","text":"%md\n```\npyspark --master local[2]\n```","user":"sysadmin","dateUpdated":"2020-06-05T00:19:33+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154994_1636474347","id":"20200429-213505_44445732","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230025"},{"title":"4 - Define a schema for the structure of the input data","text":"%md\n```\ninputDir = \"file:/tmp/telco-streaming/\"\n\nfrom pyspark.sql.types import *\nactivationsSchema = StructType([ \n  StructField(\"acct_num\", IntegerType()),\n  StructField(\"dev_id\", StringType()),\n  StructField(\"phone\", StringType()),\n  StructField(\"model\", StringType())])\n```","user":"sysadmin","dateUpdated":"2020-06-05T00:19:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154994_-1148928110","id":"20200601-192211_252144301","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230026"},{"title":"5 - Create a streaming DataFrame by reading the data you reviewed above","text":"%md\n```\nactivationsDF = spark.readStream.schema(activationsSchema).json(inputDir)\n```","user":"sysadmin","dateUpdated":"2020-06-05T00:20:27+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154995_1572408104","id":"20200429-213501_2000175821","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230027"},{"title":"6 - Display the streaming DataFrame's schema to confirm that it is set up correctly","text":"%md\n```\nactivationsDF.printSchema()\n```","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154995_-859318414","id":"20200429-213459_1552306503","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230028"},{"title":"7 - Confirm that the DataFrame's isStreaming property is set","text":"%md\n*** ??? Inspect the DataFrame's schema? ??? ***","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154996_1178907354","id":"20200429-213458_677198705","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230029"},{"title":"8 - Start a streaming query","text":"%md\n```\nactivationsQuery = activationsDF.writeStream.outputMode(\"append\").option(\"truncate\",\"false\").format(\"console\").start()\n```","user":"sysadmin","dateUpdated":"2020-06-05T00:20:42+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154997_1210700243","id":"20200429-213458_820871973","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230030"},{"title":"9 - Begin streaming data","text":"%md\n```\n/home/devuser/bin/devsh/scripts/streamtest-file.sh /home/devuser/data/telco/activations_stream/ /tmp/telco-streaming\n```","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154997_-1489984748","id":"20200429-213456_1259144389","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230031"},{"title":"10 - Confirm that the query is displaying data","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154998_989825939","id":"20200429-213456_813615217","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230032"},{"title":"11 - Stopping the stream","text":"%md\n```\nactivationsQuery.stop()\n```","user":"sysadmin","dateUpdated":"2020-06-05T00:20:48+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154998_810387774","id":"20200429-213455_307289701","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230033"},{"title":"12 - Stopping the test script","text":"%md\nTerminate the test script in the second terminal window using `Ctrl+C`.","user":"sysadmin","dateUpdated":"2020-06-05T00:15:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316154999_1845208712","id":"20200429-213454_100119688","dateCreated":"2020-06-05T00:15:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230034"},{"text":"%md\n### Display Aggregated Streaming Data to the Console","user":"sysadmin","dateUpdated":"2020-06-05T00:15:55+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316155000_1481452983","id":"20200429-213454_1477948613","dateCreated":"2020-06-05T00:15:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230035"},{"title":"13 - Clean the streaming directory","text":"%md\n```\nrm -rf /tmp/telco-streaming/*\n```","user":"sysadmin","dateUpdated":"2020-06-05T00:15:55+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316155000_737818270","id":"20200429-213454_63154555","dateCreated":"2020-06-05T00:15:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230036"},{"title":"14 - Create a DataFrame to count each device model","text":"%md\n```\nactivationCountDF = activationsDF.groupBy(\"model\").count()\n```","user":"sysadmin","dateUpdated":"2020-06-05T00:21:09+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316155001_-560977254","id":"20200429-213453_573955489","dateCreated":"2020-06-05T00:15:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230037"},{"title":"15 - Start a streaming query that aggregates the data","text":"%md\n```\nactivationCountQuery = activationCountDF.writeStream.outputMode(\"complete\").format(\"console\").start()\n```","user":"sysadmin","dateUpdated":"2020-06-05T00:21:15+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316155001_1314505416","id":"20200429-213452_810770797","dateCreated":"2020-06-05T00:15:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230038"},{"title":"16 - Run the streamtest-file.sh script again","user":"sysadmin","dateUpdated":"2020-06-05T00:15:55+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316155002_-672008626","id":"20200429-213450_1359119850","dateCreated":"2020-06-05T00:15:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230039"},{"title":"17 - Confirm that complete output mode is activated","user":"sysadmin","dateUpdated":"2020-06-05T00:15:55+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316155003_-622509778","id":"20200429-213450_472997628","dateCreated":"2020-06-05T00:15:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230040"},{"title":"18 - Stop the query and terminate the test script","text":"%md\n```\nactivationCountQuery.stop()\n```\n\n```\nCtrl+C\n```","user":"sysadmin","dateUpdated":"2020-06-05T00:21:17+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316155004_1165237303","id":"20200429-213450_463682321","dateCreated":"2020-06-05T00:15:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230041"},{"text":"%md\n### Output Data to Files","user":"sysadmin","dateUpdated":"2020-06-05T00:15:55+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316155005_542251426","id":"20200429-213449_842091323","dateCreated":"2020-06-05T00:15:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230042"},{"title":"19 - Clean the streaming directory","text":"%md\n```\nrm -rf /tmp/telco-streaming/*\n````","user":"sysadmin","dateUpdated":"2020-06-05T00:15:55+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316155006_-1716694877","id":"20200429-213449_1526293893","dateCreated":"2020-06-05T00:15:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230043"},{"title":"20 - Create a new streaming DataFrame","text":"%md\n```\ntitanic1000DF = activationsDF.where(\"model = 'Titanic 1000'\").select(\"dev_id\",\"acct_num\")\ntitanic1000DF.printSchema()\n```","user":"sysadmin","dateUpdated":"2020-06-05T00:21:31+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316155006_-718301811","id":"20200429-213448_1192486338","dateCreated":"2020-06-05T00:15:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230044"},{"title":"21 - Start a query that saves to HDFS","text":"%md\n```\nspark.conf.set(\"spark.sql.streaming.checkpointLocation\", \"/tmp/streaming-checkpoint\")\n```\n\n```\ntitanic1000Query = titanic1000DF.writeStream.trigger(processingTime=\"3 seconds\").outputMode(\"append\").format(\"csv\").option(\"path\",\"/user/zeppelin/titanic1000/\").start()\n```","user":"sysadmin","dateUpdated":"2020-06-05T00:22:11+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316155007_-294611439","id":"20200429-213447_402657581","dateCreated":"2020-06-05T00:15:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230045"},{"title":"22 - Re-run the stream test script","user":"sysadmin","dateUpdated":"2020-06-05T00:15:55+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316155008_-566298446","id":"20200429-213445_1660540485","dateCreated":"2020-06-05T00:15:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230046"},{"title":"23 - Confirm the target directory is receiving the files","user":"sysadmin","dateUpdated":"2020-06-05T00:15:55+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316155008_1546803023","id":"20200429-213445_371430499","dateCreated":"2020-06-05T00:15:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230047"},{"title":"24 - Stopping the test","text":"%md\n```\ntitanic1000Query.stop()\n```\n\n```\nCtrl+C\n```","user":"sysadmin","dateUpdated":"2020-06-05T00:22:14+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591316155009_209528490","id":"20200429-213445_2052407867","dateCreated":"2020-06-05T00:15:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230048"},{"text":"%md\nWe hope you've enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Hortonworks Apache Spark Tutorials](http://hortonworks.com/tutorials/#tuts-developers) are your natural next step where you can explore Spark in more depth.\n2. [Hortonworks Community Connection (HCC)](https://community.hortonworks.com/spaces/85/data-science.html?type=question) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Hortonworks Apache Spark Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html) - official Spark documentation.\n4. [Hortonworks Apache Zeppelin Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html) - official Zeppelin documentation.\n","user":"sysadmin","dateUpdated":"2020-06-05T00:15:55+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We hope you've enjoyed this lab. Below are additional resources that you should find useful:</p>\n<ol>\n<li><a href=\"http://hortonworks.com/tutorials/#tuts-developers\">Hortonworks Apache Spark Tutorials</a> are your natural next step where you can explore Spark in more depth.</li>\n<li><a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\">Hortonworks Community Connection (HCC)</a> is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html\">Hortonworks Apache Spark Docs</a> - official Spark documentation.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html\">Hortonworks Apache Zeppelin Docs</a> - official Zeppelin documentation.</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1591316155009_-641467928","id":"20181126-133017_244739700","dateCreated":"2020-06-05T00:15:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230049"}],"name":"PYSPARK/16-ProcessingStreamingData","id":"2FB18RDRH","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}