{"paragraphs":[{"text":"%md\n# About\n**Lab:** Exploring DataFrames Using the Apache Spark Shell\n**Objective:** You will use the Spark shell to work with DataFrames\n**File locations:** \n```\n/home/devuser/data/telco/devices.json\n```\n\n**Successful outcome:** You will be able to successfully use the Spark shell to work with DataFrames.\n**Before you begin:** Complete Lab: Working with HDFS\n**Related lessons:**  \n\n---","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>About</h1>\n<p><strong>Lab:</strong> Exploring DataFrames Using the Apache Spark Shell\n<br  /><strong>Objective:</strong> You will use the Spark shell to work with DataFrames\n<br  /><strong>File locations:</strong></p>\n<pre><code>$DEVDATA/devices.json\n</code></pre>\n<p><strong>Successful outcome:</strong> You will be able to successfully use the Spark shell to work with DataFrames.\n<br  /><strong>Before you begin:</strong> Complete Lab: Working with HDFS\n<br  /><strong>Related lessons:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591142879900_-681526398","id":"20181126-092644_1457476546","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:254065"},{"text":"%md\n# Setup","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Setup</h1>\n"}]},"apps":[],"jobName":"paragraph_1591142879903_902736514","id":"20181201-044336_178705192","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254066"},{"title":"","text":"%md\nYou will start by viewing and bookmarking the Spark documentation in your browser.\nThen you will start the Spark shell and read a simple JSON file into a DataFrame.\n\n**Important:** This exercise assumes you are comfortable working with HDFS. You can practice your skills by completing exercise `03 - Working with HDFS`.","user":"sysadmin","dateUpdated":"2020-06-03T00:08:46+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>You will start by viewing and bookmarking the Spark documentation in your browser.\n<br  />Then you will start the Spark shell and read a simple JSON file into a DataFrame.</p>\n<p><strong>Important:</strong> This exercise depends on <a href=\"http://client02.tantor.net:9995/#/notebook/2F8BDP46X\">Hands-On Exercise: Working with HDFS</a>.\n<br  /><strong><em> *** Does this link work? *** </em></strong>\n<br  />If you did not complete that exercise, run the course catch-up script and advance to the current exercise:</p>\n<pre><code>$ $DEVSH/scripts/catchup.sh\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591142879903_656412201","id":"20191112-223734_624990466","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254067"},{"text":"%md\n# Lab\n","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Lab</h1>\n"}]},"apps":[],"jobName":"paragraph_1591142879904_1263885740","id":"20181126-093358_358613711","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254068"},{"text":"%md\n### View the Spark Documentation","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>View the Spark Documentation</h3>\n"}]},"apps":[],"jobName":"paragraph_1591142879904_1548448650","id":"20200427-224625_7331506","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254069"},{"title":"1 - Open the Spark Documentation","text":"%md\nView the Spark documentation in your web browser by visiting [http://spark.apache.org/docs/2.4.0/](http://spark/apache.org/docs/2.4.0/).","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>View the Spark documentation in your web browser by visiting <a href=\"http://spark/apache.org/docs/2.4.0/\">http://spark.apache.org/docs/2.4.0/</a>.</p>\n"}]},"apps":[],"jobName":"paragraph_1591142879904_2116197612","id":"20191112-224241_1478925112","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254070"},{"title":"2 - Review the guide for SQL, DataFrames, and Datasets","text":"%md\nFrom the **Programming Guides** menu, select **SQL, DataFrames, and Datasets**. Briefly review the guide and bookmark the page for later reference.","user":"sysadmin","dateUpdated":"2020-06-03T00:13:38+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>From the <strong>Programming Guides</strong> menu, select <strong>SQL, DataFrames, and Datasets</strong>. Briefly review the guide and bookmark the page for later reference.</p>\n"}]},"apps":[],"jobName":"paragraph_1591142879904_310823505","id":"20200424-001307_182214646","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254071"},{"title":"3 - Review the API documentation","text":"%md\nFrom the API Docs menu, select Scala. Bookmark the API page for use during class. Later exercises will refer to this documentation","user":"sysadmin","dateUpdated":"2020-06-03T00:12:58+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"table","height":59,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>From the API Docs menu, select either Scala or Python, depending on your\n<br  />language preference. Bookmark the API page for use during class. Later exercises\n<br  />will refer to this documentation</p>\n"}]},"apps":[],"jobName":"paragraph_1591142879905_-1597718666","id":"20200424-001814_1198814570","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254072"},{"title":"4 - Review the SQL package","text":"%md\nNotice that the package names are displayed on the left. Use the search box or scroll down to find the `org.apache.spark.sql` package. This package contains most of the classes and objects you will be working with in this course. In particular, note the Dataset class. Although this exercise focuses on DataFrames, remember that DataFrames are simply an alias for Datasets of Row objects. So all the DataFrame operations you will practice using in this exercise are documented on the Dataset class.","user":"sysadmin","dateUpdated":"2020-06-03T00:14:33+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>If you are viewing the Python API, locate the <code>pyspark.sql</code> module. This module\n<br  />contains most of the classes you will be working with in this course. At the top are\n<br  />some of the key classes in the module. View the API for the DataFrame class; these\n<br  />are the operations you will practice using in this exercise.</p>\n"}]},"apps":[],"jobName":"paragraph_1591142879905_836622241","id":"20200424-002121_1245830488","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254073"},{"text":"%md\n### Start the Spark Shell","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Start the Spark Shell</h3>\n"}]},"apps":[],"jobName":"paragraph_1591142879905_1671422276","id":"20200521-184112_1302709995","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254074"},{"title":"5 - Starting the Spark shell","text":"%md\n**Note:** In this environment a Spark shell is started by the `spark2` interpreter so you do not need to manually start one. In a normal terminal, however, you can start the Spark shell for Scala by using the `spark-shell` command.","user":"sysadmin","dateUpdated":"2020-06-03T00:19:23+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591142879905_-1843859154","id":"20200521-191625_1692551877","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254075"},{"title":"6 - Confirm that a SparkSession has been instantiated","text":"%md\nSpark creates a `SparkSession` object for you called `spark`. Make sure the object exists. Use the command below to display information about the spark object such as: `org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@41bb5b21`","user":"sysadmin","dateUpdated":"2020-06-03T00:26:11+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Spark creates a <code>SparkSession</code> object for you called <code>spark</code>. Make sure the object\n<br  />exists. Use the first command below if you are using Python, and the second one if\n<br  />you are using Scala. (You only need to complete the exercises in Python <em>or</em> Scala.)</p>\n<p>Python will display information about the spark object such as:\n<br  /><code>&lt;pyspark.sql.session.SparkSession at</code> <em>address</em><code>&gt;</code>\n<br  />Scala will display similar information in a different format:\n<br  /><code>org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@</code><em>address</em></p>\n<p><strong>Note:</strong> In subsequent instructions, both Python and Scala commands will be shown\n<br  />but not noted explicitly; Python shell commands are in <strong>blue</strong> and preceded with\n<br  /><code>pyspark&gt;</code>, and Scala shell commands are in <strong>red</strong> and preceded with <code>scala&gt;</code>.</p>\n<p><strong><em> !!!! Needs an edit, refers to colored commands which don't exist in Zeppelin !!! </em></strong>*</p>\n"}]},"apps":[],"jobName":"paragraph_1591142879906_-894816896","id":"20200424-004925_515825214","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254076"},{"text":"%spark2\nspark","user":"sysadmin","dateUpdated":"2020-06-03T00:25:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res1: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@41bb5b21\n"}]},"apps":[],"jobName":"paragraph_1591142879906_545825014","id":"20200424-013758_201411447","dateCreated":"2020-06-03T00:07:59+0000","dateStarted":"2020-06-03T00:25:34+0000","dateFinished":"2020-06-03T00:25:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:254077"},{"title":"7 - View available Spark session methods","text":"%md\nUsing command completion, you can see all the available Spark session methods: type `spark.` (spark followed by a dot) and then the `TAB` key.\n\n**Note:** To exit the Scala Spark shell, type`sys.exit`. However, you won't need to terminate the shell in this environment.","user":"sysadmin","dateUpdated":"2020-06-03T00:26:59+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Using command completion, you can see all the available Spark session methods:\n<br  />type <code>spark.</code> (spark followed by a dot) and then the <code>TAB</code> key.</p>\n<p><strong>Note:</strong> You can exit the Scala shell by typing <code>sys.exit</code>. To exit the Python shell,\n<br  />press <code>Ctrl+D</code> or type <code>exit</code>. However, stay in the shell for now to complete the\n<br  />remainder of this exercise.</p>\n"}]},"apps":[],"jobName":"paragraph_1591142879906_701874891","id":"20200424-010436_1751918744","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254078"},{"text":"%spark2\n//spark.","user":"sysadmin","dateUpdated":"2020-06-03T00:32:11+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591142879906_-44492712","id":"20200524-190546_1424495401","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254079"},{"text":"%md\n### Read and Display a JSON File","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Read and Display a JSON File</h3>\n"}]},"apps":[],"jobName":"paragraph_1591142879907_2039550988","id":"20200427-230624_139432148","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254080"},{"title":"8 - Review the data file you will be using","text":"%md\nReview the simple text file you will be using: `/home/devuser/data/telco/devices.json`. You can view the file by  in an editor, or by starting a new terminal window then using the `less` command. (Do not modify the file.) This file contains records for each of the supported devices. For example:\n\n```\n    {\"devnum\":1,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\", \"make\":\"Sorrento\",\"model\":\"F00L\",\"dev_type\":\"phone\"}\n```\n\nNotice the field names and types of values in the first few records.","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Review the simple text file you will be using: <code>$DEVDATA/devices.json</code>. You can\n<br  />view the file either in the Pluma editor, or by starting a new terminal window then\n<br  />using the <code>less</code> command. (Do not modify the file.) This file contains records for\n<br  />each of Loudacre's supported devices. For example:</p>\n<pre><code>{\"devnum\":1,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\", \"make\":\"Sorrento\",\"model\":\"F00L\",\"dev_type\":\"phone\"}\n</code></pre>\n<p>Notice the field names and types of values in the first few records.</p>\n"}]},"apps":[],"jobName":"paragraph_1591142879907_808139438","id":"20200424-010839_474210908","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254081"},{"title":"9 - Upload the data file to HDFS","text":"%md\nUpload the data file to the /user/zeppelin/ directory in HDFS.","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Upload the data file to the /devsh_loudacre directory in HDFS:</p>\n<pre><code>$ hdfs dfs -put $DEVDATA/devices.json /devsh_loudacre/\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591142879907_1413748469","id":"20200424-011220_1523805356","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254082"},{"text":"%sh\nhdfs dfs -put /home/devuser/data/telco/devices.json /user/zeppelin/\nhdfs dfs -ls /user/zeppelin/","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 7 items\ndrwx------   - zeppelin hdfs          0 2020-05-21 12:02 /user/zeppelin/.Trash\ndrwxr-xr-x   - zeppelin hdfs          0 2020-05-21 17:42 /user/zeppelin/.sparkStaging\ndrwxrwxrwx   - zeppelin hdfs          0 2020-05-21 17:38 /user/zeppelin/conf\n-rw-r--r--   3 zeppelin hdfs       5483 2020-05-21 18:32 /user/zeppelin/devices.json\ndrwxr-xr-x   - zeppelin hdfs          0 2020-05-21 05:55 /user/zeppelin/kb\ndrwxrwxrwx   - zeppelin hdfs          0 2020-05-21 17:38 /user/zeppelin/notebook\ndrwxrwxrwx   - zeppelin hdfs          0 2020-04-29 00:53 /user/zeppelin/test\n"}]},"apps":[],"jobName":"paragraph_1591142879907_-2033510983","id":"20200430-003404_594738016","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254083"},{"title":"10 - Create a DataFrame based on the devices.json file","text":"%md\nIn the Spark shell, create a new DataFrame based on the devices.json file in HDFS.","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>In the Spark shell, create a new DataFrame based on the devices.json file in\n<br  />HDFS.</p>\n"}]},"apps":[],"jobName":"paragraph_1591142879908_940435936","id":"20200424-011308_1103327096","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254084"},{"text":"%spark2\nval devDF = spark.read.json(\"/user/zeppelin/devices.json\")","user":"sysadmin","dateUpdated":"2020-06-03T00:32:41+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1591142879908_2093250913","id":"20200424-032459_2069854050","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254085"},{"title":"11 - View the schema of the DataFrame","text":"%md\nSpark has not yet read the data in the file, but it has scanned the file to infer the schema. View the schema, and note that the column names match the record field names in the JSON file.","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591142879908_-694403122","id":"20200424-032825_299685384","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254086"},{"text":"%spark2\ndevDF.printSchema","user":"sysadmin","dateUpdated":"2020-06-03T00:32:59+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- dev_type: string (nullable = true)\n |-- devnum: long (nullable = true)\n |-- make: string (nullable = true)\n |-- model: string (nullable = true)\n |-- release_dt: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1591142879908_1054941975","id":"20200424-032841_1009875752","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254087"},{"title":"12 - Display the data using the show action","text":"%md\nDisplay the data in the DataFrame using the show function. If you don't pass an argument to `show`, Spark will display the first 20 rows in the DataFrame. For this step, display the first five rows. Note that the data is displayed in tabular form, using the column names defined in the schema.","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Display the data in the DataFrame using the show function. If you don't pass an\n<br  />argument to <code>show</code>, Spark will display the first 20 rows in the DataFrame. For this\n<br  />step, display the first five rows. Note that the data is displayed in tabular form,\n<br  />using the column names defined in the schema.</p>\n<p><strong>Note:</strong> Like many Spark queries, this command is the same whether you are using\n<br  />Scala or Python.</p>\n"}]},"apps":[],"jobName":"paragraph_1591142879909_902061923","id":"20200424-011941_1495209860","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254088"},{"title":"","text":"%spark2\ndevDF.show(5)","user":"sysadmin","dateUpdated":"2020-06-03T00:33:37+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591142879909_-1312341875","id":"20200424-033005_1468690942","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254089"},{"title":"13 - Access the data using the take action","text":"%md\nThe `show` and `printSchema` operations are actions--that is, they return a value from the distributed DataFrame to the Spark driver. Both functions display the data in a nicely formatted table. These functions are intended for interactive use in the shell, but do not allow you actually work with the data that is returned. Try using the `take` action instead, which returns a list of Row objects. You can display the data by iterating through the collection.","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The <code>show</code> and <code>printSchema</code> operations are actions&ndash;that is, they return a value\n<br  />from the distributed DataFrame to the Spark driver. Both functions display the data\n<br  />in a nicely formatted table. These functions are intended for interactive use in the\n<br  />shell, but do not allow you actually work with the data that is returned. Try using\n<br  />the <code>take</code> action instead, which returns an array (Scala) or list (Python) of Row\n<br  />objects. You can display the data by iterating through the collection.</p>\n"}]},"apps":[],"jobName":"paragraph_1591142879909_-788617131","id":"20200424-012052_1840734182","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254090"},{"text":"%spark2\nval rows = devDF.take(5)\nrows.foreach(println)","user":"sysadmin","dateUpdated":"2020-06-03T00:34:27+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Row(dev_type=u'phone', devnum=1, make=u'Sorrento', model=u'F00L', release_dt=u'2008-10-21T00:00:00.000-07:00')\nRow(dev_type=u'phone', devnum=2, make=u'Titanic', model=u'2100', release_dt=u'2010-04-19T00:00:00.000-07:00')\nRow(dev_type=u'phone', devnum=3, make=u'MeeToo', model=u'3.0', release_dt=u'2011-02-18T00:00:00.000-08:00')\nRow(dev_type=u'phone', devnum=4, make=u'MeeToo', model=u'3.1', release_dt=u'2011-09-21T00:00:00.000-07:00')\nRow(dev_type=u'phone', devnum=5, make=u'iFruit', model=u'1', release_dt=u'2008-10-21T00:00:00.000-07:00')\n"}]},"apps":[],"jobName":"paragraph_1591142879909_-213980014","id":"20200424-033258_489855410","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254091"},{"text":"%md\n### Query a DataFrame","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Query a DataFrame</h3>\n"}]},"apps":[],"jobName":"paragraph_1591142879910_554169707","id":"20200427-231428_316380624","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254092"},{"title":"14 - Count the number of items in the DataFrame","text":"%md\nUse the `count` action to return the number of items in the DataFrame.","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Use the <code>count</code> action to return the number of items in the DataFrame.</p>\n"}]},"apps":[],"jobName":"paragraph_1591142879910_909091086","id":"20200424-012559_815782442","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254093"},{"text":"%spark2\ndevDF.count()","user":"sysadmin","dateUpdated":"2020-06-03T00:34:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"50\n"}]},"apps":[],"jobName":"paragraph_1591142879910_1316445600","id":"20200424-033808_1012166995","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254094"},{"title":"15 - Use the select transformation","text":"%md\nDataFrame transformations typically return another DataFrame. Try using a `select` transformation to return a DataFrame with only the `make` and `model` columns, then display its schema. Note that only the selected columns are in the schema.","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>DataFrame transformations typically return another DataFrame. Try using a\n<br  /><code>select</code> transformation to return a DataFrame with only the <code>make</code> and <code>model</code>\n<br  />columns, then display its schema. Note that only the selected columns are in the\n<br  />schema.</p>\n<pre><code>pyspark&gt; makeModelDF = devDF.select(\"make\",\"model\")\npyspark&gt; makeModelDF.printSchema()\n</code></pre>\n<!-- -->\n<pre><code>scala&gt; val makeModelDF = devDF.select(\"make\",\"model\")\nscala&gt; makeModelDF.printSchema\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591142879910_397817001","id":"20200424-012718_762015263","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254095"},{"text":"%spark2\nval makeModelDF = devDF.select(\"make\",\"model\")\nmakeModelDF.printSchema","user":"sysadmin","dateUpdated":"2020-06-03T00:35:00+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591142879911_-57569186","id":"20200424-033915_1498601100","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254096"},{"title":"16 - Execute the query by calling an action operation","text":"%md\nA query is a series of one or more transformations followed by an action. Spark does not execute the query until you call the action operation. Display the first 20 lines of the final DataFrame in the series using the `show` action.","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>A query is a series of one or more transformations followed by an action. Spark\n<br  />does not execute the query until you call the action operation. Display the first 20\n<br  />lines of the final DataFrame in the series using the <code>show</code> action.</p>\n"}]},"apps":[],"jobName":"paragraph_1591142879911_74771236","id":"20200424-012844_1710957544","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254097"},{"text":"%spark2\nmakeModelDF.show","user":"sysadmin","dateUpdated":"2020-06-03T00:35:12+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591142879911_-277072691","id":"20200424-034057_221622157","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254098"},{"title":"17 - Chain multiple transformations in a single command","text":"%md\nTransformations in a query can be chained together. Execute a single command to show the results of a query using select and where. The resulting DataFrame will contain only the columns `devnum`, `make`, and `model`, and only the rows where the make is `Ronin`.","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Transformations in a query can be chained together. Execute a single command to\n<br  />show the results of a query using select and where. The resulting DataFrame will\n<br  />contain only the columns <code>devnum</code>, <code>make</code>, and <code>model</code>, and only the rows where the\n<br  />make is <code>Ronin</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1591142879911_-1030428213","id":"20200424-013213_1983526383","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254099"},{"text":"%pyspark\ndevDF.select(\"devnum\",\"make\",\"model\").\nwhere(\"make = 'Ronin'\").\nshow","user":"sysadmin","dateUpdated":"2020-06-03T00:35:31+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+-----+--------------+\n|devnum| make|         model|\n+------+-----+--------------+\n|    15|Ronin|Novelty Note 1|\n|    17|Ronin|Novelty Note 3|\n|    18|Ronin|Novelty Note 2|\n|    19|Ronin|Novelty Note 4|\n|    46|Ronin|            S4|\n|    47|Ronin|            S1|\n|    48|Ronin|            S3|\n|    49|Ronin|            S2|\n+------+-----+--------------+\n\n"}]},"apps":[],"jobName":"paragraph_1591142879912_1029623130","id":"20200424-034245_1247017497","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254100"},{"text":"%md\n# Result\n**You have now:** You have now successfully used the Spark shell to work with DataFrames.\n\n---","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Result</h1>\n<p><strong>You have now:</strong> You have now successfully used the Spark shell to work with DataFrames.</p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591142879912_-251627914","id":"20181126-133507_1472573213","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254101"},{"text":"%md\n# Solution\n---","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Solution</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591142879912_-1883124916","id":"20181018-125200_1133281582","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254102"},{"text":"%md\n### View the Spark Documentation","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>View the Spark Documentation</h3>\n"}]},"apps":[],"jobName":"paragraph_1591142879912_1344461280","id":"20200428-222440_1560118113","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254103"},{"title":"1 - Open the Spark Documentation","text":"","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>View the Spark documentation in your web browser by visiting <a href=\"http://spark/apache.org/docs/2.4.0/\">http://spark.apache.org/docs/2.4.0/</a>.</p>\n"}]},"apps":[],"jobName":"paragraph_1591142879913_1962451384","id":"20200428-220621_941982366","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254104"},{"title":"2 - Review the guide for SQL, DataFrames, and Datasets","text":"","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591142879913_2015430364","id":"20200428-221224_367935","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254105"},{"title":"3 - Review the API documentation","text":"","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"table","height":59,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591142879913_-1860237581","id":"20200428-221344_80691113","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254106"},{"title":"4 - Review the SQL module for Python","text":"","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591142879913_-1437965020","id":"20200428-221502_793143172","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254107"},{"text":"%md ### Start the Spark Shell","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Start the Spark Shell</h3>\n"}]},"apps":[],"jobName":"paragraph_1591142879914_-1042629810","id":"20200428-222353_254420897","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254108"},{"title":"5 - Starting the Spark shell","text":"","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591142879914_1322681759","id":"20200521-192922_1675306584","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254109"},{"title":"6 - Confirm that a SparkSession has been instantiated","text":"%spark2\nspark","user":"sysadmin","dateUpdated":"2020-06-03T00:35:36+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"<pyspark.sql.session.SparkSession object at 0x7fba79d47ed0>\n"}]},"apps":[],"jobName":"paragraph_1591142879914_1111210621","id":"20200428-221636_1873208691","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254110"},{"title":"7  - View available Spark session methods","text":"%spark2\n//spark.","user":"sysadmin","dateUpdated":"2020-06-03T00:35:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591142879914_-1086754826","id":"20200428-223059_378364109","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254111"},{"text":"%md\n### Read and Display a JSON File","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Read and Display a JSON File</h3>\n"}]},"apps":[],"jobName":"paragraph_1591142879915_453841521","id":"20200428-223102_811862238","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254112"},{"title":"8 - Review the data file you will be using","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591142879915_419373701","id":"20200428-223221_352134750","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254113"},{"title":"9 - Upload the data file to HDFS","text":"%sh\nhdfs dfs -put /home/devuser/data/telco/devices.json /user/zeppelin/\nhdfs dfs -ls /user/zeppelin/","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 5 items\ndrwx------   - zeppelin hdfs          0 2020-05-26 01:19 /user/zeppelin/.Trash\ndrwxr-xr-x   - zeppelin hdfs          0 2020-05-26 01:22 /user/zeppelin/.sparkStaging\n-rw-r--r--   3 zeppelin hdfs       5483 2020-05-26 03:38 /user/zeppelin/devices.json\ndrwxr-xr-x   - zeppelin hdfs          0 2020-05-26 01:21 /user/zeppelin/kb\ndrwxrwxrwx   - zeppelin hdfs          0 2020-05-26 03:12 /user/zeppelin/notebook\n"}]},"apps":[],"jobName":"paragraph_1591142879915_-859706707","id":"20200428-223236_87217881","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254114"},{"title":"10 - Create a DataFrame based on the devices.json file","text":"%spark2\nval devDF = spark.read.json(\"/user/zeppelin/devices.json\")","user":"sysadmin","dateUpdated":"2020-06-03T00:35:54+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1591142879915_1871612165","id":"20200428-223351_1447447811","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254115"},{"title":"11 - View the schema of the DataFrame","text":"%spark2\ndevDF.printSchema","user":"sysadmin","dateUpdated":"2020-06-03T00:36:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- dev_type: string (nullable = true)\n |-- devnum: long (nullable = true)\n |-- make: string (nullable = true)\n |-- model: string (nullable = true)\n |-- release_dt: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1591142879916_1589630502","id":"20200428-223520_174824961","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254116"},{"title":"12 - Display the data using the show action","text":"%spark2\ndevDF.show(5)","user":"sysadmin","dateUpdated":"2020-06-03T00:36:12+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------+------+--------+-----+--------------------+\n|dev_type|devnum|    make|model|          release_dt|\n+--------+------+--------+-----+--------------------+\n|   phone|     1|Sorrento| F00L|2008-10-21T00:00:...|\n|   phone|     2| Titanic| 2100|2010-04-19T00:00:...|\n|   phone|     3|  MeeToo|  3.0|2011-02-18T00:00:...|\n|   phone|     4|  MeeToo|  3.1|2011-09-21T00:00:...|\n|   phone|     5|  iFruit|    1|2008-10-21T00:00:...|\n+--------+------+--------+-----+--------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1591142879916_-415689164","id":"20200428-223618_1010809721","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254117"},{"title":"13 - Access the data using the take action","text":"%spark2\nval rows = devDF.take(5)\nrows.foreach(println)","user":"sysadmin","dateUpdated":"2020-06-03T00:36:31+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Row(dev_type=u'phone', devnum=1, make=u'Sorrento', model=u'F00L', release_dt=u'2008-10-21T00:00:00.000-07:00')\nRow(dev_type=u'phone', devnum=2, make=u'Titanic', model=u'2100', release_dt=u'2010-04-19T00:00:00.000-07:00')\nRow(dev_type=u'phone', devnum=3, make=u'MeeToo', model=u'3.0', release_dt=u'2011-02-18T00:00:00.000-08:00')\nRow(dev_type=u'phone', devnum=4, make=u'MeeToo', model=u'3.1', release_dt=u'2011-09-21T00:00:00.000-07:00')\nRow(dev_type=u'phone', devnum=5, make=u'iFruit', model=u'1', release_dt=u'2008-10-21T00:00:00.000-07:00')\n"}]},"apps":[],"jobName":"paragraph_1591142879916_-301433703","id":"20200428-223654_814080359","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254118"},{"text":"%md\n### Query a DataFrame","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Query a DataFrame</h3>\n"}]},"apps":[],"jobName":"paragraph_1591142879916_658470687","id":"20200428-223737_479505684","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254119"},{"title":"14 - Count the number of items in the DataFrame","text":"%spark2\ndevDF.count()","user":"sysadmin","dateUpdated":"2020-06-03T00:36:41+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"50\n"}]},"apps":[],"jobName":"paragraph_1591142879917_1626922850","id":"20200428-223808_1051562813","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254120"},{"title":"15 - Use the select transformation","text":"%spark2\nval makeModelDF = devDF.select(\"make\",\"model\")\nmakeModelDF.printSchema","user":"sysadmin","dateUpdated":"2020-06-03T00:36:58+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- make: string (nullable = true)\n |-- model: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1591142879917_1417536327","id":"20200428-223831_1189921640","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254121"},{"title":"16 - Execute the query by calling an action operation","text":"%spark2\nmakeModelDF.show","user":"sysadmin","dateUpdated":"2020-06-03T02:29:08+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------+--------------+\n|    make|         model|\n+--------+--------------+\n|Sorrento|          F00L|\n| Titanic|          2100|\n|  MeeToo|           3.0|\n|  MeeToo|           3.1|\n|  iFruit|             1|\n|  iFruit|             3|\n|  iFruit|             2|\n|  iFruit|             5|\n| Titanic|          1000|\n|  MeeToo|           1.0|\n|Sorrento|          F21L|\n|  iFruit|             4|\n|Sorrento|          F23L|\n| Titanic|          2200|\n|   Ronin|Novelty Note 1|\n| Titanic|          2500|\n|   Ronin|Novelty Note 3|\n|   Ronin|Novelty Note 2|\n|   Ronin|Novelty Note 4|\n|  iFruit|            3A|\n+--------+--------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1591142879917_-207618125","id":"20200428-223941_404796421","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254122"},{"title":"17 - Chain multiple transformations in a single command","text":"%spark2\ndevDF.select(\"devnum\",\"make\",\"model\").where(\"make = 'Ronin'\").show","user":"sysadmin","dateUpdated":"2020-06-03T02:29:21+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+-----+--------------+\n|devnum| make|         model|\n+------+-----+--------------+\n|    15|Ronin|Novelty Note 1|\n|    17|Ronin|Novelty Note 3|\n|    18|Ronin|Novelty Note 2|\n|    19|Ronin|Novelty Note 4|\n|    46|Ronin|            S4|\n|    47|Ronin|            S1|\n|    48|Ronin|            S3|\n|    49|Ronin|            S2|\n+------+-----+--------------+\n\n"}]},"apps":[],"jobName":"paragraph_1591142879917_-530786410","id":"20200428-224456_1487912743","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254123"},{"text":"%md\nWe hope you've enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Hortonworks Apache Spark Tutorials](http://hortonworks.com/tutorials/#tuts-developers) are your natural next step where you can explore Spark in more depth.\n2. [Hortonworks Community Connection (HCC)](https://community.hortonworks.com/spaces/85/data-science.html?type=question) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Hortonworks Apache Spark Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html) - official Spark documentation.\n4. [Hortonworks Apache Zeppelin Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html) - official Zeppelin documentation.\n","user":"sysadmin","dateUpdated":"2020-06-03T00:07:59+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We hope you've enjoyed this lab. Below are additional resources that you should find useful:</p>\n<ol>\n<li><a href=\"http://hortonworks.com/tutorials/#tuts-developers\">Hortonworks Apache Spark Tutorials</a> are your natural next step where you can explore Spark in more depth.</li>\n<li><a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\">Hortonworks Community Connection (HCC)</a> is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html\">Hortonworks Apache Spark Docs</a> - official Spark documentation.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html\">Hortonworks Apache Zeppelin Docs</a> - official Zeppelin documentation.</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1591142879918_1667870209","id":"20181126-133017_244739700","dateCreated":"2020-06-03T00:07:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254124"}],"name":"ScalaSpark/04-ExploringDataFramesUsingTheApacheSparkShell","id":"2F8WC318S","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"angular:shared_process":[],"sh:shared_process":[],"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}