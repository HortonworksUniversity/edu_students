{"paragraphs":[{"text":"%md\n# About\n**Lab:** Working with DataFrames and Schemas\n**Objective:** Practice working with structured account data and mobile device data using DataFrames\n**File locations:**\n- Data files (HDFS): /user/zeppelin/devices.json\n- Hive Tables: telco.accounts\n\n**Successful outcome:** Create and save DataFrames using different types of data sources, and infer and define schemas\n**Before you begin:** \n**Related lessons:** \n\n---","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>About</h1>\n<p><strong>Lab:</strong> Working with DataFrames and Schemas\n<br  /><strong>Objective:</strong> Practice working with structured account data and mobile device data using DataFrames\n<br  /><strong>File locations:</strong></p>\n<ul>\n<li>Data files (HDFS): /devsh_loudacre/devices.json</li>\n<li>Hive Tables: devsh.accounts</li>\n</ul>\n<p><strong>Successful outcome:</strong> Create and save DataFrames using different types of data sources, and infer and define schemas\n<br  /><strong>Before you begin:</strong>\n<br  /><strong>Related lessons:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591290514235_1015329994","id":"20181126-092644_1457476546","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4273"},{"text":"%md\n# Setup\nThis exercise requires the `telco.accounts` table, check that it exists and containts data by running the `SELECT` statement below. If the table has not already been created and loaded you can do so by completing exercise `20 - Supplemental: Creating and Loading Hive Database`.\n\n**Important:** This assume you are familiar with DataFrames, you can practice by completing exercise `04 - Exploring DataFrames Using the Apache Spark Shell`.","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Setup</h1>\n<p>This exercise requires the <code>telco.accounts</code> table, check that it exists and containts data by running the <code>SELECT</code> statement below. If the table has not already been created and loaded you can do so by completing exercise <code>20 - Supplemental: Creating and Loading Hive Database</code>.</p>\n<p><strong>Important:</strong> This assume you are familiar with DataFrames, you can practice by completing exercise 04 - Exploring DataFrames Using the Apache Spark Shell.</p>\n"}]},"apps":[],"jobName":"paragraph_1591290514239_-1586184574","id":"20181201-044336_178705192","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4274"},{"text":"%jdbc(hive)\nSELECT * FROM telco.accounts LIMIT 5;","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"accounts.acct_num":"string","accounts.acct_create_dt":"string","accounts.acct_close_dt":"string","accounts.first_name":"string","accounts.last_name":"string","accounts.address":"string","accounts.city":"string","accounts.state":"string","accounts.zipcode":"string","accounts.phone_number":"string","accounts.created":"string","accounts.modified":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"accounts.acct_num\taccounts.acct_create_dt\taccounts.acct_close_dt\taccounts.first_name\taccounts.last_name\taccounts.address\taccounts.city\taccounts.state\taccounts.zipcode\taccounts.phone_number\taccounts.created\taccounts.modified\n1\t2008-10-23 16:05:05.0\tnull\tDonald\tBecton\t2275 Washburn Street\tOakland\tCA\t94660\t5100032418\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n2\t2008-11-12 03:00:01.0\tnull\tDonna\tJones\t3885 Elliott Street\tSan Francisco\tCA\t94171\t4150835799\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n3\t2008-12-21 09:19:50.0\tnull\tDorthy\tChalmers\t4073 Whaley Lane\tSan Mateo\tCA\t94479\t6506877757\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n4\t2008-11-28 00:08:09.0\tnull\tLeila\tSpencer\t1447 Ross Street\tSan Mateo\tCA\t94444\t6503198619\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n5\t2008-11-15 23:06:06.0\tnull\tAnita\tLaughlin\t2767 Hill Street\tRichmond\tCA\t94872\t5107754354\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n"}]},"apps":[],"jobName":"paragraph_1591290514240_-1899872679","id":"20200602-184825_95278537","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4275"},{"text":"%md\n# Lab\n","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Lab</h1>\n"}]},"apps":[],"jobName":"paragraph_1591290514240_2119401368","id":"20181126-093358_358613711","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4276"},{"text":"%md\n### Create a DataFrame Based on a Hive Table","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Create a DataFrame Based on a Hive Table</h3>\n"}]},"apps":[],"jobName":"paragraph_1591290514240_-1963969134","id":"20200427-233238_1449553071","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4277"},{"title":"1 - Review the accounts table in the Hive database","text":"%md\nThis exercise uses a DataFrame based on the `accounts` table in the `telco` Hive database. You can review the schema using the `jdbc` interpreter to access Hive.","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>This exercise uses a DataFrame based on the <code>accounts</code> table in the <code>devsh</code> Hive\n<br  />database. You can review the schema using the Beeline SQL command line to access\n<br  />Hive.</p>\n<p>In a terminal session (not one that is running the Spark shell), enter the following\n<br  />command:</p>\n<pre><code>$ beeline -u jdbc:hive2://localhost:10000 -e \"DESCRIBE devsh.accounts\"\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591290514241_1796041224","id":"20200424-193917_1691179324","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4278"},{"text":"%jdbc(hive)\nDESCRIBE telco.accounts;","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"col_name":"string","data_type":"string","comment":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"col_name\tdata_type\tcomment\nacct_num\tint\t\nacct_create_dt\ttimestamp\t\nacct_close_dt\ttimestamp\t\nfirst_name\tvarchar(255)\t\nlast_name\tvarchar(255)\t\naddress\tvarchar(255)\t\ncity\tvarchar(255)\t\nstate\tvarchar(255)\t\nzipcode\tvarchar(255)\t\nphone_number\tvarchar(255)\t\ncreated\ttimestamp\t\nmodified\ttimestamp\t\n"}]},"apps":[],"jobName":"paragraph_1591290514241_-582804511","id":"20200521-194130_648124297","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4279"},{"title":"2 - Create a new DataFrame","text":"%md\nCreate a new DataFrame using the Hive `telco.accounts` table.","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame using the Hive <code>telco.accounts</code> table.</p>\n"}]},"apps":[],"jobName":"paragraph_1591290514242_1187683692","id":"20200424-194206_1083041321","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4280"},{"text":"%spark2\nval accountsDF = spark.read.table(\"telco.accounts\")","user":"sysadmin","dateUpdated":"2020-06-04T17:11:04+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591290514243_-956434922","id":"20200424-194301_536696070","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4281"},{"title":"3 - Compare the DataFrame and the Hive table","text":"%md\nPrint the schema and the first few rows of the DataFrame, and note that the schema aligns with that of the Hive table.","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Print the schema and the first few rows of the DataFrame, and note that the schema aligns with that of the Hive table.</p>\n"}]},"apps":[],"jobName":"paragraph_1591290514244_686975552","id":"20200424-194700_1657078805","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4282"},{"text":"%spark2\n","user":"sysadmin","dateUpdated":"2020-06-04T17:11:09+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591290514244_-1227753902","id":"20200522-201531_943828593","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4283"},{"title":"4 - Create a new DataFrame based on a condition","text":"%md\nCreate a new DataFrame with rows from the accounts data where the zip code is 94913, and save the result to CSV files in the `/user/zeppelin/accounts_zip94913` HDFS directory. You can do this in a single command, as shown below, or with multiple commands.","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"table","height":60,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame with rows from the accounts data where the zip\n<br  />code is 94913, and save the result to CSV files in the <code>/devsh_loudacre/accounts_zip94913</code> HDFS directory. You can do this in a single command, as shown below, or with multiple commands.</p>\n"}]},"apps":[],"jobName":"paragraph_1591290514245_721002259","id":"20200424-194924_522335325","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4284"},{"text":"%spark2\naccountsDF.where(\"zipcode = 94913\").\nwrite.option(\"header\",\"true\").\ncsv(\"/user/zeppelin/accounts_zip94913\")","user":"sysadmin","dateUpdated":"2020-06-04T17:11:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591290514245_1822986581","id":"20200424-195112_623415846","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4285"},{"title":"5 - Confirm the operation was executed correctly","text":"%md\nUse `hdfs` to view the `/user/zeppelin/accounts_zip94913` directory in HDFS and the data in one of the saved files. Confirm that the CSV file includes a header line, and that only records for the selected zip code are included.","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Use <code>hdfs</code> in a separate terminal window to view the\n<br  /><code>/devsh_loudacre/accounts_zip94913</code> directory in HDFS and the data in\n<br  />one of the saved files. Confirm that the CSV file includes a header line, and that only\n<br  />records for the selected zip code are included.</p>\n"}]},"apps":[],"jobName":"paragraph_1591290514246_-1823394844","id":"20200424-195251_938833734","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4286"},{"text":"%sh\n","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591290514247_-2009630763","id":"20200522-201740_1472082247","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4287"},{"title":"6 - Create a DataFrame from a CSV","text":"%md\n*Optional:* Try creating a new DataFrame based on the CSV files you created above. Compare the schema of the original `accountsDF` and the new DataFrame. What's different? Try again, this time setting the `inferSchema` option to true and compare again.","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><em>Optional:</em> Try creating a new DataFrame based on the CSV files you created above.\n<br  />Compare the schema of the original <code>accountsDF</code> and the new DataFrame. What's\n<br  />different? Try again, this time setting the <code>inferSchema</code> option to true and\n<br  />compare again.</p>\n"}]},"apps":[],"jobName":"paragraph_1591290514253_-250227090","id":"20200424-195331_1399907429","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4288"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591290514253_270322183","id":"20200522-201757_1455279920","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4289"},{"text":"%md\n### Define a Schema for a DataFrame","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Define a Schema for a DataFrame</h3>\n"}]},"apps":[],"jobName":"paragraph_1591290514254_-1915109833","id":"20200427-234619_2069213136","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4290"},{"title":"7 - Review the data file","text":"%md\nIf you have not done so yet, review the data in the HDFS file `/user/zeppelin/devices.json`.","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>If you have not done so yet, review the data in the HDFS file\n<br  /><code>/devsh_loudacre/devices.json</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1591290514254_-1484211997","id":"20200424-195454_1861585933","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4291"},{"text":"%sh\n","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591290514255_758522103","id":"20200430-003645_261089763","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4292"},{"title":"8 - Create a DataFrame based on the devices.json file","text":"%md\nCreate a new DataFrame based on the `devices.json` file. (This command could take several seconds while it infers the schema.)","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame based on the <code>devices.json</code> file. (This command could take several seconds while it infers the schema.)</p>\n"}]},"apps":[],"jobName":"paragraph_1591290514256_1424281585","id":"20200424-201503_1618492364","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4293"},{"text":"%spark2\nval devDF = spark.read.json(\"/user/zeppelin/devices.json\")","user":"sysadmin","dateUpdated":"2020-06-04T17:11:49+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591290514257_709203838","id":"20200424-201433_776130777","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4294"},{"title":"9 - Review the schema of the devDF DataFrame","text":"%md\nView the schema of the `devDF` DataFrame. Note the column names and types that Spark inferred from the JSON file. In particular, note that the `release_dt` column is of type `string`, whereas the data in the column actually represents a timestamp.","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>View the schema of the <code>devDF</code> DataFrame. Note the column names and types that\n<br  />Spark inferred from the JSON file. In particular, note that the <code>release_dt</code> column\n<br  />is of type <code>string</code>, whereas the data in the column actually represents a timestamp.</p>\n"}]},"apps":[],"jobName":"paragraph_1591290514257_-1712959570","id":"20200424-201330_1854493694","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4295"},{"text":"%spark2\n","user":"sysadmin","dateUpdated":"2020-06-04T17:12:00+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591290514258_1608940234","id":"20200522-201923_1097540138","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4296"},{"title":"10 - Specify the column types of the DataFrame","text":"%md\nDefine a schema that correctly specifies the column types for this DataFrame. Start by importing the package with the definitions of necessary classes and types.","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Define a schema that correctly specifies the column types for this DataFrame. Start\n<br  />by importing the package with the definitions of necessary classes and types.</p>\n"}]},"apps":[],"jobName":"paragraph_1591290514258_-1669876697","id":"20200424-201310_819844900","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4297"},{"text":"%spark2\nimport org.apache.spark.sql.types._","user":"sysadmin","dateUpdated":"2020-06-04T17:12:14+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591290514259_-1208641526","id":"20200424-201240_160502927","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4298"},{"title":"11 - Represent the column definitions with StructField objects","text":"%md\nNext, create a collection of `StructField` objects, which represent column definitions. The `release_dt` column should be a timestamp.","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Next, create a collection of <code>StructField</code> objects, which represent column\n<br  />definitions. The <code>release_dt</code> column should be a timestamp.</p>\n"}]},"apps":[],"jobName":"paragraph_1591290514260_-841710965","id":"20200424-201153_1739325405","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4299"},{"text":"%spark2\nval devColumns = List(\nStructField(\"devnum\",LongType),\nStructField(\"make\",StringType),\nStructField(\"model\",StringType),\nStructField(\"release_dt\",TimestampType),\nStructField(\"dev_type\",StringType))","user":"sysadmin","dateUpdated":"2020-06-04T17:12:48+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591290514260_-1755281027","id":"20200424-201128_583503462","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4300"},{"title":"12 - Create a schema using the column definition list","text":"%md\nCreate a schema (a `StructType` object) using the column definition list.","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a schema (a <code>StructType</code> object) using the column definition list.</p>\n"}]},"apps":[],"jobName":"paragraph_1591290514261_-1706667106","id":"20200424-201020_2045594003","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4301"},{"text":"%spark2\nval devSchema = StructType(devColumns)","user":"sysadmin","dateUpdated":"2020-06-04T17:13:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591290514262_-1529621178","id":"20200424-200751_1167632895","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4302"},{"title":"13 - Recreate the devDF DataFrame using the new schema","text":"%md\nRecreate the `devDF` DataFrame, this time using the new schema.","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Recreate the <code>devDF</code> DataFrame, this time using the new schema.</p>\n"}]},"apps":[],"jobName":"paragraph_1591290514262_82477900","id":"20200424-200207_703556537","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4303"},{"text":"%spark2\nval devDF = spark.read.schema(devSchema).json(\"/user/zeppelin/devices.json\")","user":"sysadmin","dateUpdated":"2020-06-04T17:13:18+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591290514263_249413443","id":"20200424-200134_290658193","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4304"},{"title":"14 - Confirm that the release_dt column is now of type timestamp","text":"%md\nView the schema and data of the new DataFrame, and confirm that the `release_dt` column type is now `timestamp`.","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>View the schema and data of the new DataFrame, and confirm that the\n<br  /><code>release_dt</code> column type is now <code>timestamp</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1591290514264_1190302832","id":"20200424-200027_404284113","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4305"},{"text":"%spark2\n","user":"sysadmin","dateUpdated":"2020-06-04T17:13:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591290514265_-1034253821","id":"20200522-202136_1188742309","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4306"},{"title":"15 - Save the DataFrame in Parquet format","text":"%md\nNow that the device data uses the correct schema, write the data in Parquet format, which automatically embeds the schema. Save the Parquet data files into an HDFS directory called `/user/zeppelin/devices_parquet`.","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Now that the device data uses the correct schema, write the data in Parquet format,\n<br  />which automatically embeds the schema. Save the Parquet data files into an HDFS\n<br  />directory called <code>/devsh_loudacre/devices_parquet</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1591290514265_-1471607694","id":"20200424-195942_1783798900","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4307"},{"text":"%spark2\n","user":"sysadmin","dateUpdated":"2020-06-04T17:13:42+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591290514266_208820172","id":"20200522-202142_98727568","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4308"},{"title":"16 - View the schema of the Parquet file","text":"%md\n*Optional:* Use `parquet-tools` to view the schema of the saved files. First download the HDFS directory (or an individual file), then run the command.\n\n```\n    $ hdfs dfs -get /user/zeppelin/devices_parquet /tmp/\n    $ parquet-tools schema /tmp/devices_parquet\n```\n\nNote that the type of the `release_dt` column is noted as `int96`; this is how Spark denotes a timestamp type in Parquet.\n\nFor more information about `parquet-tools`, run `parquet-tools --help`.","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><em>Optional:</em> In a separate terminal window, use <code>parquet-tools</code> to view the schema\n<br  />of the saved files. First download the HDFS directory (or an individual file), then run\n<br  />the command.</p>\n<pre><code>$ hdfs dfs -get /devsh_loudacre/devices_parquet /tmp/\n$ parquet-tools schema /tmp/devices_parquet/\n</code></pre>\n<p>Note that the type of the <code>release_dt</code> column is noted as <code>int96</code>; this is how Spark\n<br  />denotes a timestamp type in Parquet.</p>\n<p>For more information about <code>parquet-tools</code>, run <code>parquet-tools --help</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1591290514266_-1098740813","id":"20200424-195815_1058807478","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4309"},{"text":"%sh\n","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591290514267_760489991","id":"20200522-202202_1659863288","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4310"},{"title":"17 - Create a DataFrame from the Parquet files","text":"%md\nCreate a new DataFrame using the Parquet files you saved in `devices_parquet` and view its schema. Note that Spark is able to correctly infer the timestamp type of the `release_dt` column from Parquet's embedded schema.","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame using the Parquet files you saved in <code>devices_parquet</code>\n<br  />and view its schema. Note that Spark is able to correctly infer the timestamp type\n<br  />of the <code>release_dt</code> column from Parquet's embedded schema.</p>\n"}]},"apps":[],"jobName":"paragraph_1591290514268_-376980054","id":"20200424-195648_983273468","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4311"},{"text":"%spark2\n","user":"sysadmin","dateUpdated":"2020-06-04T17:14:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591290514268_1342906967","id":"20200522-202219_851657177","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4312"},{"text":"%md\n# Result\n**You have now:** \n\n---","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Result</h1>\n<p><strong>You have now:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591290514269_-214070588","id":"20181126-133507_1472573213","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4313"},{"text":"%md\n# Solution\n---","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Solution</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591290514270_1608673958","id":"20181018-125200_1133281582","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4314"},{"text":"%md\n### Create a DataFrame Based on a Hive Table","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Create a DataFrame Based on a Hive Table</h3>\n"}]},"apps":[],"jobName":"paragraph_1591290514270_-173279907","id":"20200428-225106_184286167","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4315"},{"title":"1 - Review the accounts table in the Hive database","text":"%jdbc(hive)\nDESCRIBE telco.accounts;","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":false,"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"col_name":"string","data_type":"string","comment":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"col_name\tdata_type\tcomment\nacct_num\tint\t\nacct_create_dt\ttimestamp\t\nacct_close_dt\ttimestamp\t\nfirst_name\tvarchar(255)\t\nlast_name\tvarchar(255)\t\naddress\tvarchar(255)\t\ncity\tvarchar(255)\t\nstate\tvarchar(255)\t\nzipcode\tvarchar(255)\t\nphone_number\tvarchar(255)\t\ncreated\ttimestamp\t\nmodified\ttimestamp\t\n"}]},"apps":[],"jobName":"paragraph_1591290514271_-1591414525","id":"20200428-225139_1888690996","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4316"},{"title":"2 - Create a new DataFrame","text":"%spark2\nval accountsDF = spark.read.table(\"telco.accounts\")","user":"sysadmin","dateUpdated":"2020-06-04T17:14:42+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1591290514272_-1389001438","id":"20200428-225540_857086049","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4317"},{"title":"3 - Compare the DataFrame and the Hive table","text":"%spark2\naccountsDF.printSchema","user":"sysadmin","dateUpdated":"2020-06-04T17:14:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- acct_num: integer (nullable = true)\n |-- acct_create_dt: timestamp (nullable = true)\n |-- acct_close_dt: timestamp (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- zipcode: string (nullable = true)\n |-- phone_number: string (nullable = true)\n |-- created: timestamp (nullable = true)\n |-- modified: timestamp (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1591290514272_1260417389","id":"20200428-225533_1026098327","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4318"},{"title":"4 - Create a new DataFrame based on a condition","text":"%spark2\naccountsDF.where(\"zipcode = 94913\").write.option(\"header\",\"true\").csv(\"/user/zeppelin/accounts_zip94913\")","user":"sysadmin","dateUpdated":"2020-06-04T17:15:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1591290514273_-1647066131","id":"20200428-225650_1329297880","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4319"},{"title":"5 - Confirm the operation was executed correctly","text":"%sh\nhdfs dfs -ls /user/zeppelin/accounts_zip94913\n\necho\nhdfs dfs -head /user/zeppelin/accounts_zip94913/<part-file.csv>","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Found 5 items\n-rw-r--r--   3 zeppelin hdfs          0 2020-05-26 03:42 /user/zeppelin/accounts_zip94913/_SUCCESS\n-rw-r--r--   3 zeppelin hdfs        882 2020-05-26 03:42 /user/zeppelin/accounts_zip94913/part-00000-d360ab0f-6d8d-4a21-b515-d60ea16cc7a0-c000.csv\n-rw-r--r--   3 zeppelin hdfs       1026 2020-05-26 03:42 /user/zeppelin/accounts_zip94913/part-00001-d360ab0f-6d8d-4a21-b515-d60ea16cc7a0-c000.csv\n-rw-r--r--   3 zeppelin hdfs       1289 2020-05-26 03:42 /user/zeppelin/accounts_zip94913/part-00002-d360ab0f-6d8d-4a21-b515-d60ea16cc7a0-c000.csv\n-rw-r--r--   3 zeppelin hdfs       1514 2020-05-26 03:42 /user/zeppelin/accounts_zip94913/part-00003-d360ab0f-6d8d-4a21-b515-d60ea16cc7a0-c000.csv\n\nhead: `/user/zeppelin/accounts_zip94913/part-00000-efa4f310-a43b-4e6f-9798-1caf35e97702-c000.csv': No such file or directory\n"},{"type":"TEXT","data":"ExitValue: 1"}]},"apps":[],"jobName":"paragraph_1591290514273_290204648","id":"20200428-225649_867963440","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4320"},{"title":"6 - Create a DataFrame from a CSV","text":"%spark2\nval test1DF = spark.read.option(\"header\",\"true\").csv(\"/user/zeppelin/accounts_zip94913\")\nval test2DF = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/user/zeppelin/accounts_zip94913\")\n\ntest1DF.printSchema\ntest2DF.printSchema","user":"sysadmin","dateUpdated":"2020-06-04T17:15:37+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- acct_num: string (nullable = true)\n |-- acct_create_dt: string (nullable = true)\n |-- acct_close_dt: string (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- zipcode: string (nullable = true)\n |-- phone_number: string (nullable = true)\n |-- created: string (nullable = true)\n |-- modified: string (nullable = true)\n\nroot\n |-- acct_num: integer (nullable = true)\n |-- acct_create_dt: timestamp (nullable = true)\n |-- acct_close_dt: timestamp (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- zipcode: integer (nullable = true)\n |-- phone_number: long (nullable = true)\n |-- created: timestamp (nullable = true)\n |-- modified: timestamp (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1591290514274_1067615396","id":"20200428-225649_314698363","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4321"},{"text":"%md\n### Define a Schema for a DataFrame","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Define a Schema for a DataFrame</h3>\n"}]},"apps":[],"jobName":"paragraph_1591290514274_-2059371954","id":"20200428-225648_2130864594","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4322"},{"title":"7 - Review the data file","text":"%sh\nhdfs dfs -head /user/zeppelin/devices.json","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"{\"devnum\":1,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"Sorrento\",\"model\":\"F00L\",\"dev_type\":\"phone\"}\n{\"devnum\":2,\"release_dt\":\"2010-04-19T00:00:00.000-07:00\",\"make\":\"Titanic\",\"model\":\"2100\",\"dev_type\":\"phone\"}\n{\"devnum\":3,\"release_dt\":\"2011-02-18T00:00:00.000-08:00\",\"make\":\"MeeToo\",\"model\":\"3.0\",\"dev_type\":\"phone\"}\n{\"devnum\":4,\"release_dt\":\"2011-09-21T00:00:00.000-07:00\",\"make\":\"MeeToo\",\"model\":\"3.1\",\"dev_type\":\"phone\"}\n{\"devnum\":5,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"1\",\"dev_type\":\"phone\"}\n{\"devnum\":6,\"release_dt\":\"2011-11-02T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"3\",\"dev_type\":\"phone\"}\n{\"devnum\":7,\"release_dt\":\"2010-05-20T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"2\",\"dev_type\":\"phone\"}\n{\"devnum\":8,\"release_dt\":\"2013-07-02T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"5\",\"dev_type\":\"phone\"}\n{\"devnum\":9,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"Titanic\",\"model\":\"1000\",\"dev_type\":\"phone\"}\n{\"devnum\":10,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"mak"}]},"apps":[],"jobName":"paragraph_1591290514275_937701799","id":"20200428-225648_313296430","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4323"},{"title":"8 - Create a DataFrame based on the devices.json file","text":"%spark2\nval devDF = spark.read.json(\"/user/zeppelin/devices.json\")","user":"sysadmin","dateUpdated":"2020-06-04T17:16:05+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1591290514276_-1719855989","id":"20200428-225648_1886643259","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4324"},{"title":"9 - Review the schema of the devDF DataFrame","text":"%spark2\ndevDF.printSchema","user":"sysadmin","dateUpdated":"2020-06-04T17:16:11+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- dev_type: string (nullable = true)\n |-- devnum: long (nullable = true)\n |-- make: string (nullable = true)\n |-- model: string (nullable = true)\n |-- release_dt: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1591290514276_-1777139885","id":"20200428-225647_1646122487","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4325"},{"title":"10 - Specify the column types of the DataFrame","text":"%spark2\nimport org.apache.spark.sql.types._","user":"sysadmin","dateUpdated":"2020-06-04T17:16:43+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1591290514277_214512133","id":"20200428-225647_1682536791","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4326"},{"title":"11 - Represent the column definitions with StructField objects","text":"%spark2\nval devColumns = List(\n  StructField(\"devnum\",LongType),\n  StructFielaad(\"make\",StringType),\n  StructField(\"model\",StringType),\n  StructField(\"release_dt\",TimestampType),\n  StructField(\"dev_type\",StringType))","user":"sysadmin","dateUpdated":"2020-06-04T17:16:56+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1591290514277_1584923183","id":"20200428-225646_614334074","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4327"},{"title":"12 - Create a schema using the column definitions","text":"%spark2\nval devSchema = StructType(devColumns)","user":"sysadmin","dateUpdated":"2020-06-04T17:17:11+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1591290514278_-1875903875","id":"20200428-225645_364347750","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4328"},{"title":"13 - Recreate the devDF DataFrame using the new schema","text":"%spark2\nval devDF = spark.read.schema(devSchema).json(\"/user/zeppelin/devices.json\")\ndevDF.printSchema","user":"sysadmin","dateUpdated":"2020-06-04T17:17:18+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- devnum: long (nullable = true)\n |-- make: string (nullable = true)\n |-- model: string (nullable = true)\n |-- release_dt: timestamp (nullable = true)\n |-- dev_type: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1591290514278_-1997262846","id":"20200428-225644_1723966751","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4329"},{"title":"14 - Confirm that the release_dt column is now of type timestamp","text":"%spark2\ndevDF.show","user":"sysadmin","dateUpdated":"2020-06-04T17:17:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+--------+--------------+-------------------+--------+\n|devnum|    make|         model|         release_dt|dev_type|\n+------+--------+--------------+-------------------+--------+\n|     1|Sorrento|          F00L|2008-10-21 07:00:00|   phone|\n|     2| Titanic|          2100|2010-04-19 07:00:00|   phone|\n|     3|  MeeToo|           3.0|2011-02-18 08:00:00|   phone|\n|     4|  MeeToo|           3.1|2011-09-21 07:00:00|   phone|\n|     5|  iFruit|             1|2008-10-21 07:00:00|   phone|\n|     6|  iFruit|             3|2011-11-02 07:00:00|   phone|\n|     7|  iFruit|             2|2010-05-20 07:00:00|   phone|\n|     8|  iFruit|             5|2013-07-02 07:00:00|   phone|\n|     9| Titanic|          1000|2008-10-21 07:00:00|   phone|\n|    10|  MeeToo|           1.0|2008-10-21 07:00:00|   phone|\n|    11|Sorrento|          F21L|2011-02-28 08:00:00|   phone|\n|    12|  iFruit|             4|2012-10-25 07:00:00|   phone|\n|    13|Sorrento|          F23L|2011-11-21 08:00:00|   phone|\n|    14| Titanic|          2200|2010-05-25 07:00:00|   phone|\n|    15|   Ronin|Novelty Note 1|2010-06-20 07:00:00|   phone|\n|    16| Titanic|          2500|2012-07-21 07:00:00|   phone|\n|    17|   Ronin|Novelty Note 3|2013-04-11 07:00:00|   phone|\n|    18|   Ronin|Novelty Note 2|2011-10-02 07:00:00|   phone|\n|    19|   Ronin|Novelty Note 4|2013-07-02 07:00:00|   phone|\n|    20|  iFruit|            3A|2012-07-21 07:00:00|   phone|\n+------+--------+--------------+-------------------+--------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1591290514279_-558120163","id":"20200428-230856_1582268955","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4330"},{"title":"15 - Save the DataFrame in Parquet format","text":"%spark2\ndevDF.write.parquet(\"/user/zeppelin/devices_parquet\") ","user":"sysadmin","dateUpdated":"2020-06-04T17:17:44+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1591290514280_-711554684","id":"20200428-230854_1798105788","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4331"},{"title":"16 - View the schema of the Parquet file","text":"%sh\nhdfs dfs -get /user/zeppelin/devices_parquet /tmp/\nparquet-tools schema /tmp/devices_parquet","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"bash: line 1: parquet-tools: command not found\n"},{"type":"TEXT","data":"ExitValue: 127"}]},"apps":[],"jobName":"paragraph_1591290514280_1784201917","id":"20200428-230853_1175884827","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4332"},{"title":"17 - Create a DataFrame from the Parquet files","text":"%spark2\nspark.read.parquet(\"/user/zeppelin/devices_parquet\").printSchema","user":"sysadmin","dateUpdated":"2020-06-04T17:17:59+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591290514281_1310043869","id":"20200428-230853_1544838296","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4333"},{"text":"%md\nWe hope you've enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Hortonworks Apache Spark Tutorials](http://hortonworks.com/tutorials/#tuts-developers) are your natural next step where you can explore Spark in more depth.\n2. [Hortonworks Community Connection (HCC)](https://community.hortonworks.com/spaces/85/data-science.html?type=question) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Hortonworks Apache Spark Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html) - official Spark documentation.\n4. [Hortonworks Apache Zeppelin Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html) - official Zeppelin documentation.\n","user":"sysadmin","dateUpdated":"2020-06-04T17:08:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We hope you've enjoyed this lab. Below are additional resources that you should find useful:</p>\n<ol>\n<li><a href=\"http://hortonworks.com/tutorials/#tuts-developers\">Hortonworks Apache Spark Tutorials</a> are your natural next step where you can explore Spark in more depth.</li>\n<li><a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\">Hortonworks Community Connection (HCC)</a> is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html\">Hortonworks Apache Spark Docs</a> - official Spark documentation.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html\">Hortonworks Apache Zeppelin Docs</a> - official Zeppelin documentation.</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1591290514281_1760198164","id":"20181126-133017_244739700","dateCreated":"2020-06-04T17:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4334"}],"name":"ScalaSpark/05-WorkingWithDataFramesAndSchemas","id":"2FAS8AGZ1","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}