{"paragraphs":[{"text":"%md\n# About\n**Lab:** Writing, Configuring, and Running a Spark Application\n**Objective:** Write your own Spark application instead of usng the interactive Spark shell application.\n**File locations:**\n    Hive Tables: telco.accounts\n    Python stub: accounts-by-state.py\n    \n**Successful outcome:** \n**Before you begin:** \n**Related lessons:** \n\n---","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>About</h1>\n<p><strong>Lab:</strong> Writing, Configuring, and Running a Spark Application\n<br  /><strong>Objective:</strong> Write your own Spark application instead of usng the interactive Spark shell application.\n<br  /><strong>File locations:</strong></p>\n<pre><code>Hive Tables: devsh.accounts\nScala project: $DEVSH/exercises/spark-application/accounts-by-state_project\nScala classes: stubs.AccountsByState\n               solution.AccountsByState\nPython stub: accounts-by-state.py\n</code></pre>\n<p><strong>Successful outcome:</strong>\n<br  /><strong>Before you begin:</strong>\n<br  /><strong>Related lessons:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591815454863_-1490175071","id":"20181126-092644_1457476546","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:128099"},{"text":"%md\n# Setup\n\nThe Spark application will take a single argument -- a state code (such as `CA`). The program should read the data from the telco.accounts Hive table and save the rows whose state column value matches the specified state code. Write the results to `/user/zeppelin/accounts_by_state/`*state-code* (such as `accounts_by_state/CA`).","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Setup</h1>\n<p>The Spark application will take a single argument &ndash; a state code (such as <code>CA</code>). The\n<br  />program should read the data from the devsh.accounts Hive table and save\n<br  />the rows whose state column value matches the specified state code. Write the\n<br  />results to <code>/devsh_loudacre/accounts_by_state/</code><em>state-code</em> (such as\n<br  /><code>accounts_by_state/CA</code>).</p>\n<p>Depending on which programming language you are using, follow the appropriate set of\n<br  />instructions below to write a Spark program.</p>\n<p><strong>Important:</strong> This exercise depends on <strong><em> ***Insert previous exercise title here (with link?)*** </em></strong>. If you did not complete that exercise, run the course catch-up script and advance to the current exercise:</p>\n<pre><code>$ $DEVSH/scripts/catchup.sh\n</code></pre>\n<h3>Editing Scala and Python Files</h3>\n<blockquote><p>You may use any text editor you wish to work on your application\n<br  />code. If you do not have an editor preference, you may wish to\n<br  />use Pluma, which includes language-specific support for Scala and\n<br  />Python.\n<br  />You can start Pluma on your remote desktop using the editor icon\n<br  />on the remote desktop menu bar.</p>\n</blockquote>\n"}]},"apps":[],"jobName":"paragraph_1591815454864_1330648848","id":"20181201-044336_178705192","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128100"},{"text":"%md\n# Lab\n","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Lab</h1>\n"}]},"apps":[],"jobName":"paragraph_1591815454864_-1941950358","id":"20181126-093358_358613711","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128101"},{"text":"%md\n### Write and Run a Spark Application in Python","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Write and Run a Spark Application in Python</h3>\n"}]},"apps":[],"jobName":"paragraph_1591815454864_140763974","id":"20200425-221329_1276776329","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128102"},{"title":"1 - Review the script","text":"%md\nBelow is the application code, review it and ensure you understand the purpose of each line.\n\n```\nimport sys\n\nfrom pyspark.sql import SparkSession\n\nif __name__ == \"__main__\":\n  if len(sys.argv) < 2:\n    print(sys.stderr, \"Usage: accounts-by-state.py <state-code>\")\n    sys.exit()\n\n  stateCode = sys.argv[1]\n\n  # 3. Create a SparkSesssion object\n  spark = SparkSession.builder.getOrCreate()\n  # 4. Reduce distracting output\n  spark.sparkContext.setLogLevel(\"WARN\")\n  \n  # 5. Load accounts table\n  accountsDF = spark.read.table(\"telco.accounts\")\n  # Select accounts where the state column matches the argument provided\n  stateAccountsDF = accountsDF.where(accountsDF.state == stateCode)\n  # Save the results\n  stateAccountsDF.write.mode(\"overwrite\").save(\"/user/zeppelin/accounts_by_state/\" + stateCode)\n\n  # 6. Stop the Spark session\n  spark.stop()\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Below is the application code, review it and ensure you understand the purpose of each line.</p>\n<pre><code>import sys\n\nfrom pyspark.sql import SparkSession\n\nif __name__ == \"__main__\":\n  if len(sys.argv) &lt; 2:\n    print(sys.stderr, \"Usage: accounts-by-state.py &lt;state-code&gt;\")\n    sys.exit()\n\n  stateCode = sys.argv[1]\n\n  # 3. Create a SparkSesssion object\n  spark = SparkSession.builder.getOrCreate()\n  # 4. Reduce distracting output\n  spark.sparkContext.setLogLevel(\"WARN\")\n\n  # 5. Load accounts table\n  accountsDF = spark.read.table(\"telco.accounts\")\n  # Select accounts where the state column matches the argument provided\n  stateAccountsDF = accountsDF.where(accountsDF.state == stateCode)\n  # Save the results\n  stateAccountsDF.write.mode(\"overwrite\").save(\"/user/zeppelin/accounts_by_state/\" + stateCode)\n\n  # 6. Stop the Spark session\n  spark.stop()\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591815454865_810477460","id":"20200425-221426_826399515","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128103"},{"title":"2 - Run your application","text":"%md\nRun the program, passing the state code to select. For example, to select accounts in California, use the command below.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>In a terminal window, change to the exercise working\n<br  />directory, then run the program, passing the state code to select. For example, to\n<br  />select accounts in California, use the following command:</p>\n<pre><code>$ cd $DEVSH/exercises/spark-application/\n$ spark-submit python-stubs/accounts-by-state.py CA\n</code></pre>\n<p><strong>Note:</strong> To run the solution application, use python-solution/accounts-bystate.py.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815454865_692548867","id":"20200425-221403_1751053593","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128104"},{"text":"%sh\nspark-submit /home/devuser/bin/spark/application/python-solution/accounts-by-state.py CA\n#cd /home/devuser/bin/spark/application/python-solution/\n#spark-submit accounts-by-state.py CA\n\nspark-submit /home/devuser/bin/spark/application/python-solution/accounts-by-state.py CA\n#cd /home/devuser/bin/spark/application/python-solution/\n#spark-submit accounts-by-state.py CA","user":"sysadmin","dateUpdated":"2020-06-13T17:21:18+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"accounts-by-state_project\npython-bonus\npython-solution\npython-stubs\nrun-pyspark.sh\nrun-scala.sh\n"}]},"apps":[],"jobName":"paragraph_1591815454865_-223468448","id":"20200429-181143_389236631","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128105"},{"title":"3 - Verify the data was saved correctly and use parquet-tools to verify the contents","text":"%md\nAfter the program completes, confirm the files were saved, then use `parquet-tools` to verify that the file contents are correct.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815454865_-877846756","id":"20200425-221403_1016428143","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128106"},{"text":"%sh\n","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815454866_-1963607409","id":"20200601-173820_983684384","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128107"},{"text":"%md\n### View the Spark Application UI\nIn the previous section, you ran a Python Spark application using `spark-submit`. Now view that application's Spark UI (or history server UI if the application is complete).","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>View the Spark Application UI</h3>\n<p>In the previous section, you ran a Python Spark application using <code>spark-submit</code>. Now view that application's Spark UI (or history server UI if the application is complete).</p>\n"}]},"apps":[],"jobName":"paragraph_1591815454866_670319136","id":"20200425-223045_605713232","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128108"},{"title":"4 - Visit the YARN Resource Manager UI","text":"%md\nOpen the web browser on your remote desktop and visit the YARN Resource Manager UI.\n\nTo view the Spark Application UI if your application is still running, select the **ApplicationMaster** link. To view the History Service UI if your application has completed, select the **History** link.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Open Firefox on your remote desktop and visit the YARN Resource Manager\n<br  />UI using the provided <strong>YARN RM</strong> bookmark (or go to URI <code>http://localhost:8088/</code>).</p>\n<p>To view the Spark Application UI if your application is still running, select the\n<br  /><strong>ApplicationMaster</strong> link. To view the History Service UI if your application has\n<br  />completed, select the <strong>History</strong> link.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815454866_1166820","id":"20200425-223044_1000277893","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128109"},{"text":"%md\n### Set Configuration Options Using Submit Script Flags","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Set Configuration Options Using Submit Script Flags</h3>\n"}]},"apps":[],"jobName":"paragraph_1591815454866_-659000974","id":"20200425-223044_502751025","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128110"},{"title":"5 - Specify an Application name","text":"%md\nChange to the correct directory (if necessary) and re-run the program from the previous section, this time specifying an application name. For example:","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Change to the correct directory (if necessary) and re-run the Python or Scala\n<br  />program you wrote in the previous section, this time specifying an application\n<br  />name. For example:</p>\n<pre><code>$ spark-submit --name \"Accounts by State 1\" python-stubs/accounts-by-state.py CA\n</code></pre>\n<!-- -->\n<pre><code>$ spark-submit --name \"Accounts by State 1\" --class stubs.AccountsByState target/accounts-by-state-1.0.jar CA\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591815454867_-1290472372","id":"20200425-223043_1687749516","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128111"},{"text":"%sh\nspark-submit --name \"Accounts by State 1\" /home/devuser/bin/spark/application/python-solution/accounts-by-state.py CA","user":"sysadmin","dateUpdated":"2020-06-13T17:23:29+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815454867_-348949881","id":"20200429-182714_1293429514","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128112"},{"title":"6 - Use YARN UI to confirm that the application name was set correctly","text":"%md\nGo back to the YARN RM UI in your browser, and confirm that the application name was set correctly in the list of applications.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815454867_1974694469","id":"20200425-223043_2070551072","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128113"},{"title":"7 - Use YARN UI to review the properties of the application","text":"%md\nFollow the **ApplicationMaster** or **History** link. View the **Environment** tab. Take note of the `spark.*` properties such as `master` and `app.name`.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815454867_1956372272","id":"20200425-223042_388773792","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128114"},{"title":"8 - Using the conf option","text":"%md\nYou can set most of the common application properties using submit script\nflags such as name, but for others you need to use conf. Use conf to set the\nspark.default.parallelism property, which controls how many partitions\nresult after a \"wide\" RDD operation like reduceByKey.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>You can set most of the common application properties using submit script\n<br  />flags such as name, but for others you need to use conf. Use conf to set the\n<br  />spark.default.parallelism property, which controls how many partitions\n<br  />result after a &ldquo;wide&rdquo; RDD operation like reduceByKey.</p>\n<pre><code>$ spark-submit --name \"Accounts by State 2\" \\\n--conf spark.default.parallelism=4 \\\npython-stubs/accounts-by-state.py CA\n</code></pre>\n<!-- -->\n<pre><code>$ spark-submit --name \"Accounts by State 2\" \\\n--conf spark.default.parallelism=4 \\\n--class stubs.AccountsByState \\\ntarget/accounts-by-state-1.0.jar CA\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591815454867_1706188597","id":"20200425-223042_1593700964","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128115"},{"text":"%sh\nspark-submit --name \"Accounts by State 2\" \\\n--conf spark.default.parallelism=4 /home/devuser/bin/spark/application/python-solution/accounts-by-state.py CA","user":"sysadmin","dateUpdated":"2020-06-13T17:23:45+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815454868_750725873","id":"20200429-183329_38153990","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128116"},{"title":"9 - Use YARN UI to confirm that the parallelism property is set correctly","text":"%md\nView the application history for this application to confirm that the `spark.default.parallelism` property was set correctly. (You will need to view the YARN RM UI again to view the correct application's history.)","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815454868_-661373052","id":"20200425-223042_278139677","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128117"},{"text":"%md\n### *Optional:* Review Property Setting Overrides","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3><em>Optional:</em> Review Property Setting Overrides</h3>\n"}]},"apps":[],"jobName":"paragraph_1591815454868_-1012258794","id":"20200425-223041_330189215","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128118"},{"title":"10 - Display Application Property values","text":"%md\nRerun the previous submit command with the verbose option. This will display\nyour application property default and override values.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Rerun the previous submit command with the verbose option. This will display\n<br  />your application property default and override values.</p>\n<pre><code>$ spark-submit --verbose --name \"Accounts by State 3\" \\\n--conf spark.default.parallelism=4 \\\npython-stubs/accounts-by-state.py CA\n</code></pre>\n<!-- -->\n<pre><code>$ spark-submit --verbose --name \"Accounts by State 3\" \\\n--conf spark.default.parallelism=4 \\\n--class stubs.AccountsByState \\\ntarget/accounts-by-state-1.0.jar CA\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591815454868_-1646686927","id":"20200425-223041_1300387039","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128119"},{"text":"%sh\nspark-submit --verbose --name \"Accounts by State 3\" \\\n--conf spark.default.parallelism=4 \\\n/home/devuser/bin/spark/application/python-solution/accounts-by-state.py CA","user":"sysadmin","dateUpdated":"2020-06-13T17:23:58+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815454869_-1234741929","id":"20200429-184552_168628892","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128120"},{"title":"11 - Examine the properties","text":"%md\nExamine the extra output displayed when the application starts up.\n\n**a.** The first section starts with `Using properties file`, and shows the\nfile name and the default property settings the application loaded from that\nproperties file.\n\n- What is the system properties file?\n\n- What properties are being set in the file?\n\n**b.** The second section starts with Parsed arguments. This lists the arguments\n- that is, the flags and settings - you set when running the submit script\n(except for `conf`). Submit script flags that you did not pass use their default\nvalues, if defined by the script, or are shown as `null`.\n\n- Does the list correctly include the value you set with `--name`?\n\n- Which arguments (flags) have defaults set in the script and which do not?\n\n**c.** Scroll down to the section that starts with `System properties`. This list\nshows *all* the properties set - those loaded from the system properties file,\nthose you set using submit script arguments, and those you set using the conf\nflag.\n\n- Is `spark.default.parallelism` included and set correctly?","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Examine the extra output displayed when the application starts up.</p>\n<p>a. The first section starts with <code>Using properties file</code>, and shows the\n<br  />file name and the default property settings the application loaded from that\n<br  />properties file.</p>\n<ul>\n<li><p>What is the system properties file?</p>\n</li>\n<li><p>What properties are being set in the file?</p>\n</li>\n</ul>\n<p>b. The second section starts with Parsed arguments. This lists the arguments</p>\n<ul>\n<li><p>that is, the flags and settings - you set when running the submit script\n<br  />(except for <code>conf</code>). Submit script flags that you did not pass use their default\n<br  />values, if defined by the script, or are shown as <code>null</code>.</p>\n</li>\n<li><p>Does the list correctly include the value you set with <code>--name</code>?</p>\n</li>\n<li><p>Which arguments (flags) have defaults set in the script and which do not?</p>\n</li>\n</ul>\n<p>c. Scroll down to the section that starts with <code>System properties</code>. This list\n<br  />shows <em>all</em> the properties set - those loaded from the system properties file,\n<br  />those you set using submit script arguments, and those you set using the conf\n<br  />flag.</p>\n<ul>\n<li>Is <code>spark.default.parallelism</code> included and set correctly?</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1591815454869_-375554553","id":"20200425-223040_82417787","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128121"},{"text":"%md\n### Bonus Exercise: Set Configuration Properties Programmatically\n\nIf you have more time, attempt the following extra bonus steps:","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Bonus Exercise: Set Configuration Properties Programmatically</h3>\n<p>If you have more time, attempt the following extra bonus steps:</p>\n"}]},"apps":[],"jobName":"paragraph_1591815454869_-1261233513","id":"20200425-223039_1750191840","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128122"},{"title":"1 - Set the application name","text":"%md\nUsing the builder function `appName` will set the application name.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815454869_855718979","id":"20200425-221401_1112875023","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128123"},{"title":"2 - Run the application again","text":"%md\nRe-run the application without using script options to set properties.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815454870_521326400","id":"20200425-233351_1449419144","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128124"},{"title":"3 - Use the YARN UI to confirm the application name was correctly set","text":"%md\nView the YARN UI to confirm that the application name was correctly set.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815454870_1826022597","id":"20200425-233351_1170536428","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128125"},{"text":"%md\nYou can find the bonus solution in `/home/devuser/bin/spark/application/python-bonus/`.","user":"sysadmin","dateUpdated":"2020-06-13T17:24:21+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>You can find the Python bonus solution in <code>$DEVSH/exercises/sparkapplication/python-bonus</code>.\n<br  />The Scala solution is in the <code>bonus</code> package in the exercise project.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815454870_1832432424","id":"20200425-221401_564218238","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128126"},{"text":"%md\n# Result\n**You have now:** \n\n---","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Result</h1>\n<p><strong>You have now:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591815454870_1621423778","id":"20181126-133507_1472573213","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128127"},{"text":"%md\n# Solution\n---","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Solution</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591815454871_1206665937","id":"20181018-125200_1133281582","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128128"},{"text":"%md\n### Write and Run a Spark Application in Python","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815454871_863926217","id":"20200429-052251_1522474166","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128129"},{"title":"1 - Review the script","text":"%md\n```\n# Python application stub\nimport sys\n\nfrom pyspark.sql import SparkSession\n\nif __name__ == \"__main__\":\n  if len(sys.argv) < 2:\n    print(sys.stderr, \"Usage: accounts-by-state.py <state-code>\")\n    sys.exit()\n\n  stateCode = sys.argv[1]\n\n  # 3. Create a SparkSesssion object\n  spark = SparkSession.builder.getOrCreate()\n  # 4. Reduce distracting output\n  spark.sparkContext.setLogLevel(\"WARN\")\n  \n  # 5. Load accounts table\n  accountsDF = spark.read.table(\"telco.accounts\")\n  # Select accounts where the state column matches the argument provided\n  stateAccountsDF = accountsDF.where(accountsDF.state == stateCode)\n  # Save the results\n  stateAccountsDF.write.mode(\"overwrite\").save(\"/user/zeppelin/accounts_by_state/\" + stateCode)\n\n  # 6. Stop the Spark session\n  spark.stop()\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1591815454871_2038290257","id":"20200429-052349_1220648926","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128130"},{"text":"%jdbc(hive_interactive)\nSELECT * FROM telco.accounts LIMIT 5;","user":"sysadmin","dateUpdated":"2020-06-13T20:33:07+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"accounts.acct_num":"string","accounts.acct_create_dt":"string","accounts.acct_close_dt":"string","accounts.first_name":"string","accounts.last_name":"string","accounts.address":"string","accounts.city":"string","accounts.state":"string","accounts.zipcode":"string","accounts.phone_number":"string","accounts.created":"string","accounts.modified":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"accounts.acct_num\taccounts.acct_create_dt\taccounts.acct_close_dt\taccounts.first_name\taccounts.last_name\taccounts.address\taccounts.city\taccounts.state\taccounts.zipcode\taccounts.phone_number\taccounts.created\taccounts.modified\n1\t2008-10-23 16:05:05.0\tnull\tDonald\tBecton\t2275 Washburn Street\tOakland\tCA\t94660\t5100032418\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n2\t2008-11-12 03:00:01.0\tnull\tDonna\tJones\t3885 Elliott Street\tSan Francisco\tCA\t94171\t4150835799\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n3\t2008-12-21 09:19:50.0\tnull\tDorthy\tChalmers\t4073 Whaley Lane\tSan Mateo\tCA\t94479\t6506877757\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n4\t2008-11-28 00:08:09.0\tnull\tLeila\tSpencer\t1447 Ross Street\tSan Mateo\tCA\t94444\t6503198619\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n5\t2008-11-15 23:06:06.0\tnull\tAnita\tLaughlin\t2767 Hill Street\tRichmond\tCA\t94872\t5107754354\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n"}]},"apps":[],"jobName":"paragraph_1592076450549_-1700730591","id":"20200613-192730_2096451432","dateCreated":"2020-06-13T19:27:30+0000","dateStarted":"2020-06-13T20:33:08+0000","dateFinished":"2020-06-13T20:33:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:128131"},{"text":"%pyspark\naccountsDF = spark.read.table(\"telco.accounts\")\naccountsDF.show()","user":"sysadmin","dateUpdated":"2020-06-13T20:56:50+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------+-------------------+-------------------+----------+---------+--------------------+-------------+-----+-------+------------+-------------------+-------------------+\n|acct_num|     acct_create_dt|      acct_close_dt|first_name|last_name|             address|         city|state|zipcode|phone_number|            created|           modified|\n+--------+-------------------+-------------------+----------+---------+--------------------+-------------+-----+-------+------------+-------------------+-------------------+\n|       1|2008-10-23 16:05:05|               null|    Donald|   Becton|2275 Washburn Street|      Oakland|   CA|  94660|  5100032418|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|       2|2008-11-12 03:00:01|               null|     Donna|    Jones| 3885 Elliott Street|San Francisco|   CA|  94171|  4150835799|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|       3|2008-12-21 09:19:50|               null|    Dorthy| Chalmers|    4073 Whaley Lane|    San Mateo|   CA|  94479|  6506877757|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|       4|2008-11-28 00:08:09|               null|     Leila|  Spencer|    1447 Ross Street|    San Mateo|   CA|  94444|  6503198619|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|       5|2008-11-15 23:06:06|               null|     Anita| Laughlin|    2767 Hill Street|     Richmond|   CA|  94872|  5107754354|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|       6|2008-11-20 12:39:33|2014-03-01 07:37:48|    Stevie|   Bridge|   3977 Linda Street|   Sacramento|   CA|  94264|  9162111862|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|       7|2008-12-09 10:32:12|2010-10-16 10:01:51|     David|   Eggers|    2109 Ross Street|      Oakland|   CA|  94508|  5103935529|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|       8|2008-12-15 08:49:38|               null|   Dorothy|  Koopman|   1985 Pratt Avenue|    San Mateo|   CA|  94469|  6502406661|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|       9|2008-11-07 17:58:55|2014-02-14 01:26:52|      Kara|     Kohl|     235 Fort Street|    Palo Alto|   CA|  94312|  6502384894|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|      10|2008-12-02 23:28:01|               null|     Diane|   Nelson|      921 Sardis Sta|      Oakland|   CA|  94577|  5102711264|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|      11|2008-10-27 19:37:29|               null|    Robert|   Fisher|  3710 Heliport Loop|      Oakland|   CA|  94669|  5109726353|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|      12|2008-12-21 23:21:53|               null|    Marcia|  Roberts|   3616 Augusta Park|    Palo Alto|   CA|  94322|  6505209743|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|      13|2008-10-28 14:27:18|               null|    Andres|    Cruse|518 John Daniel D...|San Francisco|   CA|  94143|  4153335996|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|      14|2008-11-29 18:25:38|               null|       Ann|    Moore|4984 Buena Vista ...|   Sacramento|   CA|  94234|  9163145431|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|      15|2008-11-14 09:22:05|               null|    Joseph|   Lackey|  3977 Snowbird Lane|     Berkeley|   CA|  94726|  5108428188|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|      16|2008-12-03 23:48:22|2013-08-01 15:10:07|     Sarah|   Duvall|    4875 Essex Court|San Francisco|   CA|  94056|  4154969289|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|      17|2008-12-27 23:31:40|               null|      Lucy|   Corley|   4834 Brown Street|   Santa Rosa|   CA|  94980|  7076068290|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|      18|2008-11-05 11:16:01|               null|    Roland| Crawford| 2951 Allison Avenue|    Palo Alto|   CA|  94373|  6506033983|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|      19|2008-11-13 23:40:53|2014-02-26 18:44:26|     Leona|     Bray|      364 Romrog Way|   Santa Rosa|   CA|  94913|  7070013038|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|      20|2008-11-02 07:55:25|               null|   Forrest|   Becker|4211 Lightning Po...|   Santa Rosa|   CA|  94902|  7075831640|2014-03-18 13:29:47|2014-03-18 13:29:47|\n+--------+-------------------+-------------------+----------+---------+--------------------+-------------+-----+-------+------------+-------------------+-------------------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://client02.cloudair.lan:4040/jobs/job?id=0"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1592081752564_1566524257","id":"20200613-205552_1989462301","dateCreated":"2020-06-13T20:55:52+0000","dateStarted":"2020-06-13T20:56:50+0000","dateFinished":"2020-06-13T20:57:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:128132"},{"title":"2 - Run your application","text":"%sh\nspark-submit /home/devuser/bin/spark/application/python-solution/accounts-by-state.py CA\n#cd /home/devuser/bin/spark/application/python-solution/\n#spark-submit accounts-by-state.py CA","user":"sysadmin","dateUpdated":"2020-06-13T21:04:23+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"20/06/13 21:04:24 INFO SparkContext: Running Spark version 2.3.2.3.1.5.0-152\n20/06/13 21:04:24 INFO SparkContext: Submitted application: accounts-by-state.py\n20/06/13 21:04:24 INFO SecurityManager: Changing view acls to: zeppelin\n20/06/13 21:04:24 INFO SecurityManager: Changing modify acls to: zeppelin\n20/06/13 21:04:24 INFO SecurityManager: Changing view acls groups to: \n20/06/13 21:04:24 INFO SecurityManager: Changing modify acls groups to: \n20/06/13 21:04:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(zeppelin); groups with view permissions: Set(); users  with modify permissions: Set(zeppelin); groups with modify permissions: Set()\n20/06/13 21:04:25 INFO Utils: Successfully started service 'sparkDriver' on port 37407.\n20/06/13 21:04:25 INFO SparkEnv: Registering MapOutputTracker\n20/06/13 21:04:25 INFO SparkEnv: Registering BlockManagerMaster\n20/06/13 21:04:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n20/06/13 21:04:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n20/06/13 21:04:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-407a0b1c-6041-4658-8fee-d82325ec0705\n20/06/13 21:04:25 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n20/06/13 21:04:25 INFO SparkEnv: Registering OutputCommitCoordinator\n20/06/13 21:04:25 INFO log: Logging initialized @1626ms\n20/06/13 21:04:25 INFO Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\n20/06/13 21:04:25 INFO Server: Started @1683ms\n20/06/13 21:04:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n20/06/13 21:04:25 INFO AbstractConnector: Started ServerConnector@6398203c{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}\n20/06/13 21:04:25 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1f7efb50{/jobs,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3715dec7{/jobs/json,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@31ab1be6{/jobs/job,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@f4cf8e8{/jobs/job/json,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@16b6fc20{/stages,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7faba13b{/stages/json,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6ae855cf{/stages/stage,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@17d1f62{/stages/stage/json,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@299374af{/stages/pool,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@709fdf34{/stages/pool/json,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3dad1640{/storage,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@54c28898{/storage/json,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@66c6b8a3{/storage/rdd,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@682ead30{/storage/rdd/json,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3de4b963{/environment,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7d22700f{/environment/json,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2e72dca8{/executors,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@33a8b37b{/executors/json,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2b9f2bcc{/executors/threadDump,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@455394b8{/executors/threadDump/json,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7d7c8726{/static,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@136282cc{/,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@10433051{/api,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@46cb9e51{/jobs/job/kill,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@372e1dd0{/stages/stage/kill,null,AVAILABLE,@Spark}\n20/06/13 21:04:25 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://client02.cloudair.lan:4041\n20/06/13 21:04:26 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm2\n20/06/13 21:04:26 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n20/06/13 21:04:26 INFO Configuration: found resource resource-types.xml at file:/etc/hadoop/3.1.5.0-152/0/resource-types.xml\n20/06/13 21:04:26 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n20/06/13 21:04:26 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n20/06/13 21:04:26 INFO Client: Setting up container launch context for our AM\n20/06/13 21:04:26 INFO Client: Setting up the launch environment for our AM container\n20/06/13 21:04:26 INFO Client: Preparing resources for our AM container\n20/06/13 21:04:27 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://cloudair/hdp/apps/3.1.5.0-152/spark2/spark2-hdp-yarn-archive.tar.gz\n20/06/13 21:04:27 INFO Client: Source and destination file systems are the same. Not copying hdfs://cloudair/hdp/apps/3.1.5.0-152/spark2/spark2-hdp-yarn-archive.tar.gz\n20/06/13 21:04:27 INFO Client: Distribute hdfs cache file as spark.sql.hive.metastore.jars for HDP, hdfsCacheFile:hdfs://cloudair/hdp/apps/3.1.5.0-152/spark2/spark2-hdp-hive-archive.tar.gz\n20/06/13 21:04:27 INFO Client: Source and destination file systems are the same. Not copying hdfs://cloudair/hdp/apps/3.1.5.0-152/spark2/spark2-hdp-hive-archive.tar.gz\n20/06/13 21:04:27 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/python/lib/pyspark.zip -> hdfs://cloudair/user/zeppelin/.sparkStaging/application_1591629524336_0016/pyspark.zip\n20/06/13 21:04:27 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip -> hdfs://cloudair/user/zeppelin/.sparkStaging/application_1591629524336_0016/py4j-0.10.7-src.zip\n20/06/13 21:04:27 INFO Client: Uploading resource file:/tmp/spark-2632fe65-4250-4d30-ae39-464f08046887/__spark_conf__2242795682009929813.zip -> hdfs://cloudair/user/zeppelin/.sparkStaging/application_1591629524336_0016/__spark_conf__.zip\n20/06/13 21:04:27 INFO SecurityManager: Changing view acls to: zeppelin\n20/06/13 21:04:27 INFO SecurityManager: Changing modify acls to: zeppelin\n20/06/13 21:04:27 INFO SecurityManager: Changing view acls groups to: \n20/06/13 21:04:27 INFO SecurityManager: Changing modify acls groups to: \n20/06/13 21:04:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(zeppelin); groups with view permissions: Set(); users  with modify permissions: Set(zeppelin); groups with modify permissions: Set()\n20/06/13 21:04:27 INFO Client: Submitting application application_1591629524336_0016 to ResourceManager\n20/06/13 21:04:27 INFO YarnClientImpl: Submitted application application_1591629524336_0016\n20/06/13 21:04:27 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1591629524336_0016 and attemptId None\n20/06/13 21:04:28 INFO Client: Application report for application_1591629524336_0016 (state: ACCEPTED)\n20/06/13 21:04:28 INFO Client: \n\t client token: N/A\n\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n\t ApplicationMaster host: N/A\n\t ApplicationMaster RPC port: -1\n\t queue: default\n\t start time: 1592082267514\n\t final status: UNDEFINED\n\t tracking URL: http://master03.cloudair.lan:8088/proxy/application_1591629524336_0016/\n\t user: zeppelin\n20/06/13 21:04:29 INFO Client: Application report for application_1591629524336_0016 (state: ACCEPTED)\n20/06/13 21:04:30 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> master02.cloudair.lan,master03.cloudair.lan, PROXY_URI_BASES -> http://master02.cloudair.lan:8088/proxy/application_1591629524336_0016,http://master03.cloudair.lan:8088/proxy/application_1591629524336_0016, RM_HA_URLS -> master02.cloudair.lan:8088,master03.cloudair.lan:8088), /proxy/application_1591629524336_0016\n20/06/13 21:04:30 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.\n20/06/13 21:04:30 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n20/06/13 21:04:30 INFO Client: Application report for application_1591629524336_0016 (state: RUNNING)\n20/06/13 21:04:30 INFO Client: \n\t client token: N/A\n\t diagnostics: N/A\n\t ApplicationMaster host: 172.18.0.51\n\t ApplicationMaster RPC port: 0\n\t queue: default\n\t start time: 1592082267514\n\t final status: UNDEFINED\n\t tracking URL: http://master03.cloudair.lan:8088/proxy/application_1591629524336_0016/\n\t user: zeppelin\n20/06/13 21:04:30 INFO YarnClientSchedulerBackend: Application application_1591629524336_0016 has started running.\n20/06/13 21:04:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43694.\n20/06/13 21:04:30 INFO NettyBlockTransferService: Server created on client02.cloudair.lan:43694\n20/06/13 21:04:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n20/06/13 21:04:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, client02.cloudair.lan, 43694, None)\n20/06/13 21:04:30 INFO BlockManagerMasterEndpoint: Registering block manager client02.cloudair.lan:43694 with 366.3 MB RAM, BlockManagerId(driver, client02.cloudair.lan, 43694, None)\n20/06/13 21:04:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, client02.cloudair.lan, 43694, None)\n20/06/13 21:04:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, client02.cloudair.lan, 43694, None)\n20/06/13 21:04:30 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\n20/06/13 21:04:30 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@37ad7259{/metrics/json,null,AVAILABLE,@Spark}\n20/06/13 21:04:30 INFO EventLoggingListener: Logging events to hdfs:/spark2-history/application_1591629524336_0016\n20/06/13 21:04:33 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.51:40900) with ID 1\n20/06/13 21:04:33 INFO BlockManagerMasterEndpoint: Registering block manager worker01.cloudair.lan:35575 with 408.9 MB RAM, BlockManagerId(1, worker01.cloudair.lan, 35575, None)\n20/06/13 21:04:33 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.52:33854) with ID 2\n20/06/13 21:04:33 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n20/06/13 21:04:33 INFO BlockManagerMasterEndpoint: Registering block manager worker02.cloudair.lan:38244 with 408.9 MB RAM, BlockManagerId(2, worker02.cloudair.lan, 38244, None)\nTraceback (most recent call last):\n  File \"/home/devuser/bin/spark/application/python-solution/accounts-by-state.py\", line 15, in <module>\n    accountsDF = spark.read.table(\"telco.accounts\")\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 288, in table\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\npyspark.sql.utils.AnalysisException: u\"Table or view not found: `telco`.`accounts`;;\\n'UnresolvedRelation `telco`.`accounts`\\n\"\n"},{"type":"TEXT","data":"ExitValue: 1"}]},"apps":[],"jobName":"paragraph_1591815454871_1190865546","id":"20200429-180046_1605716636","dateCreated":"2020-06-10T18:57:34+0000","dateStarted":"2020-06-13T21:04:23+0000","dateFinished":"2020-06-13T21:04:34+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:128133"},{"title":"3 - Verify the data was saved correctly and use parquet-tools to verify the contents","text":"%sh\nhdfs dfs -ls /user/zeppelin/accounts_by_state/\n#parquet-tools","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815454871_-477372903","id":"20200429-180045_1098646529","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128134"},{"text":"%md\n### View the Spark Application UI","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>View the Spark Application UI</h3>\n"}]},"apps":[],"jobName":"paragraph_1591815454872_1068405509","id":"20200429-180038_1523159961","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128135"},{"title":"4 - Visit the YARN Resource Manager UI","text":"","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815454872_-942184804","id":"20200429-184123_787430428","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128136"},{"text":"%md\n### Set Configuration Options Using Submit Script Flags","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Set Configuration Options Using Submit Script Flags</h3>\n"}]},"apps":[],"jobName":"paragraph_1591815454872_1170371372","id":"20200429-184152_595857218","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128137"},{"title":"5 - Specify an Application name","text":"%sh\nspark-submit --name \"Accounts by State 1\" /home/devuser/bin/spark/application/python-solution/accounts-by-state.py CA\n","user":"sysadmin","dateUpdated":"2020-06-13T17:24:31+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815454872_-1620022436","id":"20200429-180037_1857916811","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128138"},{"title":"6 - Use YARN UI to confirm that the application name was set correctly","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815454873_-289814835","id":"20200429-180036_1023993316","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128139"},{"title":"7 - Use YARN UI to review the properties of the application","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815454873_1948657365","id":"20200429-180036_1907901285","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128140"},{"title":"8 - Using the conf option","text":"%sh\nspark-submit --name \"Accounts by State 2\" --conf spark.default.parallelism=4 /home/devuser/bin/spark/application/python-solution/accounts-by-state.py CA\n","user":"sysadmin","dateUpdated":"2020-06-13T17:24:38+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815454873_-507485978","id":"20200429-180036_2040189704","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128141"},{"title":"9 - Use YARN UI to confirm that the parallelism property is set correctly","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815454873_1646672529","id":"20200429-180035_845632759","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128142"},{"text":"%md\n### *Optional:* Review Property Setting Overrides","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3><em>Optional:</em> Review Property Setting Overrides</h3>\n"}]},"apps":[],"jobName":"paragraph_1591815454874_585045498","id":"20200429-180034_1618506913","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128143"},{"title":"10 - Display Application Property values","text":"%sh\nspark-submit --verbose --name \"Accounts by State 3\" --conf spark.default.parallelism=4 /home/devuser/bin/spark/application/python-solution/accounts-by-state.py CA","user":"sysadmin","dateUpdated":"2020-06-13T17:24:48+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815454874_520809774","id":"20200429-180034_1684671470","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128144"},{"title":"11 - Examine the properties","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815454874_1292090375","id":"20200429-180033_1369633160","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128145"},{"text":"%md\n### Bonus Exercise: Set Configuration Properties Programmatically","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Bonus Exercise: Set Configuration Properties Programmatically</h3>\n"}]},"apps":[],"jobName":"paragraph_1591815454874_-1488970343","id":"20200429-180032_1717945093","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128146"},{"title":"1 - Set the application name","text":"%md\n```\nspark = SparkSession.builder.appName(\"Accounts by State 3\").getOrCreate()\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815454874_910232489","id":"20200429-180032_1878392489","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128147"},{"title":"2 - Run the application again","text":"%sh\nspark-submit /home/devuser/bin/spark/application/python-bonus/accounts-by-state.py CA\n#cd /home/devuser/bin/spark/application/python-bonus/\n#spark-submit accounts-by-state.py CA","user":"sysadmin","dateUpdated":"2020-06-13T17:25:23+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815454875_1519636687","id":"20200429-180031_1146863009","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128148"},{"title":"3 - Use the YARN UI to confirm the application name was correctly set","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815454875_-2138628681","id":"20200429-180029_822001470","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128149"},{"text":"%md\nWe hope you've enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Hortonworks Apache Spark Tutorials](http://hortonworks.com/tutorials/#tuts-developers) are your natural next step where you can explore Spark in more depth.\n2. [Hortonworks Community Connection (HCC)](https://community.hortonworks.com/spaces/85/data-science.html?type=question) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Hortonworks Apache Spark Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html) - official Spark documentation.\n4. [Hortonworks Apache Zeppelin Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html) - official Zeppelin documentation.\n","user":"sysadmin","dateUpdated":"2020-06-10T18:57:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We hope you've enjoyed this lab. Below are additional resources that you should find useful:</p>\n<ol>\n<li><a href=\"http://hortonworks.com/tutorials/#tuts-developers\">Hortonworks Apache Spark Tutorials</a> are your natural next step where you can explore Spark in more depth.</li>\n<li><a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\">Hortonworks Community Connection (HCC)</a> is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html\">Hortonworks Apache Spark Docs</a> - official Spark documentation.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html\">Hortonworks Apache Zeppelin Docs</a> - official Zeppelin documentation.</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1591815454875_214811883","id":"20181126-133017_244739700","dateCreated":"2020-06-10T18:57:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128150"}],"name":"PYSPARK/12-Writing,Configuring,AndRunningASparkApplication","id":"2FB2A7BX2","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}