{"paragraphs":[{"text":"%md\n# About This Lab\n**Objective:** Become familiar with dataframes schemas operations\n**File locations:**\n**Successful outcome:**\n**Before you begin:**\n**Related lessons:** Working with Dataframes and Schemas\n\n---","user":"anonymous","dateUpdated":"2020-01-11T19:40:08-0800","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578792895774_2138312980","id":"20171105-200834_1116095891","dateCreated":"2020-01-11T17:34:55-0800","dateStarted":"2020-01-11T19:44:02-0800","dateFinished":"2020-01-11T19:44:02-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:95599"},{"text":"%md\n# Setup\n---","user":"anonymous","dateUpdated":"2020-01-11T17:34:55-0800","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578792895777_-1679258516","id":"20181114-164229_902436001","dateCreated":"2020-01-11T17:34:55-0800","dateStarted":"2020-01-11T19:44:02-0800","dateFinished":"2020-01-11T19:44:02-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95600"},{"text":"%sh\n\nhdfs dfs -rm -r -f /devsh_loudacre/accounts_zip94913\nhdfs dfs -rm -r -f /devsh_loudacre/devices_parquet","user":"anonymous","dateUpdated":"2020-01-11T19:42:37-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578798920988_-849979286","id":"20200111-191520_207232986","dateCreated":"2020-01-11T19:15:20-0800","dateStarted":"2020-01-11T19:44:02-0800","dateFinished":"2020-01-11T19:44:07-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95601"},{"text":"%md\n# Lab\n---","user":"anonymous","dateUpdated":"2020-01-11T17:34:55-0800","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578792895780_-954746849","id":"20181114-164844_1661453681","dateCreated":"2020-01-11T17:34:55-0800","dateStarted":"2020-01-11T19:44:07-0800","dateFinished":"2020-01-11T19:44:07-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95602"},{"text":"%md\n## Create a Dataframe Based on a Hive Table","user":"anonymous","dateUpdated":"2020-01-11T17:37:25-0800","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578792895781_395057320","id":"20171105-200519_752831754","dateCreated":"2020-01-11T17:34:55-0800","dateStarted":"2020-01-11T19:44:07-0800","dateFinished":"2020-01-11T19:44:07-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95603"},{"text":"%md\nThe %sql magic provides you with a Spark SQL environment that allows you to interact with Hive tables.","user":"anonymous","dateUpdated":"2020-01-11T17:50:45-0800","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578792895782_280715724","id":"20181115-084123_1911813743","dateCreated":"2020-01-11T17:34:55-0800","dateStarted":"2020-01-11T19:44:07-0800","dateFinished":"2020-01-11T19:44:07-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95604"},{"title":"1 - Review the schema of the devsh.accounts table","text":"%sql","user":"anonymous","dateUpdated":"2020-01-11T17:39:10-0800","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578792895783_129598517","id":"20171105-200623_656362182","dateCreated":"2020-01-11T17:34:55-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95605"},{"title":"2 - Create a new DataFrame using the Hive devsh.accounts table","text":"%pyspark\n","user":"anonymous","dateUpdated":"2020-01-11T17:54:05-0800","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578792895785_194126470","id":"20171105-201709_849284875","dateCreated":"2020-01-11T17:34:55-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95606"},{"text":"%md\nPrint the schema and the first few rows of the DataFrame, and note that the schema aligns with that of the Hive table.","user":"anonymous","dateUpdated":"2020-01-11T17:57:42-0800","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578792895786_130698967","id":"20171105-201449_1118165660","dateCreated":"2020-01-11T17:34:55-0800","dateStarted":"2020-01-11T19:44:07-0800","dateFinished":"2020-01-11T19:44:07-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95607"},{"title":"3 - View the dataframe and confirm alignment with the Hive table","text":"%pyspark\n","user":"anonymous","dateUpdated":"2020-01-11T17:58:50-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578794266540_1992973226","id":"20200111-175746_198257135","dateCreated":"2020-01-11T17:57:46-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95608"},{"text":"%md\nCreate a new DataFrame with rows from the accounts data where the zip code is 94913, and save the result to CSV files in the /devsh_loudacre/ accounts_zip94913 HDFS directory. \nYou can do this in a single command, as shown below, or with multiple commands.","user":"anonymous","dateUpdated":"2020-01-11T18:01:33-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578794455721_694238139","id":"20200111-180055_1261073848","dateCreated":"2020-01-11T18:00:55-0800","dateStarted":"2020-01-11T19:44:07-0800","dateFinished":"2020-01-11T19:44:07-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95609"},{"title":"4 - Create a new dataframe and save it to HDFS","text":"%pyspark","user":"anonymous","dateUpdated":"2020-01-11T18:02:58-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578794512313_-1516886090","id":"20200111-180152_647295647","dateCreated":"2020-01-11T18:01:52-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95610"},{"text":"%md\nUse hdfs to view the /devsh_loudacre/accounts_zip94913 directory in HDFS and the data in one of the saved files.\nConfirm that the CSV file includes a header line, and that only records for the selected zip code are included.","user":"anonymous","dateUpdated":"2020-01-11T18:06:03-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578794717251_-1243672470","id":"20200111-180517_2057464815","dateCreated":"2020-01-11T18:05:17-0800","dateStarted":"2020-01-11T19:44:07-0800","dateFinished":"2020-01-11T19:44:07-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95611"},{"title":"5 - Confirm successful write to HDFS","text":"%sh\n","user":"anonymous","dateUpdated":"2020-01-11T18:07:00-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578794776624_-2065625029","id":"20200111-180616_878429565","dateCreated":"2020-01-11T18:06:16-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95612"},{"text":"%md\n#### Optional: \nTry creating a new DataFrame based on the CSV files you created above. \nCompare the schema of the original accountsDF and the new DataFrame. What’s different? \nTry again, this time setting the inferSchema option to true and compare again.","user":"anonymous","dateUpdated":"2020-01-11T18:26:04-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578795146938_1870866927","id":"20200111-181226_903550862","dateCreated":"2020-01-11T18:12:26-0800","dateStarted":"2020-01-11T19:44:07-0800","dateFinished":"2020-01-11T19:44:07-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95613"},{"title":"6 - Optional: study the impact of the inferSchema option","text":"%pyspark\n","user":"anonymous","dateUpdated":"2020-01-11T18:20:16-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578795260649_-541045315","id":"20200111-181420_1328402570","dateCreated":"2020-01-11T18:14:20-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95614"},{"text":"%md\n### Define a Schema for a DataFrame\nIf you have not done so yet, review the data in the HDFS file /devsh_loudacre/devices.json.","user":"anonymous","dateUpdated":"2020-01-11T18:29:06-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578795990057_-977690823","id":"20200111-182630_966116104","dateCreated":"2020-01-11T18:26:30-0800","dateStarted":"2020-01-11T19:44:07-0800","dateFinished":"2020-01-11T19:44:07-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95615"},{"title":"7 - Review the data in /devsh_loudacre/devices.json","text":"%sh\n","user":"anonymous","dateUpdated":"2020-01-11T18:30:35-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578796153073_535343044","id":"20200111-182913_1175240105","dateCreated":"2020-01-11T18:29:13-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95616"},{"text":"%md\nCreate a new DataFrame based on the devices.json file. (This command could take several seconds while it infers the schema.)","user":"anonymous","dateUpdated":"2020-01-11T18:33:18-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578796361666_1803965612","id":"20200111-183241_1034944500","dateCreated":"2020-01-11T18:32:41-0800","dateStarted":"2020-01-11T19:44:08-0800","dateFinished":"2020-01-11T19:44:08-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95617"},{"title":"8 - Read the devices.json file into a devDF dataframe","text":"%pyspark\n","user":"anonymous","dateUpdated":"2020-01-11T18:53:56-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578796311019_-328584359","id":"20200111-183151_1635217379","dateCreated":"2020-01-11T18:31:51-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95618"},{"text":"%md\nView the schema of the devDF DataFrame. Note the column names and types that Spark inferred from the JSON file. \nIn particular, note that the release_dt column is of type string, whereas the data in the column actually represents a timestamp.","user":"anonymous","dateUpdated":"2020-01-11T18:53:11-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578796538124_-229367483","id":"20200111-183538_253681976","dateCreated":"2020-01-11T18:35:38-0800","dateStarted":"2020-01-11T19:44:08-0800","dateFinished":"2020-01-11T19:44:08-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95619"},{"title":"9 - View the schema of the devDF dataframe","text":"%pyspark","user":"anonymous","dateUpdated":"2020-01-11T18:58:27-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578796566510_-1535963812","id":"20200111-183606_516389013","dateCreated":"2020-01-11T18:36:06-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95620"},{"text":"%md\nDefine a schema that correctly specifies the column types for this DataFrame. \nStart by importing the package with the definitions of necessary classes and types.\nNext, create a collection of StructField objects, which represent column definitions. \nThe release_dt column should be a timestamp.\n\n```spark\nfrom pyspark.sql.types import *\ndevColumns = [ StructField(\"devnum\",LongType()), StructField(\"make\",StringType()), StructField(\"model\",StringType()), StructField(\"release_dt\",TimestampType()), StructField(\"dev_type\",StringType())]\n```","user":"anonymous","dateUpdated":"2020-01-11T19:23:49-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578797797504_-589577523","id":"20200111-185637_433874821","dateCreated":"2020-01-11T18:56:37-0800","dateStarted":"2020-01-11T19:44:08-0800","dateFinished":"2020-01-11T19:44:08-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95621"},{"title":"10 - Define a schema","text":"%pyspark\n\n","user":"anonymous","dateUpdated":"2020-01-11T19:00:13-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578797861976_2121476243","id":"20200111-185741_1002287600","dateCreated":"2020-01-11T18:57:41-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95622"},{"title":"11 - Create a schema (a StructType object) using the column definition list","text":"%pyspark\n","user":"anonymous","dateUpdated":"2020-01-11T19:03:59-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578798219946_218710335","id":"20200111-190339_1337771160","dateCreated":"2020-01-11T19:03:39-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95623"},{"title":"12 - Recreate the devDF DataFrame, this time using the new schema","text":"%pyspark\n","user":"anonymous","dateUpdated":"2020-01-11T19:09:33-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578798329933_1677289977","id":"20200111-190529_818511213","dateCreated":"2020-01-11T19:05:29-0800","dateStarted":"2020-01-11T19:06:25-0800","dateFinished":"2020-01-11T19:06:25-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95624"},{"title":"13 - View the schema and data of the new DataFrame, and confirm that the release_dt column type is now timestamp","text":"%pyspark\n","user":"anonymous","dateUpdated":"2020-01-11T19:08:32-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578798471480_-689684769","id":"20200111-190751_1912942450","dateCreated":"2020-01-11T19:07:51-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95625"},{"text":"%md\nNow that the device data uses the correct schema, write the data in Parquet format, which automatically embeds the schema. \nSave the Parquet data files into an HDFS directory called /devsh_loudacre/devices_parquet.","user":"anonymous","dateUpdated":"2020-01-11T19:16:38-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578798975763_266603977","id":"20200111-191615_1691108697","dateCreated":"2020-01-11T19:16:15-0800","dateStarted":"2020-01-11T19:44:08-0800","dateFinished":"2020-01-11T19:44:08-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95626"},{"title":"14 - Save the devDF dataframe to /devsh_loudacre/devices_parquet using the parquet format","text":"%pyspark\n","user":"anonymous","dateUpdated":"2020-01-11T19:18:32-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578799003120_-1125179261","id":"20200111-191643_697955659","dateCreated":"2020-01-11T19:16:43-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95627"},{"text":"%md\n#### Optional: \n\nUse parquet-tools to view the schema of the saved files. First download the HDFS directory (or an individual file), then run the command.\n\n```shell\n$ hdfs dfs -get /devsh_loudacre/devices_parquet /tmp/ \n$ parquet-tools schema /tmp/devices_parquet/\n```\n\nIn CDH 5, the parquet-tools command could be used directly on files in HDFS. In CDH 6, this is no longer the case. \nAs a workaround, you can use: hadoop jar /opt/cloudera/parcels/CDH/jars/parquet-tools-1.9.0- cdh6.1.1.jar schema hdfs://localhost/devsh_loudacre/devices_parquet/. Related JIRA: CDH-80574.\n\nNote that the type of the release_dt column is noted as int96; this is how Spark denotes a timestamp type in Parquet.\nFor more information about parquet-tools, run parquet-tools --help.","user":"anonymous","dateUpdated":"2020-01-11T19:26:28-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578799324006_1873474803","id":"20200111-192204_1182605032","dateCreated":"2020-01-11T19:22:04-0800","dateStarted":"2020-01-11T19:44:08-0800","dateFinished":"2020-01-11T19:44:08-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95628"},{"title":"15 - Optional - Confirm the schema of the saved files using parquet-tools","text":"%sh","user":"anonymous","dateUpdated":"2020-01-11T19:30:17-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578799601009_-740634347","id":"20200111-192641_1103008129","dateCreated":"2020-01-11T19:26:41-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95629"},{"text":"%md\nCreate a new DataFrame using the Parquet files you saved in devices_parquet and view its schema.\nNote that Spark is able to correctly infer the timestamp type of the release_dt column from Parquet’s embedded schema.","user":"anonymous","dateUpdated":"2020-01-11T19:31:27-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578799844662_1220070118","id":"20200111-193044_783469452","dateCreated":"2020-01-11T19:30:44-0800","dateStarted":"2020-01-11T19:44:08-0800","dateFinished":"2020-01-11T19:44:08-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95630"},{"title":"16 - Optional - Read the parquet files back in a dataframe to confirm the schema is well saved","text":"%pyspark\n","user":"anonymous","dateUpdated":"2020-01-11T19:36:48-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578799906165_-1238949276","id":"20200111-193146_2016640957","dateCreated":"2020-01-11T19:31:46-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95631"},{"text":"%md\n# Result\n**You have now:** explored different methods to define the schema of a dataframe.\n\n---","user":"anonymous","dateUpdated":"2020-01-11T19:15:01-0800","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578792895787_-1492060300","id":"20181119-142716_792318228","dateCreated":"2020-01-11T17:34:55-0800","dateStarted":"2020-01-11T19:44:08-0800","dateFinished":"2020-01-11T19:44:08-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95632"},{"text":"%md\n# Solution\n---","user":"anonymous","dateUpdated":"2020-01-11T17:34:55-0800","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578792895788_1069184726","id":"20171113-155535_1769142099","dateCreated":"2020-01-11T17:34:55-0800","dateStarted":"2020-01-11T19:44:08-0800","dateFinished":"2020-01-11T19:44:08-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95633"},{"title":"1 - Review the schema of the devsh.accounts table","text":"%sql\n\nDESCRIBE devsh.accounts","user":"anonymous","dateUpdated":"2020-01-11T17:50:24-0800","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"col_name":"string","data_type":"string","comment":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578793152513_-90585176","id":"20200111-173912_1607664098","dateCreated":"2020-01-11T17:39:12-0800","dateStarted":"2020-01-11T19:44:08-0800","dateFinished":"2020-01-11T19:44:08-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95634"},{"title":"2 - Create a new DataFrame using the Hive devsh.accounts table","text":"%pyspark\n\naccountsDF = spark.read.table(\"devsh.accounts\")","user":"anonymous","dateUpdated":"2020-01-11T17:56:01-0800","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578794064250_-806171156","id":"20200111-175424_830219293","dateCreated":"2020-01-11T17:54:24-0800","dateStarted":"2020-01-11T19:44:08-0800","dateFinished":"2020-01-11T19:44:09-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95635"},{"title":"3 - View the dataframe and confirm alignment with the Hive table","text":"%pyspark\n\nz.show(accountsDF)","user":"anonymous","dateUpdated":"2020-01-11T17:59:37-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"acct_num":"string","acct_create_dt":"string","acct_close_dt":"string","first_name":"string","last_name":"string","address":"string","city":"string","state":"string","zipcode":"string","phone_number":"string","created":"string","modified":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578794329811_-692684277","id":"20200111-175849_1440047182","dateCreated":"2020-01-11T17:58:49-0800","dateStarted":"2020-01-11T19:44:09-0800","dateFinished":"2020-01-11T19:44:09-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95636"},{"title":"4 - Create a new dataframe and save it to HDFS","text":"%pyspark\naccountsDF.where(\"zipcode = 94913\").write.option(\"header\",\"true\").csv(\"/devsh_loudacre/accounts_zip94913\")","user":"anonymous","dateUpdated":"2020-01-11T18:04:21-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578794582279_-1599870564","id":"20200111-180302_674401673","dateCreated":"2020-01-11T18:03:02-0800","dateStarted":"2020-01-11T19:44:09-0800","dateFinished":"2020-01-11T19:44:11-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95637"},{"title":"5 - Confirm successful write to HDFS","text":"%sh\nhdfs dfs -ls /devsh_loudacre/accounts_zip94913\nhdfs dfs -cat /devsh_loudacre/accounts_zip94913/part-0000*","user":"anonymous","dateUpdated":"2020-01-11T18:11:22-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578794820167_-691007339","id":"20200111-180700_593022127","dateCreated":"2020-01-11T18:07:00-0800","dateStarted":"2020-01-11T19:44:11-0800","dateFinished":"2020-01-11T19:44:16-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95638"},{"title":"6 - Optional: study the impact of the inferSchema option","text":"%pyspark\n\nspark.read.csv(\"/devsh_loudacre/accounts_zip94913\").printSchema()\nspark.read.option(\"header\",\"true\").csv(\"/devsh_loudacre/accounts_zip94913\").printSchema()\nspark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/devsh_loudacre/accounts_zip94913\").printSchema()","user":"anonymous","dateUpdated":"2020-01-11T18:24:51-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578795621132_-1538569949","id":"20200111-182021_1532001378","dateCreated":"2020-01-11T18:20:21-0800","dateStarted":"2020-01-11T19:44:16-0800","dateFinished":"2020-01-11T19:44:22-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95639"},{"title":"7 - Review the data in /devsh_loudacre/devices.json","text":"%sh\n\nhdfs dfs -cat /devsh_loudacre/devices.json","user":"anonymous","dateUpdated":"2020-01-11T18:31:13-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578796234511_-1306202865","id":"20200111-183034_19858163","dateCreated":"2020-01-11T18:30:34-0800","dateStarted":"2020-01-11T19:44:22-0800","dateFinished":"2020-01-11T19:44:24-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95640"},{"title":"8 - Read the devices.json file into a devDF dataframe","text":"%pyspark\n\ndevDF = spark.read.json(\"/devsh_loudacre/devices.json\")","user":"anonymous","dateUpdated":"2020-01-11T18:54:26-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578796446582_-1704374307","id":"20200111-183406_507235503","dateCreated":"2020-01-11T18:34:06-0800","dateStarted":"2020-01-11T19:44:24-0800","dateFinished":"2020-01-11T19:44:25-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95641"},{"title":"9- View the schema of the devDF dataframe","text":"%pyspark\n\ndevDF.printSchema()","user":"anonymous","dateUpdated":"2020-01-11T18:55:28-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578797704306_-1810150260","id":"20200111-185504_2076885456","dateCreated":"2020-01-11T18:55:04-0800","dateStarted":"2020-01-11T19:44:25-0800","dateFinished":"2020-01-11T19:44:25-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95642"},{"title":"10 - Define a schema","text":"%pyspark\n\nfrom pyspark.sql.types import *\ndevColumns = [ StructField(\"devnum\",LongType()), StructField(\"make\",StringType()), StructField(\"model\",StringType()), StructField(\"release_dt\",TimestampType()), StructField(\"dev_type\",StringType())]\n","user":"anonymous","dateUpdated":"2020-01-11T19:01:15-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578798012503_-1758353350","id":"20200111-190012_576363507","dateCreated":"2020-01-11T19:00:12-0800","dateStarted":"2020-01-11T19:44:25-0800","dateFinished":"2020-01-11T19:44:25-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95643"},{"title":"11 - Create a schema (a StructType object) using the column definition list","text":"%pyspark\n\ndevSchema = StructType(devColumns)","user":"anonymous","dateUpdated":"2020-01-11T19:04:56-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578798245632_693447473","id":"20200111-190405_1345983159","dateCreated":"2020-01-11T19:04:05-0800","dateStarted":"2020-01-11T19:44:25-0800","dateFinished":"2020-01-11T19:44:25-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95644"},{"title":"12 - Recreate the devDF DataFrame, this time using the new schema","text":"%pyspark\n\ndevDF = spark.read.schema(devSchema).json(\"/devsh_loudacre/devices.json\")\n","user":"anonymous","dateUpdated":"2020-01-11T19:10:13-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578798572213_680880455","id":"20200111-190932_1097085748","dateCreated":"2020-01-11T19:09:32-0800","dateStarted":"2020-01-11T19:44:25-0800","dateFinished":"2020-01-11T19:44:25-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95645"},{"title":"13 - View the schema and data of the new DataFrame, and confirm that the release_dt column type is now timestamp","text":"%pyspark\n\ndevDF.printSchema()\nz.show(devDF)","user":"anonymous","dateUpdated":"2020-01-11T19:11:13-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"1":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"devnum":"string","make":"string","model":"string","release_dt":"string","dev_type":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578798529259_1187471432","id":"20200111-190849_1182876565","dateCreated":"2020-01-11T19:08:49-0800","dateStarted":"2020-01-11T19:44:25-0800","dateFinished":"2020-01-11T19:44:26-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95646"},{"title":"14 - Save the devDF dataframe to /devsh_loudacre/devices_parquet using the parquet format","text":"%pyspark\n\ndevDF.write.parquet(\"/devsh_loudacre/devices_parquet\")","user":"anonymous","dateUpdated":"2020-01-11T19:20:57-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578799111640_2115319218","id":"20200111-191831_2119240237","dateCreated":"2020-01-11T19:18:31-0800","dateStarted":"2020-01-11T19:44:26-0800","dateFinished":"2020-01-11T19:44:27-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95647"},{"title":"15 - Optional - Confirm the schema of the saved files using parquet-tools","text":"%sh\n\nhdfs dfs -get /devsh_loudacre/devices_parquet /tmp/\nparquet-tools schema /tmp/devices_parquet/","user":"anonymous","dateUpdated":"2020-01-11T19:29:26-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578799699012_1477906423","id":"20200111-192819_852011514","dateCreated":"2020-01-11T19:28:19-0800","dateStarted":"2020-01-11T19:44:27-0800","dateFinished":"2020-01-11T19:44:31-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95648"},{"title":"16 - Optional - Read the parquet files back in a dataframe to confirm the schema is well saved","text":"%pyspark\n\nspark.read.parquet(\"/devsh_loudacre/devices_parquet\").printSchema()","user":"anonymous","dateUpdated":"2020-01-11T19:35:14-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578799996930_817009161","id":"20200111-193316_622744995","dateCreated":"2020-01-11T19:33:16-0800","dateStarted":"2020-01-11T19:44:31-0800","dateFinished":"2020-01-11T19:44:31-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95649"},{"title":"Additional resources","text":"%md\nWe hope you've enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Cloudera Tutorials](http://cloudera.com/tutorials.html) are your natural next step where you can explore Spark in more depth.\n2. [Cloudera Community](https://community.cloudera.com) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Apache Spark Documentation](https://spark.apache.org/documentation.html) - official Spark documentation.\n4. [Apache Zeppelin Project Home Page](https://zeppelin.apache.org) - official Zeppelin web site.","user":"anonymous","dateUpdated":"2020-01-11T17:34:55-0800","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":10,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578792895791_841753883","id":"20181116-135131_93712280","dateCreated":"2020-01-11T17:34:55-0800","dateStarted":"2020-01-11T19:44:31-0800","dateFinished":"2020-01-11T19:44:31-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95650"},{"text":"%angular\n</br>\n</br>\n</br>\n</br>\n<center>\n<a href=\"https://www.cloudera.com/about/training/courses.html\">\n  <img src=\"https://www.cloudera.com/content/dam/www/marketing/media-kit/logo-assets/cloudera_logo_darkorange.png\" alt=\"Cloudera University\" style=\"width:280px;height:40px;border:0;\" align=\"middle\">\n</a>\n</center>\n</br>\n</br>","user":"anonymous","dateUpdated":"2020-01-11T17:34:55-0800","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":2,"editorMode":"ace/mode/undefined","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578792895792_56244044","id":"20200110-154537_1406191376","dateCreated":"2020-01-11T17:34:55-0800","dateStarted":"2020-01-11T19:44:31-0800","dateFinished":"2020-01-11T19:44:31-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95651"},{"text":"%angular\n","user":"anonymous","dateUpdated":"2020-01-11T17:34:55-0800","config":{"editorSetting":{"language":"scala","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/undefined","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578792895793_-1779631222","id":"20200110-162013_302547143","dateCreated":"2020-01-11T17:34:55-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95652"}],"name":"Labs/Pyspark/WorkingWithDataframesAndSchemas","id":"2EXS7S871","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"angular:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}