{"paragraphs":[{"text":"%md\n# About This Lab\n**Objective:** Explore data using Pair RDDs\n**File locations:** /devsh_loudacre/weblogs\n**Successful outcome:**\n**Before you begin:**\n**Related lessons:** Aggregating Data with Pair RDDs\n\n---","user":"anonymous","dateUpdated":"2020-01-17T01:56:54-0800","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579250011900_-1544335388","id":"20171105-200834_1116095891","dateCreated":"2020-01-17T00:33:31-0800","dateStarted":"2020-01-17T01:58:16-0800","dateFinished":"2020-01-17T01:58:16-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:12718"},{"text":"%md\n# Setup\n---\n\nThe following cells ensure that this notebook can run from top to bottom without errors any number of times.","user":"anonymous","dateUpdated":"2020-01-17T01:56:54-0800","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579250011910_-1875991487","id":"20181114-164229_902436001","dateCreated":"2020-01-17T00:33:31-0800","dateStarted":"2020-01-17T01:58:17-0800","dateFinished":"2020-01-17T01:58:17-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12719"},{"text":"%sh\n\nhdfs dfs -rm -r -f /devsh_loudacre/weblogs\nhdfs dfs -put /home/training/training_materials/devsh/data/weblogs /devsh_loudacre","user":"anonymous","dateUpdated":"2020-01-17T01:56:54-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579251564474_-2012065590","id":"20200117-005924_766635286","dateCreated":"2020-01-17T00:59:24-0800","dateStarted":"2020-01-17T01:58:17-0800","dateFinished":"2020-01-17T01:58:32-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12720"},{"text":"%md\n# Lab\n---","user":"anonymous","dateUpdated":"2020-01-17T01:57:11-0800","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579250011911_2050719195","id":"20181114-164844_1661453681","dateCreated":"2020-01-17T00:33:31-0800","dateStarted":"2020-01-17T01:58:32-0800","dateFinished":"2020-01-17T01:58:32-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12721"},{"text":"%md\n### Explore Web Log Files\n\nIn this section, you will create a pair RDD based on data in the weblogs data files, and use that RDD to explore the data.\n\n**Tip:** In this exercise, you will be reducing and joining large datasets, which can take a lot of time and may result in memory errors resulting from the limited resources available in the course exercise environment.\nTo avoid this problem, perform these exercises with a subset of the web log files by using a wildcard: textFile(\"/ devsh_loudacre/weblogs/*2.log\") includes only filenames ending with 2.log.\n\nUsing map-reduce logic, count the number of requests from each user.\na. Use map to create a pair RDD with the user ID as the key and the integer 1 as the value. (The user ID is the third field in each line.) \nYour data will look something like this:\n\n```pyspark\n(userid,1)\n(userid,1)\n(userid,1)\n...\n```\n\nb. Use reduceByKey to sum the values for each user ID.\nYour RDD data will be similar to this:\n```pyspark\n(userid,5)\n(userid,7)\n(userid,2)\n...\n```","user":"anonymous","dateUpdated":"2020-01-17T01:57:11-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579250247647_-1163024478","id":"20200117-003727_1887180227","dateCreated":"2020-01-17T00:37:27-0800","dateStarted":"2020-01-17T01:58:32-0800","dateFinished":"2020-01-17T01:58:32-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12722"},{"title":"1 - Count the number of requests from each user","text":"%pyspark\n\n","user":"anonymous","dateUpdated":"2020-01-17T01:57:11-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579251182073_-354918993","id":"20200117-005302_91523840","dateCreated":"2020-01-17T00:53:02-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12723"},{"text":"%md\nUse countByKey to determine how many users visited the site for each frequency.\nThat is, how many users visited once, twice, three times, and so on.\na. Use map to reverse the key and value, like this:\n\n```pyspark\n(5,userid)\n(7,userid)\n(2,userid)\n...\n```\n\nb. Use the countByKey action to return a map of frequency: user-count pairs.","user":"anonymous","dateUpdated":"2020-01-17T01:57:11-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579251902315_230841326","id":"20200117-010502_1114454940","dateCreated":"2020-01-17T01:05:02-0800","dateStarted":"2020-01-17T01:58:33-0800","dateFinished":"2020-01-17T01:58:33-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12724"},{"title":"2 - Determine how many users visited the site for each frequency","text":"%pyspark\n\n","user":"anonymous","dateUpdated":"2020-01-17T01:57:11-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579252085700_1810674070","id":"20200117-010805_1450618394","dateCreated":"2020-01-17T01:08:05-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12725"},{"text":"%md\nCreate an RDD where the user ID is the key, and the value is the list of all the IP addresses that user has connected from. (IP address is the first field in each request line.)\n**Hint:** Map to (userid,ipaddress) and then use groupByKey.\n```pyspark\n(userid,20.1.34.55)\n(userid,245.33.1.1)\n(userid,65.50.196.141)\n...\n```\n```pyspark\n(userid,[20.1.34.55, 74.125.239.98])\n(userid,[75.175.32.10, 245.33.1.1, 66.79.233.99])\n(userid,[65.50.196.141])\n...\n```","user":"anonymous","dateUpdated":"2020-01-17T01:57:11-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579252250492_-502486223","id":"20200117-011050_1975747792","dateCreated":"2020-01-17T01:10:50-0800","dateStarted":"2020-01-17T01:58:33-0800","dateFinished":"2020-01-17T01:58:33-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12726"},{"title":"3 - Create an RDD where the user ID is the key, and the value is the list of all the IP addresses that user has connected from","text":"%pyspark\n\n","user":"anonymous","dateUpdated":"2020-01-17T01:57:12-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579252427354_181257256","id":"20200117-011347_475883542","dateCreated":"2020-01-17T01:13:47-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12727"},{"text":"%md\n### Join Web Log Data with Account Data\nReview the accounts data located in /user/hive/warehouse/devsh.db/ accounts, which contains the data in the Hive devsh.accounts table.\nThe first field in each line is the user ID, which corresponds to the user ID in the web server logs.\nThe other fields include account details such as creation date, first and last name, and so on.\n\nJoin the accounts data with the weblog data to produce a dataset keyed by user ID which contains the user account information and the number of website hits for that user.\n\na. Create an RDD, based on the accounts data, consisting of key/value-array pairs: (userid,[values...])\n```pyspark\n(9012,[9012,2008-11-24 10:04:08,\\N,Cheryl,West, 4905 Olive Street,...])\n(2312,[2312,2008-11-23 14:05:07,\\N,Elizabeth,Kerns, 4703 Eva Pearl Street,Richmond,CA,...])\n(1195,[1195,2008-11-02 17:12:12,2013-07-18 16:42:36,Melissa, Roman,3539 James Martin Circle,Oakland,CA,...])\n...\n```\nb. Join the pair RDD with the set of user-id/hit-count pairs calculated in the first step.\n```pyspark\n(9012,([9012,2008-11-24 10:04:08,\\N,Cheryl,West, 4905 Olive Street,San Francisco,CA,...],4))\n(2312,([2312,2008-11-23 14:05:07,\\N,Elizabeth,Kerns, 4703 Eva Pearl Street,Richmond,CA,...],8))\n(1195,([1195,2008-11-02 17:12:12,2013-07-18 16:42:36,Melissa, Roman,3539 James Martin Circle,Oakland,CA,...],1))\n```\nDisplay the user ID, hit count, and first name (4th value) and last name (5th value) for the first five elements. \nThe output should look similar to this (but your example data may be different):\n```pyspark\n9012 4 Cheryl West\n1123 8 Elizabeth Kerns\n1093 2 Melissa Roman\n...\n```","user":"anonymous","dateUpdated":"2020-01-17T01:57:12-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579252663443_-1364416705","id":"20200117-011743_666063642","dateCreated":"2020-01-17T01:17:43-0800","dateStarted":"2020-01-17T01:58:33-0800","dateFinished":"2020-01-17T01:58:33-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12728"},{"title":"4 - Join the accounts data with the weblog data to produce a dataset keyed by user ID","text":"%pyspark\n\n","user":"anonymous","dateUpdated":"2020-01-17T01:57:12-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579252735477_1144419376","id":"20200117-011855_1432753489","dateCreated":"2020-01-17T01:18:55-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12729"},{"text":"%md\n### Bonus Exercises\n\nIf you have more time, attempt the following extra bonus exercises:","user":"anonymous","dateUpdated":"2020-01-17T01:57:12-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579253550191_1322761909","id":"20200117-013230_460158553","dateCreated":"2020-01-17T01:32:30-0800","dateStarted":"2020-01-17T01:58:33-0800","dateFinished":"2020-01-17T01:58:33-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12730"},{"title":"5 - Use keyBy to create an RDD of account data with the postal code (9th field in the CSV file) as the key","text":"%pyspark\n","user":"anonymous","dateUpdated":"2020-01-17T01:57:12-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579253594589_1179231689","id":"20200117-013314_98050276","dateCreated":"2020-01-17T01:33:14-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12731"},{"text":"%md\nCreate a pair RDD with postal code as the key and a list of names (Last Name,First Name) in that postal code as the value.\n\n* **Hint:** First name and last name are the 4th and 5th fields respectively.\n* Optional: Try using the mapValues operation.","user":"anonymous","dateUpdated":"2020-01-17T01:57:12-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579254361216_-1620282449","id":"20200117-014601_570000018","dateCreated":"2020-01-17T01:46:01-0800","dateStarted":"2020-01-17T01:58:33-0800","dateFinished":"2020-01-17T01:58:33-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12732"},{"title":"6 - Create a pair RDD with postal code as the key and a list of names (Last Name,First Name) in that postal code as the value","text":"%pyspark\n\n","user":"anonymous","dateUpdated":"2020-01-17T01:57:12-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579254357979_2085038999","id":"20200117-014557_1061955362","dateCreated":"2020-01-17T01:45:57-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12733"},{"text":"%md\nSort the data by postal code, then for the first five postal codes, display the code and list the names in that postal zone. \nFor example:\n```pyspark\n--- 85003 \nJenkins,Thad \nRick,Edward \nLindsay,Ivy \n...\n--- 85004 \nMorris,Eric \nReiser,Hazel \nGregg,Alicia \nPreston,Elizabeth \n...\n```\n","user":"anonymous","dateUpdated":"2020-01-17T01:57:12-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579254553819_-1771756981","id":"20200117-014913_2008282954","dateCreated":"2020-01-17T01:49:13-0800","dateStarted":"2020-01-17T01:58:33-0800","dateFinished":"2020-01-17T01:58:33-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12734"},{"title":"7 - Sort the data by postal code, then for the first five postal codes, display the code and list the names in that postal zone","text":"%pyspark\n","user":"anonymous","dateUpdated":"2020-01-17T01:57:12-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579254551217_68869393","id":"20200117-014911_64851727","dateCreated":"2020-01-17T01:49:11-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12735"},{"text":"%md\n# Result\n**You have now:** explored the Loudacre web server log files, as well as the Loudacre user account data, using key-value pair RDDs.\n\n---","user":"anonymous","dateUpdated":"2020-01-17T01:57:12-0800","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579250011912_-1965428476","id":"20181119-142716_792318228","dateCreated":"2020-01-17T00:33:31-0800","dateStarted":"2020-01-17T01:58:33-0800","dateFinished":"2020-01-17T01:58:33-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12736"},{"text":"%md\n# Solution\n---","user":"anonymous","dateUpdated":"2020-01-17T01:57:12-0800","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579250011913_2039635048","id":"20171113-155535_1769142099","dateCreated":"2020-01-17T00:33:31-0800","dateStarted":"2020-01-17T01:58:33-0800","dateFinished":"2020-01-17T01:58:33-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12737"},{"title":"1 - Count the number of requests from each user","text":"%pyspark\n\n# Create an RDD based on a subset of weblogs (those ending in digit 2)\nlogsRDD = sc.textFile(\"/devsh_loudacre/weblogs/*2.log\")\n\n# map each request (line) to a pair (userid, 1), then sum the values\nuserReqsRDD = logsRDD.map(lambda line: line.split(' ')).map(lambda words: (words[2],1)).reduceByKey(lambda count1,count2: count1 + count2)","user":"anonymous","dateUpdated":"2020-01-17T01:57:13-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579251316209_30915497","id":"20200117-005516_1434443054","dateCreated":"2020-01-17T00:55:16-0800","dateStarted":"2020-01-17T01:58:33-0800","dateFinished":"2020-01-17T01:58:33-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12738"},{"title":"2 - Determine how many users visited the site for each frequency","text":"%pyspark\n\n# Show the count frequencies\nfreqCountMap = userReqsRDD.map(lambda (userid,freq): (freq,userid)).countByKey()\nprint freqCountMap","user":"anonymous","dateUpdated":"2020-01-17T01:57:13-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579252172302_1609942492","id":"20200117-010932_956520230","dateCreated":"2020-01-17T01:09:32-0800","dateStarted":"2020-01-17T01:58:33-0800","dateFinished":"2020-01-17T01:58:43-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12739"},{"title":"3 - Create an RDD where the user ID is the key, and the value is the list of all the IP addresses that user has connected from","text":"%pyspark\n\n# Group IPs by user ID\nuserIPsRDD = logsRDD .map(lambda line: line.split(' ')).map(lambda words: (words[2],words[0])).groupByKey()\n\n# print out the first 10 user ids, and their IP list\nfor (userid,ips) in userIPsRDD.take(10):\n   print userid, \":\"\n   for ip in ips: print \"\\t\",ip","user":"anonymous","dateUpdated":"2020-01-17T01:57:22-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579252549676_899506915","id":"20200117-011549_208751082","dateCreated":"2020-01-17T01:15:49-0800","dateStarted":"2020-01-17T01:58:43-0800","dateFinished":"2020-01-17T01:58:45-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12740"},{"title":"4 - Join the accounts data with the weblog data to produce a dataset keyed by user ID","text":"%pyspark\n\n# Map account data to (userid,[values....])\naccountsData = \"/user/hive/warehouse/devsh.db/accounts\"\naccountsRDD = sc.textFile(accountsData).map(lambda s: s.split(',')).map(lambda account: (account[0],account))\n\n# Join account data with userreqs then merge hit count into valuelist   \naccountHitsRDD = accountsRDD.join(userReqsRDD)\n\n# Display userid, hit count, first name, last name for the first 5 elements\nfor (userid,(values,count)) in accountHitsRDD.take(5) : \n    print  userid, count, values[3], values[4]\n","user":"anonymous","dateUpdated":"2020-01-17T01:57:23-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579253459785_-913826383","id":"20200117-013059_733352759","dateCreated":"2020-01-17T01:30:59-0800","dateStarted":"2020-01-17T01:58:45-0800","dateFinished":"2020-01-17T01:58:48-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12741"},{"title":"5 - Use keyBy to create an RDD of account data with the postal code (9th field in the CSV file) as the key","text":"%pyspark\n\naccountsdata = \"/user/hive/warehouse/devsh.db/accounts\"\naccountsByPCode = sc.textFile(accountsdata).map(lambda s: s.split(',')).keyBy(lambda account: account[8])","user":"anonymous","dateUpdated":"2020-01-17T01:57:27-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579253716167_-1376712780","id":"20200117-013516_1552283233","dateCreated":"2020-01-17T01:35:16-0800","dateStarted":"2020-01-17T01:58:48-0800","dateFinished":"2020-01-17T01:58:48-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12742"},{"title":"6 - Create a pair RDD with postal code as the key and a list of names (Last Name,First Name) in that postal code as the value","text":"%pyspark\n\nnamesByPCode = accountsByPCode.mapValues(lambda account: account[4] + ',' + account[3]).groupByKey()","user":"anonymous","dateUpdated":"2020-01-17T01:57:27-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579254419314_-520002663","id":"20200117-014659_1109568109","dateCreated":"2020-01-17T01:46:59-0800","dateStarted":"2020-01-17T01:58:48-0800","dateFinished":"2020-01-17T01:58:48-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12743"},{"title":"7 - Sort the data by postal code, then for the first five postal codes, display the code and list the names in that postal zone","text":"%pyspark\n\nfor (pcode,names) in namesByPCode.sortByKey().take(5):\n   print \"---\" ,pcode\n   for name in names: print name","user":"anonymous","dateUpdated":"2020-01-17T01:57:27-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579254583824_854258727","id":"20200117-014943_557010225","dateCreated":"2020-01-17T01:49:43-0800","dateStarted":"2020-01-17T01:58:48-0800","dateFinished":"2020-01-17T01:58:50-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12744"},{"title":"Additional resources","text":"%md\nWe hope you've enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Cloudera Tutorials](http://cloudera.com/tutorials.html) are your natural next step where you can explore Spark in more depth.\n2. [Cloudera Community](https://community.cloudera.com) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Apache Spark Documentation](https://spark.apache.org/documentation.html) - official Spark documentation.\n4. [Apache Zeppelin Project Home Page](https://zeppelin.apache.org) - official Zeppelin web site.","user":"anonymous","dateUpdated":"2020-01-17T01:57:29-0800","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":10,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579250011915_164437952","id":"20181116-135131_93712280","dateCreated":"2020-01-17T00:33:31-0800","dateStarted":"2020-01-17T01:58:50-0800","dateFinished":"2020-01-17T01:58:50-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12745"},{"text":"%angular\n</br>\n</br>\n</br>\n</br>\n<center>\n<a href=\"https://www.cloudera.com/about/training/courses.html\">\n  <img src=\"https://www.cloudera.com/content/dam/www/marketing/media-kit/logo-assets/cloudera_logo_darkorange.png\" alt=\"Cloudera University\" style=\"width:280px;height:40px;border:0;\" align=\"middle\">\n</a>\n</center>\n</br>\n</br>","user":"anonymous","dateUpdated":"2020-01-17T01:57:29-0800","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":2,"editorMode":"ace/mode/undefined","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579250011916_-1306879932","id":"20200110-154537_1406191376","dateCreated":"2020-01-17T00:33:31-0800","dateStarted":"2020-01-17T01:58:50-0800","dateFinished":"2020-01-17T01:58:50-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12746"},{"text":"%angular\n","user":"anonymous","dateUpdated":"2020-01-17T01:57:32-0800","config":{"editorSetting":{"language":"scala","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/undefined","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579250011917_-498673642","id":"20200110-162013_302547143","dateCreated":"2020-01-17T00:33:31-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12747"}],"name":"Labs/Pyspark/JoiningDataUsingPairRDDs","id":"2F1EPU923","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"angular:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}