{"paragraphs":[{"text":"%md\n# About This Lab\n**Objective:** explore how Spark executes RDD and DataFrame/Dataset queries\n**File locations:** /devsh_loudacre/\n**Successful outcome:**\n**Before you begin:**\n**Related lessons:** Spark Distributed Processing\n\n---","user":"anonymous","dateUpdated":"2020-01-17T05:46:46-0800","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579264649173_1533502920","id":"20171105-200834_1116095891","dateCreated":"2020-01-17T04:37:29-0800","dateStarted":"2020-01-17T05:47:52-0800","dateFinished":"2020-01-17T05:47:52-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4788"},{"text":"%md\n# Setup\n---\nThe following cells ensure that this notebook can run from top to bottom without errors any number of times.","user":"anonymous","dateUpdated":"2020-01-17T05:46:47-0800","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579264649174_-1239483718","id":"20181114-164229_902436001","dateCreated":"2020-01-17T04:37:29-0800","dateStarted":"2020-01-17T05:47:52-0800","dateFinished":"2020-01-17T05:47:52-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4789"},{"text":"%md\n# Lab\n---","user":"anonymous","dateUpdated":"2020-01-17T05:46:47-0800","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579264649174_85408099","id":"20181114-164844_1661453681","dateCreated":"2020-01-17T04:37:29-0800","dateStarted":"2020-01-17T05:47:52-0800","dateFinished":"2020-01-17T05:47:52-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4790"},{"text":"%md\n### Explore Partitioning of File-Based RDDs\n\nReview the accounts data files in HDFS. (Refer to the data file location shown at the start of this exercise.) Take note of the number and sizes of files.","user":"anonymous","dateUpdated":"2020-01-17T05:46:47-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579265037259_-271865516","id":"20200117-044357_507670769","dateCreated":"2020-01-17T04:43:57-0800","dateStarted":"2020-01-17T05:47:52-0800","dateFinished":"2020-01-17T05:47:52-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4791"},{"title":"1 - Create an RDD called accountsRDD by reading the accounts data, splitting it by commas, and keying it by account ID, which is the first field of each line","text":"%pyspark\n\n","user":"anonymous","dateUpdated":"2020-01-17T05:46:47-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579265099558_1884815623","id":"20200117-044459_2003586621","dateCreated":"2020-01-17T04:44:59-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4792"},{"title":"2 - Find the number of partitions in the new RDD","text":"%pyspark\n\n","user":"anonymous","dateUpdated":"2020-01-17T05:46:47-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579265243792_1904982879","id":"20200117-044723_1102357003","dateCreated":"2020-01-17T04:47:23-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4793"},{"text":"%md\nUse toDebugString to view the lineage and execution plan of accountsRDD.\nHow many partitions are in the resulting RDD? How may stages does the query have?","user":"anonymous","dateUpdated":"2020-01-17T05:46:47-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579265315390_1050660177","id":"20200117-044835_1375433688","dateCreated":"2020-01-17T04:48:35-0800","dateStarted":"2020-01-17T05:47:53-0800","dateFinished":"2020-01-17T05:47:53-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4794"},{"title":"3 - Use toDebugString to view the lineage and execution plan of accountsRDD","text":"%pyspark\n\n","user":"anonymous","dateUpdated":"2020-01-17T05:46:47-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579265313201_-680257151","id":"20200117-044833_1225781822","dateCreated":"2020-01-17T04:48:33-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4795"},{"text":"%md\n### Explore Execution of RDD Queries\n\nCall count on accountsRDD to count the number of accounts. This will trigger execution of a job.","user":"anonymous","dateUpdated":"2020-01-17T05:46:47-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579265620681_1068330107","id":"20200117-045340_1571704960","dateCreated":"2020-01-17T04:53:40-0800","dateStarted":"2020-01-17T05:47:53-0800","dateFinished":"2020-01-17T05:47:53-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4796"},{"title":"4 - Call count on accountsRDD to count the number of accounts","text":"%pyspark\n\n","user":"anonymous","dateUpdated":"2020-01-17T05:46:47-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579265654359_-1953954248","id":"20200117-045414_2096176284","dateCreated":"2020-01-17T04:54:14-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4797"},{"text":"%md\nIn the browser, view the application in the YARN RM UI using the provided bookmark (or http://localhost:8088) and click through to view the Spark Application UI.","user":"anonymous","dateUpdated":"2020-01-17T05:46:47-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579265790110_1513313517","id":"20200117-045630_789605596","dateCreated":"2020-01-17T04:56:30-0800","dateStarted":"2020-01-17T05:47:53-0800","dateFinished":"2020-01-17T05:47:53-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4798"},{"text":"%md\nMake sure the Jobs tab is selected, and review the list of completed jobs.\nThe most recent job, which you triggered by calling count, should be at the top of the list.\n(Note that the job description is usually based on the action that triggered the job execution.)\nConfirm that the number of stages is correct, and the number of tasks completed for the job matches the number of RDD partitions you noted when you used toDebugString.","user":"anonymous","dateUpdated":"2020-01-17T05:46:48-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579265826743_-1088663397","id":"20200117-045706_1428741707","dateCreated":"2020-01-17T04:57:06-0800","dateStarted":"2020-01-17T05:47:53-0800","dateFinished":"2020-01-17T05:47:53-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4799"},{"text":"%md\nClick on the job description to view details of the job.\nThis will list all the stages in the job, which in this case is one.","user":"anonymous","dateUpdated":"2020-01-17T05:46:48-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579265888842_-1333325636","id":"20200117-045808_179196009","dateCreated":"2020-01-17T04:58:08-0800","dateStarted":"2020-01-17T05:47:53-0800","dateFinished":"2020-01-17T05:47:53-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4800"},{"text":"%md\nClick on DAG Visualization to see a diagram of the execution plan based on the RDD’s lineage.\nThe main diagram displays only the stages, but if you click on a stage, it will show you the tasks within that stage.","user":"anonymous","dateUpdated":"2020-01-17T05:46:48-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579266444870_-9130289","id":"20200117-050724_564772540","dateCreated":"2020-01-17T05:07:24-0800","dateStarted":"2020-01-17T05:47:53-0800","dateFinished":"2020-01-17T05:47:53-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4801"},{"text":"%md\n### Optional: \nExplore the partitioning and DAG of a more complex query like the one below. Before you view the execution plan or job details, try to figure out how many stages the job will have.\n\n```pyspark\nlogsRDD = sc.textFile(\"/devsh_loudacre/weblogs\")\nuserReqsRDD = logsRDD.map(lambda line: line.split(' ')).map(lambda words: (words[2],1)).reduceByKey(lambda v1,v2: v1 + v2)\naccountHitsRDD = accountsRDD.join(userReqsRDD)\n```\n\nThis query loads Loudacre’s web log data, and calculates how many times each user visited. Then it joins that user count data with account data for each user.","user":"anonymous","dateUpdated":"2020-01-17T05:46:48-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579266531439_-1196935377","id":"20200117-050851_1123289202","dateCreated":"2020-01-17T05:08:51-0800","dateStarted":"2020-01-17T05:47:53-0800","dateFinished":"2020-01-17T05:47:53-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4802"},{"title":"5 - Explore the partitioning and DAG of a more complex query","text":"%pyspark\n\n","user":"anonymous","dateUpdated":"2020-01-17T05:46:48-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579266725605_-73965935","id":"20200117-051205_1521067842","dateCreated":"2020-01-17T05:12:05-0800","dateStarted":"2020-01-17T05:14:40-0800","dateFinished":"2020-01-17T05:14:40-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4803"},{"text":"%md\n**Note:** If you execute the query multiple times, you may note that some tasks within a stage are marked as “skipped.” \nThis is because whenever a shuffle operation is executed, Spark temporarily caches the data that was shuffled. \nSubsequent executions of the same query re-use that data if it’s available to save some steps and improve performance.","user":"anonymous","dateUpdated":"2020-01-17T05:46:48-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579266934674_1431759355","id":"20200117-051534_965974048","dateCreated":"2020-01-17T05:15:34-0800","dateStarted":"2020-01-17T05:47:53-0800","dateFinished":"2020-01-17T05:47:53-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4804"},{"text":"%md\n### Explore Execution of DataFrame Queries","user":"anonymous","dateUpdated":"2020-01-17T05:46:48-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579267050412_130955543","id":"20200117-051730_1309419952","dateCreated":"2020-01-17T05:17:30-0800","dateStarted":"2020-01-17T05:47:53-0800","dateFinished":"2020-01-17T05:47:53-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4805"},{"title":"6 - Create a DataFrame of active accounts from the accounts table in the devsh database","text":"%pyspark\n\n","user":"anonymous","dateUpdated":"2020-01-17T05:46:49-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579267079524_-993784177","id":"20200117-051759_1282938945","dateCreated":"2020-01-17T05:17:59-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4806"},{"title":"7 - View the full execution plan for the new DataFrame.","text":"%pyspark\n","user":"anonymous","dateUpdated":"2020-01-17T05:46:49-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579267516194_1566665605","id":"20200117-052516_1058306200","dateCreated":"2020-01-17T05:25:16-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4807"},{"text":"%md\nCan you locate the line in the physical plan corresponding to the command to load the devsh.accounts table into a DataFrame?\nHow many stages do you think this query has?","user":"anonymous","dateUpdated":"2020-01-17T05:46:49-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579267585828_1124788667","id":"20200117-052625_2036810127","dateCreated":"2020-01-17T05:26:25-0800","dateStarted":"2020-01-17T05:47:54-0800","dateFinished":"2020-01-17T05:47:54-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4808"},{"title":"8 - Call the DataFrame’s show function to execute the query","text":"%pyspark\n\n","user":"anonymous","dateUpdated":"2020-01-17T05:46:49-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579267656461_-22162159","id":"20200117-052736_1379537345","dateCreated":"2020-01-17T05:27:36-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4809"},{"text":"%md\nView the Spark Application UI and choose the SQL tab. \nThis displays a list of DataFrame and Dataset queries you have executed, with the most recent query at the top.","user":"anonymous","dateUpdated":"2020-01-17T05:46:49-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579267798950_-1177030868","id":"20200117-052958_2003036727","dateCreated":"2020-01-17T05:29:58-0800","dateStarted":"2020-01-17T05:47:54-0800","dateFinished":"2020-01-17T05:47:54-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4810"},{"text":"%md\nClick the description for the top query to see the visualization of the query’s execution.\nYou can also see the query’s full execution plan by opening the Details panel below the visualization graph.","user":"anonymous","dateUpdated":"2020-01-17T05:46:49-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579267831494_841605372","id":"20200117-053031_478789607","dateCreated":"2020-01-17T05:30:31-0800","dateStarted":"2020-01-17T05:47:54-0800","dateFinished":"2020-01-17T05:47:54-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4811"},{"text":"%md\nThe first step in the execution is a HiveTableScan, which loaded the account data into the DataFrame.\nHover your mouse over the step to show the step’s execution plan. Compare that to the physical plan for the query.\nNote that it is the same as the last line in the physical execution plan, because it is the first step to execute.\nDid you correctly identify this line in the execution plan as the one corresponding to the DataFrame.read.table operation?","user":"anonymous","dateUpdated":"2020-01-17T05:46:49-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579267861475_-605198465","id":"20200117-053101_120350036","dateCreated":"2020-01-17T05:31:01-0800","dateStarted":"2020-01-17T05:47:54-0800","dateFinished":"2020-01-17T05:47:54-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4812"},{"text":"%md\nClick the SQL tab to return to the main SQL query summary tab.\nThe Job IDs column provides links to the jobs that executed as part of this query execution.\nIn this case, the query consisted of just a single job.\nClick the job’s ID to view the job details.\nThis will display a list of stages that were completed for the query.\n\n* How many stages executed? \n* Is that the number of stages you predicted it would be?","user":"anonymous","dateUpdated":"2020-01-17T05:46:49-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579267903702_-977500426","id":"20200117-053143_1088049350","dateCreated":"2020-01-17T05:31:43-0800","dateStarted":"2020-01-17T05:47:54-0800","dateFinished":"2020-01-17T05:47:54-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4813"},{"text":"%md\n**Optional:** Click the description of the stage to view metrics on the execution of the stage and its tasks.","user":"anonymous","dateUpdated":"2020-01-17T05:46:50-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579267970479_2108431432","id":"20200117-053250_1980302210","dateCreated":"2020-01-17T05:32:50-0800","dateStarted":"2020-01-17T05:47:54-0800","dateFinished":"2020-01-17T05:47:54-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4814"},{"text":"%md\nThe previous query was very simple, involving just a single data source with a where to return only active accounts.\nTry executing a more complex query that joins data from two different data sources.\nThis query reads in the accountdevice data file, which maps account IDs to associated device IDs.\nThen it joins that data with the DataFrame of active accounts you created above.\nThe result is DataFrame consisting of all device IDs in use by currently active accounts.\n\n```pyspark\naccountDeviceDF = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/devsh_loudacre/accountdevice\")\nactiveAcctDevsDF = activeAccountsDF.join(accountDeviceDF,accountDeviceDF.acct_num == accountDeviceDF.account_id).select(\"device_id\")\n```","user":"anonymous","dateUpdated":"2020-01-17T05:46:50-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579268010084_592348981","id":"20200117-053330_26904564","dateCreated":"2020-01-17T05:33:30-0800","dateStarted":"2020-01-17T05:47:54-0800","dateFinished":"2020-01-17T05:47:54-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4815"},{"title":"9 - Try executing a more complex query that joins data from two different data sources","text":"%pyspark\n\n","user":"anonymous","dateUpdated":"2020-01-17T05:46:50-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579268199772_1322388360","id":"20200117-053639_1801859674","dateCreated":"2020-01-17T05:36:39-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4816"},{"text":"%md\nReview the full execution plan using explain, as you did with the previous DataFrame.\nCan you identify which lines in the execution plan load the two different data sources?\nHow many stages do you think this query will execute?","user":"anonymous","dateUpdated":"2020-01-17T05:46:50-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579268476889_-243439882","id":"20200117-054116_1603737239","dateCreated":"2020-01-17T05:41:16-0800","dateStarted":"2020-01-17T05:47:54-0800","dateFinished":"2020-01-17T05:47:54-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4817"},{"text":"%md\nExecute the query and review the execution visualization in the Spark UI.\n\n* What differences do you see between the execution of the earlier query and this one?\n* How many stages executed? \n* Is this what you expected?","user":"anonymous","dateUpdated":"2020-01-17T05:46:50-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579268589426_-1937814241","id":"20200117-054309_1513388745","dateCreated":"2020-01-17T05:43:09-0800","dateStarted":"2020-01-17T05:47:54-0800","dateFinished":"2020-01-17T05:47:54-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4818"},{"text":"%md\n**Optional: Explore an even more complex query that involves multiple joins with three data sources** \nYou can use the last query in the solutions file for this exercise (in the /home/trainingtraining_materials/devsh/exercises/query-execution/solution/ directory). \nThat query creates a list of device IDs, makes, and models, and the number of active accounts that use that type of device, sorted in order from most popular device type to least.","user":"anonymous","dateUpdated":"2020-01-17T05:46:50-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579268661773_1532800257","id":"20200117-054421_1624313613","dateCreated":"2020-01-17T05:44:21-0800","dateStarted":"2020-01-17T05:47:54-0800","dateFinished":"2020-01-17T05:47:54-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4819"},{"text":"%md\n# Result\n**You have now:**\n\n* explored RDD partitioning and lineage-based execution plans using the Spark and the Spark Application UI and\n* explored how Catalyst executes DataFrame and Dataset queries.\n\n---","user":"anonymous","dateUpdated":"2020-01-17T05:46:50-0800","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579264649175_-2115692952","id":"20181119-142716_792318228","dateCreated":"2020-01-17T04:37:29-0800","dateStarted":"2020-01-17T05:47:55-0800","dateFinished":"2020-01-17T05:47:55-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4820"},{"text":"%md\n# Solution\n---","user":"anonymous","dateUpdated":"2020-01-17T05:46:51-0800","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579264649175_1679420374","id":"20171113-155535_1769142099","dateCreated":"2020-01-17T04:37:29-0800","dateStarted":"2020-01-17T05:47:55-0800","dateFinished":"2020-01-17T05:47:55-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4821"},{"title":"1 - Create an RDD called accountsRDD by reading the accounts data, splitting it by commas, and keying it by account ID, which is the first field of each line","text":"%pyspark\n\naccountsRDD = sc.textFile(\"/user/hive/warehouse/devsh.db/accounts\").map(lambda line: line.split(',')).map(lambda account: (account[0],account))","user":"anonymous","dateUpdated":"2020-01-17T05:46:51-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579265145530_1119699753","id":"20200117-044545_184891645","dateCreated":"2020-01-17T04:45:45-0800","dateStarted":"2020-01-17T05:47:55-0800","dateFinished":"2020-01-17T05:47:55-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4822"},{"title":"2 - Find the number of partitions in the new RDD","text":"%pyspark\n\naccountsRDD.getNumPartitions()","user":"anonymous","dateUpdated":"2020-01-17T05:46:51-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579265260058_1827427681","id":"20200117-044740_410268300","dateCreated":"2020-01-17T04:47:40-0800","dateStarted":"2020-01-17T05:47:55-0800","dateFinished":"2020-01-17T05:47:55-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4823"},{"title":"3 - Use toDebugString to view the lineage and execution plan of accountsRDD","text":"%pyspark\n\nprint accountsRDD.toDebugString()","user":"anonymous","dateUpdated":"2020-01-17T05:46:51-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":97.45,"optionOpen":false}}},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579265371689_963064010","id":"20200117-044931_905827162","dateCreated":"2020-01-17T04:49:31-0800","dateStarted":"2020-01-17T05:47:55-0800","dateFinished":"2020-01-17T05:47:55-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4824"},{"title":"4 - Call count on accountsRDD to count the number of accounts","text":"%pyspark\n\naccountsRDD.count()","user":"anonymous","dateUpdated":"2020-01-17T05:46:51-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579265713934_-684408167","id":"20200117-045513_637956581","dateCreated":"2020-01-17T04:55:13-0800","dateStarted":"2020-01-17T05:47:55-0800","dateFinished":"2020-01-17T05:47:56-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4825"},{"title":"5 - Explore the partitioning and DAG of a more complex query","text":"%pyspark\n\nlogsRDD = sc.textFile(\"/devsh_loudacre/weblogs\")\nuserReqsRDD = logsRDD.map(lambda line: line.split(' ')).map(lambda words: (words[2],1)).reduceByKey(lambda v1,v2: v1 + v2)\naccountHitsRDD = accountsRDD.join(userReqsRDD)","user":"anonymous","dateUpdated":"2020-01-17T05:47:00-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579266909119_718453167","id":"20200117-051509_1183347014","dateCreated":"2020-01-17T05:15:09-0800","dateStarted":"2020-01-17T05:47:56-0800","dateFinished":"2020-01-17T05:47:56-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4826"},{"title":"6 - Create a DataFrame of active accounts from the accounts table in the devsh database","text":"%pyspark\n\naccountsDF = spark.read.table(\"devsh.accounts\")\nactiveAccountsDF = accountsDF.select(\"acct_num\").where(accountsDF.acct_close_dt.isNull())","user":"anonymous","dateUpdated":"2020-01-17T05:47:00-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579267102953_-33541758","id":"20200117-051822_1933457879","dateCreated":"2020-01-17T05:18:22-0800","dateStarted":"2020-01-17T05:47:56-0800","dateFinished":"2020-01-17T05:47:56-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4827"},{"title":"7 - View the full execution plan for the new DataFrame.","text":"%pyspark\n\nactiveAccountsDF.explain(True)","user":"anonymous","dateUpdated":"2020-01-17T05:47:00-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579267532816_-464441763","id":"20200117-052532_1956675346","dateCreated":"2020-01-17T05:25:32-0800","dateStarted":"2020-01-17T05:47:56-0800","dateFinished":"2020-01-17T05:47:56-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4828"},{"title":"8 - Call the DataFrame’s show function to execute the query","text":"%pyspark\n\nactiveAccountsDF.show()","user":"anonymous","dateUpdated":"2020-01-17T05:47:00-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579267684713_662264013","id":"20200117-052804_2089720584","dateCreated":"2020-01-17T05:28:04-0800","dateStarted":"2020-01-17T05:47:56-0800","dateFinished":"2020-01-17T05:47:56-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4829"},{"title":"9 - Try executing a more complex query that joins data from two different data sources","text":"%pyspark\n\naccountDeviceDF = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/devsh_loudacre/accountdevice\")\nactiveAcctDevsDF = activeAccountsDF.join(accountDeviceDF,activeAccountsDF.acct_num == accountDeviceDF.account_id).select(\"device_id\")","user":"anonymous","dateUpdated":"2020-01-17T05:47:01-0800","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true,"editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579268323087_734149458","id":"20200117-053843_722086419","dateCreated":"2020-01-17T05:38:43-0800","dateStarted":"2020-01-17T05:47:56-0800","dateFinished":"2020-01-17T05:47:57-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4830"},{"title":"Additional resources","text":"%md\nWe hope you've enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Cloudera Tutorials](http://cloudera.com/tutorials.html) are your natural next step where you can explore Spark in more depth.\n2. [Cloudera Community](https://community.cloudera.com) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Apache Spark Documentation](https://spark.apache.org/documentation.html) - official Spark documentation.\n4. [Apache Zeppelin Project Home Page](https://zeppelin.apache.org) - official Zeppelin web site.\n","user":"anonymous","dateUpdated":"2020-01-17T05:47:05-0800","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":10,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579264649176_1542361729","id":"20181116-135131_93712280","dateCreated":"2020-01-17T04:37:29-0800","dateStarted":"2020-01-17T05:47:57-0800","dateFinished":"2020-01-17T05:47:57-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4831"},{"text":"%angular\n</br>\n</br>\n</br>\n</br>\n<center>\n<a href=\"https://www.cloudera.com/about/training/courses.html\">\n  <img src=\"https://www.cloudera.com/content/dam/www/marketing/media-kit/logo-assets/cloudera_logo_darkorange.png\" alt=\"Cloudera Educational Services\" style=\"width:280px;height:40px;border:0;\" align=\"middle\">\n</a>\n</center>\n</br>\n</br>","user":"anonymous","dateUpdated":"2020-01-17T05:47:05-0800","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":2,"editorMode":"ace/mode/undefined","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579264649177_-1230529669","id":"20200110-154537_1406191376","dateCreated":"2020-01-17T04:37:29-0800","dateStarted":"2020-01-17T05:47:57-0800","dateFinished":"2020-01-17T05:47:57-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4832"},{"text":"%angular\n","user":"anonymous","dateUpdated":"2020-01-17T05:47:05-0800","config":{"editorSetting":{"language":"scala","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/undefined","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579264649177_940478764","id":"20200110-162013_302547143","dateCreated":"2020-01-17T04:37:29-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4833"}],"name":"Labs/Pyspark/ExploringQueryExecution","id":"2EZWCZB2G","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"angular:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}